{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4ab6e7a8ed1f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "<ipython-input-3-4ab6e7a8ed1f>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-75ca61343034>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-28bf1c894daa>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-8-28bf1c894daa>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "<ipython-input-8-28bf1c894daa>:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-8-28bf1c894daa>:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-860e882d2345>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "<ipython-input-9-860e882d2345>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "<ipython-input-9-860e882d2345>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.484015\n",
      "Epoch 1, loss: 2.356050\n",
      "Epoch 2, loss: 2.317979\n",
      "Epoch 3, loss: 2.306623\n",
      "Epoch 4, loss: 2.303297\n",
      "Epoch 5, loss: 2.302304\n",
      "Epoch 6, loss: 2.301991\n",
      "Epoch 7, loss: 2.301899\n",
      "Epoch 8, loss: 2.301881\n",
      "Epoch 9, loss: 2.301857\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fad426b6790>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.127\n",
      "Epoch 0, loss: 2.301890\n",
      "Epoch 1, loss: 2.301866\n",
      "Epoch 2, loss: 2.301834\n",
      "Epoch 3, loss: 2.301874\n",
      "Epoch 4, loss: 2.301892\n",
      "Epoch 5, loss: 2.301864\n",
      "Epoch 6, loss: 2.301880\n",
      "Epoch 7, loss: 2.301844\n",
      "Epoch 8, loss: 2.301878\n",
      "Epoch 9, loss: 2.301869\n",
      "Epoch 10, loss: 2.301858\n",
      "Epoch 11, loss: 2.301852\n",
      "Epoch 12, loss: 2.301887\n",
      "Epoch 13, loss: 2.301867\n",
      "Epoch 14, loss: 2.301868\n",
      "Epoch 15, loss: 2.301866\n",
      "Epoch 16, loss: 2.301873\n",
      "Epoch 17, loss: 2.301863\n",
      "Epoch 18, loss: 2.301867\n",
      "Epoch 19, loss: 2.301881\n",
      "Epoch 20, loss: 2.301849\n",
      "Epoch 21, loss: 2.301864\n",
      "Epoch 22, loss: 2.301849\n",
      "Epoch 23, loss: 2.301870\n",
      "Epoch 24, loss: 2.301869\n",
      "Epoch 25, loss: 2.301857\n",
      "Epoch 26, loss: 2.301877\n",
      "Epoch 27, loss: 2.301867\n",
      "Epoch 28, loss: 2.301849\n",
      "Epoch 29, loss: 2.301854\n",
      "Epoch 30, loss: 2.301862\n",
      "Epoch 31, loss: 2.301862\n",
      "Epoch 32, loss: 2.301853\n",
      "Epoch 33, loss: 2.301877\n",
      "Epoch 34, loss: 2.301866\n",
      "Epoch 35, loss: 2.301865\n",
      "Epoch 36, loss: 2.301858\n",
      "Epoch 37, loss: 2.301851\n",
      "Epoch 38, loss: 2.301872\n",
      "Epoch 39, loss: 2.301859\n",
      "Epoch 40, loss: 2.301866\n",
      "Epoch 41, loss: 2.301858\n",
      "Epoch 42, loss: 2.301863\n",
      "Epoch 43, loss: 2.301867\n",
      "Epoch 44, loss: 2.301864\n",
      "Epoch 45, loss: 2.301872\n",
      "Epoch 46, loss: 2.301868\n",
      "Epoch 47, loss: 2.301883\n",
      "Epoch 48, loss: 2.301870\n",
      "Epoch 49, loss: 2.301858\n",
      "Epoch 50, loss: 2.301856\n",
      "Epoch 51, loss: 2.301865\n",
      "Epoch 52, loss: 2.301870\n",
      "Epoch 53, loss: 2.301870\n",
      "Epoch 54, loss: 2.301863\n",
      "Epoch 55, loss: 2.301847\n",
      "Epoch 56, loss: 2.301873\n",
      "Epoch 57, loss: 2.301872\n",
      "Epoch 58, loss: 2.301868\n",
      "Epoch 59, loss: 2.301851\n",
      "Epoch 60, loss: 2.301860\n",
      "Epoch 61, loss: 2.301859\n",
      "Epoch 62, loss: 2.301867\n",
      "Epoch 63, loss: 2.301865\n",
      "Epoch 64, loss: 2.301871\n",
      "Epoch 65, loss: 2.301869\n",
      "Epoch 66, loss: 2.301848\n",
      "Epoch 67, loss: 2.301860\n",
      "Epoch 68, loss: 2.301864\n",
      "Epoch 69, loss: 2.301860\n",
      "Epoch 70, loss: 2.301864\n",
      "Epoch 71, loss: 2.301873\n",
      "Epoch 72, loss: 2.301851\n",
      "Epoch 73, loss: 2.301871\n",
      "Epoch 74, loss: 2.301889\n",
      "Epoch 75, loss: 2.301869\n",
      "Epoch 76, loss: 2.301877\n",
      "Epoch 77, loss: 2.301871\n",
      "Epoch 78, loss: 2.301864\n",
      "Epoch 79, loss: 2.301866\n",
      "Epoch 80, loss: 2.301871\n",
      "Epoch 81, loss: 2.301868\n",
      "Epoch 82, loss: 2.301862\n",
      "Epoch 83, loss: 2.301852\n",
      "Epoch 84, loss: 2.301876\n",
      "Epoch 85, loss: 2.301856\n",
      "Epoch 86, loss: 2.301868\n",
      "Epoch 87, loss: 2.301865\n",
      "Epoch 88, loss: 2.301863\n",
      "Epoch 89, loss: 2.301878\n",
      "Epoch 90, loss: 2.301864\n",
      "Epoch 91, loss: 2.301873\n",
      "Epoch 92, loss: 2.301861\n",
      "Epoch 93, loss: 2.301872\n",
      "Epoch 94, loss: 2.301884\n",
      "Epoch 95, loss: 2.301877\n",
      "Epoch 96, loss: 2.301870\n",
      "Epoch 97, loss: 2.301869\n",
      "Epoch 98, loss: 2.301898\n",
      "Epoch 99, loss: 2.301862\n",
      "Accuracy after training for 100 epochs:  0.121\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.300568\n",
      "Epoch 1, loss: 2.299670\n",
      "Epoch 2, loss: 2.298840\n",
      "Epoch 3, loss: 2.297995\n",
      "Epoch 4, loss: 2.297213\n",
      "Epoch 5, loss: 2.296454\n",
      "Epoch 6, loss: 2.295691\n",
      "Epoch 7, loss: 2.294923\n",
      "Epoch 8, loss: 2.294199\n",
      "Epoch 9, loss: 2.293450\n",
      "Epoch 10, loss: 2.292717\n",
      "Epoch 11, loss: 2.291991\n",
      "Epoch 12, loss: 2.291290\n",
      "Epoch 13, loss: 2.290583\n",
      "Epoch 14, loss: 2.289877\n",
      "Epoch 15, loss: 2.289177\n",
      "Epoch 16, loss: 2.288472\n",
      "Epoch 17, loss: 2.287801\n",
      "Epoch 18, loss: 2.287132\n",
      "Epoch 19, loss: 2.286445\n",
      "Epoch 20, loss: 2.285772\n",
      "Epoch 21, loss: 2.285099\n",
      "Epoch 22, loss: 2.284433\n",
      "Epoch 23, loss: 2.283793\n",
      "Epoch 24, loss: 2.283141\n",
      "Epoch 25, loss: 2.282484\n",
      "Epoch 26, loss: 2.281822\n",
      "Epoch 27, loss: 2.281200\n",
      "Epoch 28, loss: 2.280561\n",
      "Epoch 29, loss: 2.279926\n",
      "Epoch 30, loss: 2.279301\n",
      "Epoch 31, loss: 2.278680\n",
      "Epoch 32, loss: 2.278045\n",
      "Epoch 33, loss: 2.277447\n",
      "Epoch 34, loss: 2.276844\n",
      "Epoch 35, loss: 2.276228\n",
      "Epoch 36, loss: 2.275625\n",
      "Epoch 37, loss: 2.274996\n",
      "Epoch 38, loss: 2.274455\n",
      "Epoch 39, loss: 2.273822\n",
      "Epoch 40, loss: 2.273220\n",
      "Epoch 41, loss: 2.272638\n",
      "Epoch 42, loss: 2.272049\n",
      "Epoch 43, loss: 2.271479\n",
      "Epoch 44, loss: 2.270903\n",
      "Epoch 45, loss: 2.270333\n",
      "Epoch 46, loss: 2.269768\n",
      "Epoch 47, loss: 2.269205\n",
      "Epoch 48, loss: 2.268641\n",
      "Epoch 49, loss: 2.268076\n",
      "Epoch 50, loss: 2.267513\n",
      "Epoch 51, loss: 2.266955\n",
      "Epoch 52, loss: 2.266414\n",
      "Epoch 53, loss: 2.265866\n",
      "Epoch 54, loss: 2.265312\n",
      "Epoch 55, loss: 2.264781\n",
      "Epoch 56, loss: 2.264228\n",
      "Epoch 57, loss: 2.263694\n",
      "Epoch 58, loss: 2.263172\n",
      "Epoch 59, loss: 2.262653\n",
      "Epoch 60, loss: 2.262115\n",
      "Epoch 61, loss: 2.261579\n",
      "Epoch 62, loss: 2.261097\n",
      "Epoch 63, loss: 2.260548\n",
      "Epoch 64, loss: 2.260042\n",
      "Epoch 65, loss: 2.259518\n",
      "Epoch 66, loss: 2.259035\n",
      "Epoch 67, loss: 2.258536\n",
      "Epoch 68, loss: 2.258014\n",
      "Epoch 69, loss: 2.257509\n",
      "Epoch 70, loss: 2.257018\n",
      "Epoch 71, loss: 2.256521\n",
      "Epoch 72, loss: 2.256023\n",
      "Epoch 73, loss: 2.255526\n",
      "Epoch 74, loss: 2.255064\n",
      "Epoch 75, loss: 2.254552\n",
      "Epoch 76, loss: 2.254073\n",
      "Epoch 77, loss: 2.253598\n",
      "Epoch 78, loss: 2.253124\n",
      "Epoch 79, loss: 2.252674\n",
      "Epoch 80, loss: 2.252158\n",
      "Epoch 81, loss: 2.251695\n",
      "Epoch 82, loss: 2.251227\n",
      "Epoch 83, loss: 2.250768\n",
      "Epoch 84, loss: 2.250292\n",
      "Epoch 85, loss: 2.249852\n",
      "Epoch 86, loss: 2.249399\n",
      "Epoch 87, loss: 2.248938\n",
      "Epoch 88, loss: 2.248482\n",
      "Epoch 89, loss: 2.248029\n",
      "Epoch 90, loss: 2.247562\n",
      "Epoch 91, loss: 2.247110\n",
      "Epoch 92, loss: 2.246689\n",
      "Epoch 93, loss: 2.246221\n",
      "Epoch 94, loss: 2.245792\n",
      "Epoch 95, loss: 2.245363\n",
      "Epoch 96, loss: 2.244907\n",
      "Epoch 97, loss: 2.244475\n",
      "Epoch 98, loss: 2.244061\n",
      "Epoch 99, loss: 2.243616\n",
      "Epoch 100, loss: 2.243179\n",
      "Epoch 101, loss: 2.242769\n",
      "Epoch 102, loss: 2.242351\n",
      "Epoch 103, loss: 2.241931\n",
      "Epoch 104, loss: 2.241504\n",
      "Epoch 105, loss: 2.241103\n",
      "Epoch 106, loss: 2.240661\n",
      "Epoch 107, loss: 2.240273\n",
      "Epoch 108, loss: 2.239847\n",
      "Epoch 109, loss: 2.239437\n",
      "Epoch 110, loss: 2.239037\n",
      "Epoch 111, loss: 2.238631\n",
      "Epoch 112, loss: 2.238203\n",
      "Epoch 113, loss: 2.237821\n",
      "Epoch 114, loss: 2.237431\n",
      "Epoch 115, loss: 2.237025\n",
      "Epoch 116, loss: 2.236605\n",
      "Epoch 117, loss: 2.236214\n",
      "Epoch 118, loss: 2.235822\n",
      "Epoch 119, loss: 2.235435\n",
      "Epoch 120, loss: 2.235046\n",
      "Epoch 121, loss: 2.234680\n",
      "Epoch 122, loss: 2.234277\n",
      "Epoch 123, loss: 2.233884\n",
      "Epoch 124, loss: 2.233516\n",
      "Epoch 125, loss: 2.233129\n",
      "Epoch 126, loss: 2.232744\n",
      "Epoch 127, loss: 2.232366\n",
      "Epoch 128, loss: 2.232012\n",
      "Epoch 129, loss: 2.231635\n",
      "Epoch 130, loss: 2.231269\n",
      "Epoch 131, loss: 2.230871\n",
      "Epoch 132, loss: 2.230527\n",
      "Epoch 133, loss: 2.230155\n",
      "Epoch 134, loss: 2.229790\n",
      "Epoch 135, loss: 2.229421\n",
      "Epoch 136, loss: 2.229071\n",
      "Epoch 137, loss: 2.228725\n",
      "Epoch 138, loss: 2.228331\n",
      "Epoch 139, loss: 2.228000\n",
      "Epoch 140, loss: 2.227628\n",
      "Epoch 141, loss: 2.227288\n",
      "Epoch 142, loss: 2.226931\n",
      "Epoch 143, loss: 2.226587\n",
      "Epoch 144, loss: 2.226246\n",
      "Epoch 145, loss: 2.225891\n",
      "Epoch 146, loss: 2.225534\n",
      "Epoch 147, loss: 2.225202\n",
      "Epoch 148, loss: 2.224846\n",
      "Epoch 149, loss: 2.224524\n",
      "Epoch 150, loss: 2.224210\n",
      "Epoch 151, loss: 2.223827\n",
      "Epoch 152, loss: 2.223500\n",
      "Epoch 153, loss: 2.223157\n",
      "Epoch 154, loss: 2.222846\n",
      "Epoch 155, loss: 2.222512\n",
      "Epoch 156, loss: 2.222175\n",
      "Epoch 157, loss: 2.221842\n",
      "Epoch 158, loss: 2.221518\n",
      "Epoch 159, loss: 2.221211\n",
      "Epoch 160, loss: 2.220852\n",
      "Epoch 161, loss: 2.220539\n",
      "Epoch 162, loss: 2.220240\n",
      "Epoch 163, loss: 2.219910\n",
      "Epoch 164, loss: 2.219605\n",
      "Epoch 165, loss: 2.219270\n",
      "Epoch 166, loss: 2.218954\n",
      "Epoch 167, loss: 2.218641\n",
      "Epoch 168, loss: 2.218329\n",
      "Epoch 169, loss: 2.218019\n",
      "Epoch 170, loss: 2.217709\n",
      "Epoch 171, loss: 2.217409\n",
      "Epoch 172, loss: 2.217094\n",
      "Epoch 173, loss: 2.216782\n",
      "Epoch 174, loss: 2.216490\n",
      "Epoch 175, loss: 2.216170\n",
      "Epoch 176, loss: 2.215886\n",
      "Epoch 177, loss: 2.215583\n",
      "Epoch 178, loss: 2.215261\n",
      "Epoch 179, loss: 2.214978\n",
      "Epoch 180, loss: 2.214676\n",
      "Epoch 181, loss: 2.214387\n",
      "Epoch 182, loss: 2.214083\n",
      "Epoch 183, loss: 2.213788\n",
      "Epoch 184, loss: 2.213499\n",
      "Epoch 185, loss: 2.213221\n",
      "Epoch 186, loss: 2.212947\n",
      "Epoch 187, loss: 2.212646\n",
      "Epoch 188, loss: 2.212358\n",
      "Epoch 189, loss: 2.212069\n",
      "Epoch 190, loss: 2.211788\n",
      "Epoch 191, loss: 2.211489\n",
      "Epoch 192, loss: 2.211216\n",
      "Epoch 193, loss: 2.210945\n",
      "Epoch 194, loss: 2.210658\n",
      "Epoch 195, loss: 2.210387\n",
      "Epoch 196, loss: 2.210107\n",
      "Epoch 197, loss: 2.209847\n",
      "Epoch 198, loss: 2.209569\n",
      "Epoch 199, loss: 2.209283\n",
      "Epoch 0, loss: 2.214512\n",
      "Epoch 1, loss: 2.214135\n",
      "Epoch 2, loss: 2.213755\n",
      "Epoch 3, loss: 2.213411\n",
      "Epoch 4, loss: 2.213071\n",
      "Epoch 5, loss: 2.212742\n",
      "Epoch 6, loss: 2.212406\n",
      "Epoch 7, loss: 2.212101\n",
      "Epoch 8, loss: 2.211782\n",
      "Epoch 9, loss: 2.211484\n",
      "Epoch 10, loss: 2.211151\n",
      "Epoch 11, loss: 2.210868\n",
      "Epoch 12, loss: 2.210552\n",
      "Epoch 13, loss: 2.210260\n",
      "Epoch 14, loss: 2.209973\n",
      "Epoch 15, loss: 2.209668\n",
      "Epoch 16, loss: 2.209373\n",
      "Epoch 17, loss: 2.209062\n",
      "Epoch 18, loss: 2.208779\n",
      "Epoch 19, loss: 2.208505\n",
      "Epoch 20, loss: 2.208206\n",
      "Epoch 21, loss: 2.207922\n",
      "Epoch 22, loss: 2.207647\n",
      "Epoch 23, loss: 2.207350\n",
      "Epoch 24, loss: 2.207071\n",
      "Epoch 25, loss: 2.206804\n",
      "Epoch 26, loss: 2.206532\n",
      "Epoch 27, loss: 2.206249\n",
      "Epoch 28, loss: 2.205977\n",
      "Epoch 29, loss: 2.205713\n",
      "Epoch 30, loss: 2.205430\n",
      "Epoch 31, loss: 2.205168\n",
      "Epoch 32, loss: 2.204899\n",
      "Epoch 33, loss: 2.204647\n",
      "Epoch 34, loss: 2.204370\n",
      "Epoch 35, loss: 2.204111\n",
      "Epoch 36, loss: 2.203828\n",
      "Epoch 37, loss: 2.203578\n",
      "Epoch 38, loss: 2.203317\n",
      "Epoch 39, loss: 2.203052\n",
      "Epoch 40, loss: 2.202808\n",
      "Epoch 41, loss: 2.202560\n",
      "Epoch 42, loss: 2.202303\n",
      "Epoch 43, loss: 2.202018\n",
      "Epoch 44, loss: 2.201800\n",
      "Epoch 45, loss: 2.201542\n",
      "Epoch 46, loss: 2.201284\n",
      "Epoch 47, loss: 2.201050\n",
      "Epoch 48, loss: 2.200773\n",
      "Epoch 49, loss: 2.200530\n",
      "Epoch 50, loss: 2.200286\n",
      "Epoch 51, loss: 2.200046\n",
      "Epoch 52, loss: 2.199809\n",
      "Epoch 53, loss: 2.199547\n",
      "Epoch 54, loss: 2.199337\n",
      "Epoch 55, loss: 2.199105\n",
      "Epoch 56, loss: 2.198846\n",
      "Epoch 57, loss: 2.198630\n",
      "Epoch 58, loss: 2.198393\n",
      "Epoch 59, loss: 2.198156\n",
      "Epoch 60, loss: 2.197919\n",
      "Epoch 61, loss: 2.197678\n",
      "Epoch 62, loss: 2.197470\n",
      "Epoch 63, loss: 2.197196\n",
      "Epoch 64, loss: 2.196978\n",
      "Epoch 65, loss: 2.196773\n",
      "Epoch 66, loss: 2.196525\n",
      "Epoch 67, loss: 2.196301\n",
      "Epoch 68, loss: 2.196079\n",
      "Epoch 69, loss: 2.195852\n",
      "Epoch 70, loss: 2.195630\n",
      "Epoch 71, loss: 2.195423\n",
      "Epoch 72, loss: 2.195191\n",
      "Epoch 73, loss: 2.194973\n",
      "Epoch 74, loss: 2.194741\n",
      "Epoch 75, loss: 2.194539\n",
      "Epoch 76, loss: 2.194321\n",
      "Epoch 77, loss: 2.194077\n",
      "Epoch 78, loss: 2.193870\n",
      "Epoch 79, loss: 2.193669\n",
      "Epoch 80, loss: 2.193482\n",
      "Epoch 81, loss: 2.193228\n",
      "Epoch 82, loss: 2.193042\n",
      "Epoch 83, loss: 2.192828\n",
      "Epoch 84, loss: 2.192650\n",
      "Epoch 85, loss: 2.192393\n",
      "Epoch 86, loss: 2.192199\n",
      "Epoch 87, loss: 2.191998\n",
      "Epoch 88, loss: 2.191789\n",
      "Epoch 89, loss: 2.191559\n",
      "Epoch 90, loss: 2.191367\n",
      "Epoch 91, loss: 2.191163\n",
      "Epoch 92, loss: 2.190963\n",
      "Epoch 93, loss: 2.190762\n",
      "Epoch 94, loss: 2.190571\n",
      "Epoch 95, loss: 2.190367\n",
      "Epoch 96, loss: 2.190178\n",
      "Epoch 97, loss: 2.189959\n",
      "Epoch 98, loss: 2.189758\n",
      "Epoch 99, loss: 2.189597\n",
      "Epoch 100, loss: 2.189367\n",
      "Epoch 101, loss: 2.189177\n",
      "Epoch 102, loss: 2.188980\n",
      "Epoch 103, loss: 2.188774\n",
      "Epoch 104, loss: 2.188582\n",
      "Epoch 105, loss: 2.188412\n",
      "Epoch 106, loss: 2.188191\n",
      "Epoch 107, loss: 2.188005\n",
      "Epoch 108, loss: 2.187818\n",
      "Epoch 109, loss: 2.187631\n",
      "Epoch 110, loss: 2.187445\n",
      "Epoch 111, loss: 2.187266\n",
      "Epoch 112, loss: 2.187055\n",
      "Epoch 113, loss: 2.186897\n",
      "Epoch 114, loss: 2.186698\n",
      "Epoch 115, loss: 2.186511\n",
      "Epoch 116, loss: 2.186314\n",
      "Epoch 117, loss: 2.186123\n",
      "Epoch 118, loss: 2.185961\n",
      "Epoch 119, loss: 2.185793\n",
      "Epoch 120, loss: 2.185597\n",
      "Epoch 121, loss: 2.185413\n",
      "Epoch 122, loss: 2.185239\n",
      "Epoch 123, loss: 2.185057\n",
      "Epoch 124, loss: 2.184876\n",
      "Epoch 125, loss: 2.184698\n",
      "Epoch 126, loss: 2.184517\n",
      "Epoch 127, loss: 2.184347\n",
      "Epoch 128, loss: 2.184161\n",
      "Epoch 129, loss: 2.183989\n",
      "Epoch 130, loss: 2.183797\n",
      "Epoch 131, loss: 2.183622\n",
      "Epoch 132, loss: 2.183471\n",
      "Epoch 133, loss: 2.183274\n",
      "Epoch 134, loss: 2.183099\n",
      "Epoch 135, loss: 2.182941\n",
      "Epoch 136, loss: 2.182769\n",
      "Epoch 137, loss: 2.182625\n",
      "Epoch 138, loss: 2.182437\n",
      "Epoch 139, loss: 2.182266\n",
      "Epoch 140, loss: 2.182098\n",
      "Epoch 141, loss: 2.181911\n",
      "Epoch 142, loss: 2.181748\n",
      "Epoch 143, loss: 2.181589\n",
      "Epoch 144, loss: 2.181431\n",
      "Epoch 145, loss: 2.181268\n",
      "Epoch 146, loss: 2.181117\n",
      "Epoch 147, loss: 2.180925\n",
      "Epoch 148, loss: 2.180751\n",
      "Epoch 149, loss: 2.180596\n",
      "Epoch 150, loss: 2.180442\n",
      "Epoch 151, loss: 2.180283\n",
      "Epoch 152, loss: 2.180110\n",
      "Epoch 153, loss: 2.179952\n",
      "Epoch 154, loss: 2.179803\n",
      "Epoch 155, loss: 2.179630\n",
      "Epoch 156, loss: 2.179458\n",
      "Epoch 157, loss: 2.179321\n",
      "Epoch 158, loss: 2.179156\n",
      "Epoch 159, loss: 2.178986\n",
      "Epoch 160, loss: 2.178839\n",
      "Epoch 161, loss: 2.178692\n",
      "Epoch 162, loss: 2.178528\n",
      "Epoch 163, loss: 2.178352\n",
      "Epoch 164, loss: 2.178217\n",
      "Epoch 165, loss: 2.178079\n",
      "Epoch 166, loss: 2.177933\n",
      "Epoch 167, loss: 2.177764\n",
      "Epoch 168, loss: 2.177603\n",
      "Epoch 169, loss: 2.177444\n",
      "Epoch 170, loss: 2.177290\n",
      "Epoch 171, loss: 2.177137\n",
      "Epoch 172, loss: 2.176970\n",
      "Epoch 173, loss: 2.176831\n",
      "Epoch 174, loss: 2.176680\n",
      "Epoch 175, loss: 2.176534\n",
      "Epoch 176, loss: 2.176378\n",
      "Epoch 177, loss: 2.176253\n",
      "Epoch 178, loss: 2.176102\n",
      "Epoch 179, loss: 2.175950\n",
      "Epoch 180, loss: 2.175811\n",
      "Epoch 181, loss: 2.175657\n",
      "Epoch 182, loss: 2.175507\n",
      "Epoch 183, loss: 2.175350\n",
      "Epoch 184, loss: 2.175214\n",
      "Epoch 185, loss: 2.175055\n",
      "Epoch 186, loss: 2.174901\n",
      "Epoch 187, loss: 2.174773\n",
      "Epoch 188, loss: 2.174627\n",
      "Epoch 189, loss: 2.174490\n",
      "Epoch 190, loss: 2.174333\n",
      "Epoch 191, loss: 2.174214\n",
      "Epoch 192, loss: 2.174065\n",
      "Epoch 193, loss: 2.173916\n",
      "Epoch 194, loss: 2.173785\n",
      "Epoch 195, loss: 2.173639\n",
      "Epoch 196, loss: 2.173475\n",
      "Epoch 197, loss: 2.173346\n",
      "Epoch 198, loss: 2.173206\n",
      "Epoch 199, loss: 2.173100\n",
      "Epoch 0, loss: 2.182828\n",
      "Epoch 1, loss: 2.182590\n",
      "Epoch 2, loss: 2.182359\n",
      "Epoch 3, loss: 2.182160\n",
      "Epoch 4, loss: 2.181932\n",
      "Epoch 5, loss: 2.181743\n",
      "Epoch 6, loss: 2.181571\n",
      "Epoch 7, loss: 2.181367\n",
      "Epoch 8, loss: 2.181219\n",
      "Epoch 9, loss: 2.181052\n",
      "Epoch 10, loss: 2.180878\n",
      "Epoch 11, loss: 2.180728\n",
      "Epoch 12, loss: 2.180576\n",
      "Epoch 13, loss: 2.180414\n",
      "Epoch 14, loss: 2.180245\n",
      "Epoch 15, loss: 2.180113\n",
      "Epoch 16, loss: 2.179955\n",
      "Epoch 17, loss: 2.179807\n",
      "Epoch 18, loss: 2.179628\n",
      "Epoch 19, loss: 2.179495\n",
      "Epoch 20, loss: 2.179360\n",
      "Epoch 21, loss: 2.179219\n",
      "Epoch 22, loss: 2.179050\n",
      "Epoch 23, loss: 2.178911\n",
      "Epoch 24, loss: 2.178766\n",
      "Epoch 25, loss: 2.178623\n",
      "Epoch 26, loss: 2.178491\n",
      "Epoch 27, loss: 2.178359\n",
      "Epoch 28, loss: 2.178221\n",
      "Epoch 29, loss: 2.178066\n",
      "Epoch 30, loss: 2.177936\n",
      "Epoch 31, loss: 2.177799\n",
      "Epoch 32, loss: 2.177635\n",
      "Epoch 33, loss: 2.177513\n",
      "Epoch 34, loss: 2.177361\n",
      "Epoch 35, loss: 2.177243\n",
      "Epoch 36, loss: 2.177117\n",
      "Epoch 37, loss: 2.176971\n",
      "Epoch 38, loss: 2.176821\n",
      "Epoch 39, loss: 2.176703\n",
      "Epoch 40, loss: 2.176596\n",
      "Epoch 41, loss: 2.176426\n",
      "Epoch 42, loss: 2.176305\n",
      "Epoch 43, loss: 2.176176\n",
      "Epoch 44, loss: 2.176033\n",
      "Epoch 45, loss: 2.175916\n",
      "Epoch 46, loss: 2.175782\n",
      "Epoch 47, loss: 2.175656\n",
      "Epoch 48, loss: 2.175531\n",
      "Epoch 49, loss: 2.175390\n",
      "Epoch 50, loss: 2.175259\n",
      "Epoch 51, loss: 2.175132\n",
      "Epoch 52, loss: 2.174993\n",
      "Epoch 53, loss: 2.174869\n",
      "Epoch 54, loss: 2.174747\n",
      "Epoch 55, loss: 2.174628\n",
      "Epoch 56, loss: 2.174506\n",
      "Epoch 57, loss: 2.174373\n",
      "Epoch 58, loss: 2.174266\n",
      "Epoch 59, loss: 2.174125\n",
      "Epoch 60, loss: 2.173995\n",
      "Epoch 61, loss: 2.173878\n",
      "Epoch 62, loss: 2.173750\n",
      "Epoch 63, loss: 2.173625\n",
      "Epoch 64, loss: 2.173514\n",
      "Epoch 65, loss: 2.173371\n",
      "Epoch 66, loss: 2.173293\n",
      "Epoch 67, loss: 2.173156\n",
      "Epoch 68, loss: 2.173038\n",
      "Epoch 69, loss: 2.172911\n",
      "Epoch 70, loss: 2.172799\n",
      "Epoch 71, loss: 2.172665\n",
      "Epoch 72, loss: 2.172576\n",
      "Epoch 73, loss: 2.172434\n",
      "Epoch 74, loss: 2.172308\n",
      "Epoch 75, loss: 2.172201\n",
      "Epoch 76, loss: 2.172076\n",
      "Epoch 77, loss: 2.171967\n",
      "Epoch 78, loss: 2.171846\n",
      "Epoch 79, loss: 2.171749\n",
      "Epoch 80, loss: 2.171615\n",
      "Epoch 81, loss: 2.171504\n",
      "Epoch 82, loss: 2.171405\n",
      "Epoch 83, loss: 2.171266\n",
      "Epoch 84, loss: 2.171146\n",
      "Epoch 85, loss: 2.171055\n",
      "Epoch 86, loss: 2.170939\n",
      "Epoch 87, loss: 2.170822\n",
      "Epoch 88, loss: 2.170705\n",
      "Epoch 89, loss: 2.170604\n",
      "Epoch 90, loss: 2.170482\n",
      "Epoch 91, loss: 2.170386\n",
      "Epoch 92, loss: 2.170272\n",
      "Epoch 93, loss: 2.170166\n",
      "Epoch 94, loss: 2.170031\n",
      "Epoch 95, loss: 2.169942\n",
      "Epoch 96, loss: 2.169815\n",
      "Epoch 97, loss: 2.169712\n",
      "Epoch 98, loss: 2.169596\n",
      "Epoch 99, loss: 2.169487\n",
      "Epoch 100, loss: 2.169372\n",
      "Epoch 101, loss: 2.169271\n",
      "Epoch 102, loss: 2.169157\n",
      "Epoch 103, loss: 2.169030\n",
      "Epoch 104, loss: 2.168947\n",
      "Epoch 105, loss: 2.168835\n",
      "Epoch 106, loss: 2.168733\n",
      "Epoch 107, loss: 2.168614\n",
      "Epoch 108, loss: 2.168513\n",
      "Epoch 109, loss: 2.168415\n",
      "Epoch 110, loss: 2.168276\n",
      "Epoch 111, loss: 2.168192\n",
      "Epoch 112, loss: 2.168089\n",
      "Epoch 113, loss: 2.167992\n",
      "Epoch 114, loss: 2.167886\n",
      "Epoch 115, loss: 2.167801\n",
      "Epoch 116, loss: 2.167659\n",
      "Epoch 117, loss: 2.167559\n",
      "Epoch 118, loss: 2.167461\n",
      "Epoch 119, loss: 2.167360\n",
      "Epoch 120, loss: 2.167259\n",
      "Epoch 121, loss: 2.167154\n",
      "Epoch 122, loss: 2.167051\n",
      "Epoch 123, loss: 2.166940\n",
      "Epoch 124, loss: 2.166837\n",
      "Epoch 125, loss: 2.166752\n",
      "Epoch 126, loss: 2.166627\n",
      "Epoch 127, loss: 2.166539\n",
      "Epoch 128, loss: 2.166419\n",
      "Epoch 129, loss: 2.166334\n",
      "Epoch 130, loss: 2.166240\n",
      "Epoch 131, loss: 2.166128\n",
      "Epoch 132, loss: 2.166029\n",
      "Epoch 133, loss: 2.165936\n",
      "Epoch 134, loss: 2.165810\n",
      "Epoch 135, loss: 2.165714\n",
      "Epoch 136, loss: 2.165617\n",
      "Epoch 137, loss: 2.165518\n",
      "Epoch 138, loss: 2.165432\n",
      "Epoch 139, loss: 2.165349\n",
      "Epoch 140, loss: 2.165238\n",
      "Epoch 141, loss: 2.165119\n",
      "Epoch 142, loss: 2.165041\n",
      "Epoch 143, loss: 2.164948\n",
      "Epoch 144, loss: 2.164854\n",
      "Epoch 145, loss: 2.164752\n",
      "Epoch 146, loss: 2.164640\n",
      "Epoch 147, loss: 2.164557\n",
      "Epoch 148, loss: 2.164462\n",
      "Epoch 149, loss: 2.164357\n",
      "Epoch 150, loss: 2.164249\n",
      "Epoch 151, loss: 2.164153\n",
      "Epoch 152, loss: 2.164067\n",
      "Epoch 153, loss: 2.163976\n",
      "Epoch 154, loss: 2.163883\n",
      "Epoch 155, loss: 2.163764\n",
      "Epoch 156, loss: 2.163675\n",
      "Epoch 157, loss: 2.163587\n",
      "Epoch 158, loss: 2.163513\n",
      "Epoch 159, loss: 2.163412\n",
      "Epoch 160, loss: 2.163294\n",
      "Epoch 161, loss: 2.163215\n",
      "Epoch 162, loss: 2.163107\n",
      "Epoch 163, loss: 2.163031\n",
      "Epoch 164, loss: 2.162935\n",
      "Epoch 165, loss: 2.162847\n",
      "Epoch 166, loss: 2.162743\n",
      "Epoch 167, loss: 2.162672\n",
      "Epoch 168, loss: 2.162537\n",
      "Epoch 169, loss: 2.162466\n",
      "Epoch 170, loss: 2.162370\n",
      "Epoch 171, loss: 2.162289\n",
      "Epoch 172, loss: 2.162203\n",
      "Epoch 173, loss: 2.162101\n",
      "Epoch 174, loss: 2.162011\n",
      "Epoch 175, loss: 2.161941\n",
      "Epoch 176, loss: 2.161838\n",
      "Epoch 177, loss: 2.161747\n",
      "Epoch 178, loss: 2.161653\n",
      "Epoch 179, loss: 2.161559\n",
      "Epoch 180, loss: 2.161461\n",
      "Epoch 181, loss: 2.161383\n",
      "Epoch 182, loss: 2.161268\n",
      "Epoch 183, loss: 2.161208\n",
      "Epoch 184, loss: 2.161108\n",
      "Epoch 185, loss: 2.161019\n",
      "Epoch 186, loss: 2.160922\n",
      "Epoch 187, loss: 2.160857\n",
      "Epoch 188, loss: 2.160751\n",
      "Epoch 189, loss: 2.160661\n",
      "Epoch 190, loss: 2.160589\n",
      "Epoch 191, loss: 2.160489\n",
      "Epoch 192, loss: 2.160405\n",
      "Epoch 193, loss: 2.160312\n",
      "Epoch 194, loss: 2.160231\n",
      "Epoch 195, loss: 2.160137\n",
      "Epoch 196, loss: 2.160041\n",
      "Epoch 197, loss: 2.159973\n",
      "Epoch 198, loss: 2.159869\n",
      "Epoch 199, loss: 2.159793\n",
      "Epoch 0, loss: 2.158583\n",
      "Epoch 1, loss: 2.158382\n",
      "Epoch 2, loss: 2.158167\n",
      "Epoch 3, loss: 2.158022\n",
      "Epoch 4, loss: 2.157853\n",
      "Epoch 5, loss: 2.157705\n",
      "Epoch 6, loss: 2.157558\n",
      "Epoch 7, loss: 2.157418\n",
      "Epoch 8, loss: 2.157272\n",
      "Epoch 9, loss: 2.157136\n",
      "Epoch 10, loss: 2.157016\n",
      "Epoch 11, loss: 2.156888\n",
      "Epoch 12, loss: 2.156801\n",
      "Epoch 13, loss: 2.156679\n",
      "Epoch 14, loss: 2.156552\n",
      "Epoch 15, loss: 2.156428\n",
      "Epoch 16, loss: 2.156342\n",
      "Epoch 17, loss: 2.156209\n",
      "Epoch 18, loss: 2.156077\n",
      "Epoch 19, loss: 2.155976\n",
      "Epoch 20, loss: 2.155861\n",
      "Epoch 21, loss: 2.155751\n",
      "Epoch 22, loss: 2.155628\n",
      "Epoch 23, loss: 2.155551\n",
      "Epoch 24, loss: 2.155440\n",
      "Epoch 25, loss: 2.155310\n",
      "Epoch 26, loss: 2.155205\n",
      "Epoch 27, loss: 2.155106\n",
      "Epoch 28, loss: 2.155009\n",
      "Epoch 29, loss: 2.154876\n",
      "Epoch 30, loss: 2.154771\n",
      "Epoch 31, loss: 2.154666\n",
      "Epoch 32, loss: 2.154569\n",
      "Epoch 33, loss: 2.154457\n",
      "Epoch 34, loss: 2.154369\n",
      "Epoch 35, loss: 2.154270\n",
      "Epoch 36, loss: 2.154183\n",
      "Epoch 37, loss: 2.154061\n",
      "Epoch 38, loss: 2.153953\n",
      "Epoch 39, loss: 2.153866\n",
      "Epoch 40, loss: 2.153748\n",
      "Epoch 41, loss: 2.153642\n",
      "Epoch 42, loss: 2.153558\n",
      "Epoch 43, loss: 2.153464\n",
      "Epoch 44, loss: 2.153346\n",
      "Epoch 45, loss: 2.153258\n",
      "Epoch 46, loss: 2.153172\n",
      "Epoch 47, loss: 2.153065\n",
      "Epoch 48, loss: 2.152953\n",
      "Epoch 49, loss: 2.152858\n",
      "Epoch 50, loss: 2.152773\n",
      "Epoch 51, loss: 2.152683\n",
      "Epoch 52, loss: 2.152584\n",
      "Epoch 53, loss: 2.152489\n",
      "Epoch 54, loss: 2.152391\n",
      "Epoch 55, loss: 2.152300\n",
      "Epoch 56, loss: 2.152201\n",
      "Epoch 57, loss: 2.152090\n",
      "Epoch 58, loss: 2.152009\n",
      "Epoch 59, loss: 2.151929\n",
      "Epoch 60, loss: 2.151816\n",
      "Epoch 61, loss: 2.151729\n",
      "Epoch 62, loss: 2.151644\n",
      "Epoch 63, loss: 2.151549\n",
      "Epoch 64, loss: 2.151448\n",
      "Epoch 65, loss: 2.151357\n",
      "Epoch 66, loss: 2.151256\n",
      "Epoch 67, loss: 2.151185\n",
      "Epoch 68, loss: 2.151110\n",
      "Epoch 69, loss: 2.151018\n",
      "Epoch 70, loss: 2.150894\n",
      "Epoch 71, loss: 2.150803\n",
      "Epoch 72, loss: 2.150730\n",
      "Epoch 73, loss: 2.150634\n",
      "Epoch 74, loss: 2.150544\n",
      "Epoch 75, loss: 2.150450\n",
      "Epoch 76, loss: 2.150388\n",
      "Epoch 77, loss: 2.150277\n",
      "Epoch 78, loss: 2.150186\n",
      "Epoch 79, loss: 2.150106\n",
      "Epoch 80, loss: 2.150018\n",
      "Epoch 81, loss: 2.149927\n",
      "Epoch 82, loss: 2.149834\n",
      "Epoch 83, loss: 2.149766\n",
      "Epoch 84, loss: 2.149683\n",
      "Epoch 85, loss: 2.149578\n",
      "Epoch 86, loss: 2.149505\n",
      "Epoch 87, loss: 2.149413\n",
      "Epoch 88, loss: 2.149328\n",
      "Epoch 89, loss: 2.149243\n",
      "Epoch 90, loss: 2.149156\n",
      "Epoch 91, loss: 2.149082\n",
      "Epoch 92, loss: 2.148984\n",
      "Epoch 93, loss: 2.148903\n",
      "Epoch 94, loss: 2.148812\n",
      "Epoch 95, loss: 2.148744\n",
      "Epoch 96, loss: 2.148648\n",
      "Epoch 97, loss: 2.148582\n",
      "Epoch 98, loss: 2.148504\n",
      "Epoch 99, loss: 2.148397\n",
      "Epoch 100, loss: 2.148340\n",
      "Epoch 101, loss: 2.148245\n",
      "Epoch 102, loss: 2.148152\n",
      "Epoch 103, loss: 2.148080\n",
      "Epoch 104, loss: 2.147988\n",
      "Epoch 105, loss: 2.147942\n",
      "Epoch 106, loss: 2.147820\n",
      "Epoch 107, loss: 2.147744\n",
      "Epoch 108, loss: 2.147670\n",
      "Epoch 109, loss: 2.147590\n",
      "Epoch 110, loss: 2.147510\n",
      "Epoch 111, loss: 2.147419\n",
      "Epoch 112, loss: 2.147337\n",
      "Epoch 113, loss: 2.147271\n",
      "Epoch 114, loss: 2.147177\n",
      "Epoch 115, loss: 2.147096\n",
      "Epoch 116, loss: 2.147041\n",
      "Epoch 117, loss: 2.146950\n",
      "Epoch 118, loss: 2.146866\n",
      "Epoch 119, loss: 2.146783\n",
      "Epoch 120, loss: 2.146711\n",
      "Epoch 121, loss: 2.146620\n",
      "Epoch 122, loss: 2.146573\n",
      "Epoch 123, loss: 2.146472\n",
      "Epoch 124, loss: 2.146398\n",
      "Epoch 125, loss: 2.146326\n",
      "Epoch 126, loss: 2.146248\n",
      "Epoch 127, loss: 2.146165\n",
      "Epoch 128, loss: 2.146097\n",
      "Epoch 129, loss: 2.146008\n",
      "Epoch 130, loss: 2.145926\n",
      "Epoch 131, loss: 2.145860\n",
      "Epoch 132, loss: 2.145791\n",
      "Epoch 133, loss: 2.145711\n",
      "Epoch 134, loss: 2.145632\n",
      "Epoch 135, loss: 2.145568\n",
      "Epoch 136, loss: 2.145473\n",
      "Epoch 137, loss: 2.145409\n",
      "Epoch 138, loss: 2.145318\n",
      "Epoch 139, loss: 2.145249\n",
      "Epoch 140, loss: 2.145180\n",
      "Epoch 141, loss: 2.145088\n",
      "Epoch 142, loss: 2.145019\n",
      "Epoch 143, loss: 2.144964\n",
      "Epoch 144, loss: 2.144885\n",
      "Epoch 145, loss: 2.144795\n",
      "Epoch 146, loss: 2.144723\n",
      "Epoch 147, loss: 2.144662\n",
      "Epoch 148, loss: 2.144592\n",
      "Epoch 149, loss: 2.144514\n",
      "Epoch 150, loss: 2.144424\n",
      "Epoch 151, loss: 2.144353\n",
      "Epoch 152, loss: 2.144280\n",
      "Epoch 153, loss: 2.144214\n",
      "Epoch 154, loss: 2.144128\n",
      "Epoch 155, loss: 2.144067\n",
      "Epoch 156, loss: 2.143984\n",
      "Epoch 157, loss: 2.143908\n",
      "Epoch 158, loss: 2.143856\n",
      "Epoch 159, loss: 2.143766\n",
      "Epoch 160, loss: 2.143710\n",
      "Epoch 161, loss: 2.143635\n",
      "Epoch 162, loss: 2.143547\n",
      "Epoch 163, loss: 2.143498\n",
      "Epoch 164, loss: 2.143446\n",
      "Epoch 165, loss: 2.143358\n",
      "Epoch 166, loss: 2.143270\n",
      "Epoch 167, loss: 2.143199\n",
      "Epoch 168, loss: 2.143143\n",
      "Epoch 169, loss: 2.143074\n",
      "Epoch 170, loss: 2.143002\n",
      "Epoch 171, loss: 2.142926\n",
      "Epoch 172, loss: 2.142869\n",
      "Epoch 173, loss: 2.142804\n",
      "Epoch 174, loss: 2.142711\n",
      "Epoch 175, loss: 2.142659\n",
      "Epoch 176, loss: 2.142578\n",
      "Epoch 177, loss: 2.142516\n",
      "Epoch 178, loss: 2.142446\n",
      "Epoch 179, loss: 2.142363\n",
      "Epoch 180, loss: 2.142309\n",
      "Epoch 181, loss: 2.142224\n",
      "Epoch 182, loss: 2.142160\n",
      "Epoch 183, loss: 2.142084\n",
      "Epoch 184, loss: 2.142013\n",
      "Epoch 185, loss: 2.141947\n",
      "Epoch 186, loss: 2.141900\n",
      "Epoch 187, loss: 2.141829\n",
      "Epoch 188, loss: 2.141741\n",
      "Epoch 189, loss: 2.141693\n",
      "Epoch 190, loss: 2.141623\n",
      "Epoch 191, loss: 2.141567\n",
      "Epoch 192, loss: 2.141490\n",
      "Epoch 193, loss: 2.141426\n",
      "Epoch 194, loss: 2.141331\n",
      "Epoch 195, loss: 2.141288\n",
      "Epoch 196, loss: 2.141217\n",
      "Epoch 197, loss: 2.141144\n",
      "Epoch 198, loss: 2.141095\n",
      "Epoch 199, loss: 2.141013\n",
      "Epoch 0, loss: 2.148001\n",
      "Epoch 1, loss: 2.147828\n",
      "Epoch 2, loss: 2.147676\n",
      "Epoch 3, loss: 2.147550\n",
      "Epoch 4, loss: 2.147446\n",
      "Epoch 5, loss: 2.147329\n",
      "Epoch 6, loss: 2.147203\n",
      "Epoch 7, loss: 2.147125\n",
      "Epoch 8, loss: 2.147010\n",
      "Epoch 9, loss: 2.146915\n",
      "Epoch 10, loss: 2.146830\n",
      "Epoch 11, loss: 2.146746\n",
      "Epoch 12, loss: 2.146641\n",
      "Epoch 13, loss: 2.146552\n",
      "Epoch 14, loss: 2.146455\n",
      "Epoch 15, loss: 2.146370\n",
      "Epoch 16, loss: 2.146275\n",
      "Epoch 17, loss: 2.146207\n",
      "Epoch 18, loss: 2.146106\n",
      "Epoch 19, loss: 2.146023\n",
      "Epoch 20, loss: 2.145940\n",
      "Epoch 21, loss: 2.145854\n",
      "Epoch 22, loss: 2.145760\n",
      "Epoch 23, loss: 2.145677\n",
      "Epoch 24, loss: 2.145605\n",
      "Epoch 25, loss: 2.145515\n",
      "Epoch 26, loss: 2.145443\n",
      "Epoch 27, loss: 2.145354\n",
      "Epoch 28, loss: 2.145273\n",
      "Epoch 29, loss: 2.145185\n",
      "Epoch 30, loss: 2.145109\n",
      "Epoch 31, loss: 2.145025\n",
      "Epoch 32, loss: 2.144935\n",
      "Epoch 33, loss: 2.144843\n",
      "Epoch 34, loss: 2.144783\n",
      "Epoch 35, loss: 2.144694\n",
      "Epoch 36, loss: 2.144615\n",
      "Epoch 37, loss: 2.144511\n",
      "Epoch 38, loss: 2.144469\n",
      "Epoch 39, loss: 2.144369\n",
      "Epoch 40, loss: 2.144287\n",
      "Epoch 41, loss: 2.144216\n",
      "Epoch 42, loss: 2.144144\n",
      "Epoch 43, loss: 2.144047\n",
      "Epoch 44, loss: 2.143998\n",
      "Epoch 45, loss: 2.143918\n",
      "Epoch 46, loss: 2.143840\n",
      "Epoch 47, loss: 2.143756\n",
      "Epoch 48, loss: 2.143681\n",
      "Epoch 49, loss: 2.143596\n",
      "Epoch 50, loss: 2.143533\n",
      "Epoch 51, loss: 2.143449\n",
      "Epoch 52, loss: 2.143371\n",
      "Epoch 53, loss: 2.143282\n",
      "Epoch 54, loss: 2.143232\n",
      "Epoch 55, loss: 2.143158\n",
      "Epoch 56, loss: 2.143082\n",
      "Epoch 57, loss: 2.143000\n",
      "Epoch 58, loss: 2.142919\n",
      "Epoch 59, loss: 2.142881\n",
      "Epoch 60, loss: 2.142779\n",
      "Epoch 61, loss: 2.142704\n",
      "Epoch 62, loss: 2.142612\n",
      "Epoch 63, loss: 2.142563\n",
      "Epoch 64, loss: 2.142489\n",
      "Epoch 65, loss: 2.142419\n",
      "Epoch 66, loss: 2.142346\n",
      "Epoch 67, loss: 2.142265\n",
      "Epoch 68, loss: 2.142201\n",
      "Epoch 69, loss: 2.142128\n",
      "Epoch 70, loss: 2.142051\n",
      "Epoch 71, loss: 2.142000\n",
      "Epoch 72, loss: 2.141910\n",
      "Epoch 73, loss: 2.141873\n",
      "Epoch 74, loss: 2.141760\n",
      "Epoch 75, loss: 2.141695\n",
      "Epoch 76, loss: 2.141661\n",
      "Epoch 77, loss: 2.141564\n",
      "Epoch 78, loss: 2.141511\n",
      "Epoch 79, loss: 2.141421\n",
      "Epoch 80, loss: 2.141354\n",
      "Epoch 81, loss: 2.141293\n",
      "Epoch 82, loss: 2.141243\n",
      "Epoch 83, loss: 2.141142\n",
      "Epoch 84, loss: 2.141100\n",
      "Epoch 85, loss: 2.141016\n",
      "Epoch 86, loss: 2.140973\n",
      "Epoch 87, loss: 2.140890\n",
      "Epoch 88, loss: 2.140833\n",
      "Epoch 89, loss: 2.140760\n",
      "Epoch 90, loss: 2.140679\n",
      "Epoch 91, loss: 2.140613\n",
      "Epoch 92, loss: 2.140549\n",
      "Epoch 93, loss: 2.140475\n",
      "Epoch 94, loss: 2.140429\n",
      "Epoch 95, loss: 2.140350\n",
      "Epoch 96, loss: 2.140295\n",
      "Epoch 97, loss: 2.140223\n",
      "Epoch 98, loss: 2.140146\n",
      "Epoch 99, loss: 2.140084\n",
      "Epoch 100, loss: 2.140015\n",
      "Epoch 101, loss: 2.139946\n",
      "Epoch 102, loss: 2.139899\n",
      "Epoch 103, loss: 2.139820\n",
      "Epoch 104, loss: 2.139768\n",
      "Epoch 105, loss: 2.139712\n",
      "Epoch 106, loss: 2.139637\n",
      "Epoch 107, loss: 2.139573\n",
      "Epoch 108, loss: 2.139493\n",
      "Epoch 109, loss: 2.139426\n",
      "Epoch 110, loss: 2.139368\n",
      "Epoch 111, loss: 2.139301\n",
      "Epoch 112, loss: 2.139265\n",
      "Epoch 113, loss: 2.139198\n",
      "Epoch 114, loss: 2.139132\n",
      "Epoch 115, loss: 2.139054\n",
      "Epoch 116, loss: 2.139015\n",
      "Epoch 117, loss: 2.138939\n",
      "Epoch 118, loss: 2.138868\n",
      "Epoch 119, loss: 2.138806\n",
      "Epoch 120, loss: 2.138754\n",
      "Epoch 121, loss: 2.138678\n",
      "Epoch 122, loss: 2.138621\n",
      "Epoch 123, loss: 2.138549\n",
      "Epoch 124, loss: 2.138504\n",
      "Epoch 125, loss: 2.138455\n",
      "Epoch 126, loss: 2.138360\n",
      "Epoch 127, loss: 2.138292\n",
      "Epoch 128, loss: 2.138230\n",
      "Epoch 129, loss: 2.138190\n",
      "Epoch 130, loss: 2.138140\n",
      "Epoch 131, loss: 2.138053\n",
      "Epoch 132, loss: 2.138008\n",
      "Epoch 133, loss: 2.137947\n",
      "Epoch 134, loss: 2.137880\n",
      "Epoch 135, loss: 2.137816\n",
      "Epoch 136, loss: 2.137766\n",
      "Epoch 137, loss: 2.137707\n",
      "Epoch 138, loss: 2.137622\n",
      "Epoch 139, loss: 2.137576\n",
      "Epoch 140, loss: 2.137523\n",
      "Epoch 141, loss: 2.137470\n",
      "Epoch 142, loss: 2.137391\n",
      "Epoch 143, loss: 2.137350\n",
      "Epoch 144, loss: 2.137269\n",
      "Epoch 145, loss: 2.137226\n",
      "Epoch 146, loss: 2.137151\n",
      "Epoch 147, loss: 2.137106\n",
      "Epoch 148, loss: 2.137026\n",
      "Epoch 149, loss: 2.136986\n",
      "Epoch 150, loss: 2.136939\n",
      "Epoch 151, loss: 2.136855\n",
      "Epoch 152, loss: 2.136810\n",
      "Epoch 153, loss: 2.136740\n",
      "Epoch 154, loss: 2.136708\n",
      "Epoch 155, loss: 2.136631\n",
      "Epoch 156, loss: 2.136587\n",
      "Epoch 157, loss: 2.136524\n",
      "Epoch 158, loss: 2.136465\n",
      "Epoch 159, loss: 2.136383\n",
      "Epoch 160, loss: 2.136347\n",
      "Epoch 161, loss: 2.136281\n",
      "Epoch 162, loss: 2.136236\n",
      "Epoch 163, loss: 2.136163\n",
      "Epoch 164, loss: 2.136111\n",
      "Epoch 165, loss: 2.136058\n",
      "Epoch 166, loss: 2.135985\n",
      "Epoch 167, loss: 2.135929\n",
      "Epoch 168, loss: 2.135884\n",
      "Epoch 169, loss: 2.135819\n",
      "Epoch 170, loss: 2.135761\n",
      "Epoch 171, loss: 2.135695\n",
      "Epoch 172, loss: 2.135641\n",
      "Epoch 173, loss: 2.135588\n",
      "Epoch 174, loss: 2.135539\n",
      "Epoch 175, loss: 2.135474\n",
      "Epoch 176, loss: 2.135430\n",
      "Epoch 177, loss: 2.135348\n",
      "Epoch 178, loss: 2.135295\n",
      "Epoch 179, loss: 2.135252\n",
      "Epoch 180, loss: 2.135198\n",
      "Epoch 181, loss: 2.135139\n",
      "Epoch 182, loss: 2.135102\n",
      "Epoch 183, loss: 2.135020\n",
      "Epoch 184, loss: 2.134952\n",
      "Epoch 185, loss: 2.134910\n",
      "Epoch 186, loss: 2.134872\n",
      "Epoch 187, loss: 2.134821\n",
      "Epoch 188, loss: 2.134746\n",
      "Epoch 189, loss: 2.134695\n",
      "Epoch 190, loss: 2.134657\n",
      "Epoch 191, loss: 2.134592\n",
      "Epoch 192, loss: 2.134512\n",
      "Epoch 193, loss: 2.134457\n",
      "Epoch 194, loss: 2.134416\n",
      "Epoch 195, loss: 2.134377\n",
      "Epoch 196, loss: 2.134317\n",
      "Epoch 197, loss: 2.134250\n",
      "Epoch 198, loss: 2.134196\n",
      "Epoch 199, loss: 2.134170\n",
      "Epoch 0, loss: 2.136124\n",
      "Epoch 1, loss: 2.136035\n",
      "Epoch 2, loss: 2.135951\n",
      "Epoch 3, loss: 2.135879\n",
      "Epoch 4, loss: 2.135811\n",
      "Epoch 5, loss: 2.135735\n",
      "Epoch 6, loss: 2.135681\n",
      "Epoch 7, loss: 2.135633\n",
      "Epoch 8, loss: 2.135559\n",
      "Epoch 9, loss: 2.135503\n",
      "Epoch 10, loss: 2.135443\n",
      "Epoch 11, loss: 2.135375\n",
      "Epoch 12, loss: 2.135324\n",
      "Epoch 13, loss: 2.135262\n",
      "Epoch 14, loss: 2.135215\n",
      "Epoch 15, loss: 2.135166\n",
      "Epoch 16, loss: 2.135089\n",
      "Epoch 17, loss: 2.135037\n",
      "Epoch 18, loss: 2.134983\n",
      "Epoch 19, loss: 2.134938\n",
      "Epoch 20, loss: 2.134887\n",
      "Epoch 21, loss: 2.134822\n",
      "Epoch 22, loss: 2.134779\n",
      "Epoch 23, loss: 2.134715\n",
      "Epoch 24, loss: 2.134653\n",
      "Epoch 25, loss: 2.134603\n",
      "Epoch 26, loss: 2.134564\n",
      "Epoch 27, loss: 2.134510\n",
      "Epoch 28, loss: 2.134444\n",
      "Epoch 29, loss: 2.134403\n",
      "Epoch 30, loss: 2.134334\n",
      "Epoch 31, loss: 2.134299\n",
      "Epoch 32, loss: 2.134231\n",
      "Epoch 33, loss: 2.134171\n",
      "Epoch 34, loss: 2.134135\n",
      "Epoch 35, loss: 2.134056\n",
      "Epoch 36, loss: 2.134021\n",
      "Epoch 37, loss: 2.133970\n",
      "Epoch 38, loss: 2.133919\n",
      "Epoch 39, loss: 2.133868\n",
      "Epoch 40, loss: 2.133818\n",
      "Epoch 41, loss: 2.133759\n",
      "Epoch 42, loss: 2.133703\n",
      "Epoch 43, loss: 2.133657\n",
      "Epoch 44, loss: 2.133612\n",
      "Epoch 45, loss: 2.133555\n",
      "Epoch 46, loss: 2.133515\n",
      "Epoch 47, loss: 2.133461\n",
      "Epoch 48, loss: 2.133392\n",
      "Epoch 49, loss: 2.133358\n",
      "Epoch 50, loss: 2.133308\n",
      "Epoch 51, loss: 2.133261\n",
      "Epoch 52, loss: 2.133211\n",
      "Epoch 53, loss: 2.133158\n",
      "Epoch 54, loss: 2.133092\n",
      "Epoch 55, loss: 2.133072\n",
      "Epoch 56, loss: 2.133007\n",
      "Epoch 57, loss: 2.132957\n",
      "Epoch 58, loss: 2.132909\n",
      "Epoch 59, loss: 2.132864\n",
      "Epoch 60, loss: 2.132791\n",
      "Epoch 61, loss: 2.132768\n",
      "Epoch 62, loss: 2.132704\n",
      "Epoch 63, loss: 2.132664\n",
      "Epoch 64, loss: 2.132626\n",
      "Epoch 65, loss: 2.132555\n",
      "Epoch 66, loss: 2.132529\n",
      "Epoch 67, loss: 2.132466\n",
      "Epoch 68, loss: 2.132433\n",
      "Epoch 69, loss: 2.132397\n",
      "Epoch 70, loss: 2.132342\n",
      "Epoch 71, loss: 2.132279\n",
      "Epoch 72, loss: 2.132213\n",
      "Epoch 73, loss: 2.132184\n",
      "Epoch 74, loss: 2.132132\n",
      "Epoch 75, loss: 2.132070\n",
      "Epoch 76, loss: 2.132027\n",
      "Epoch 77, loss: 2.131992\n",
      "Epoch 78, loss: 2.131944\n",
      "Epoch 79, loss: 2.131899\n",
      "Epoch 80, loss: 2.131846\n",
      "Epoch 81, loss: 2.131798\n",
      "Epoch 82, loss: 2.131736\n",
      "Epoch 83, loss: 2.131707\n",
      "Epoch 84, loss: 2.131668\n",
      "Epoch 85, loss: 2.131632\n",
      "Epoch 86, loss: 2.131566\n",
      "Epoch 87, loss: 2.131512\n",
      "Epoch 88, loss: 2.131468\n",
      "Epoch 89, loss: 2.131430\n",
      "Epoch 90, loss: 2.131373\n",
      "Epoch 91, loss: 2.131325\n",
      "Epoch 92, loss: 2.131288\n",
      "Epoch 93, loss: 2.131248\n",
      "Epoch 94, loss: 2.131205\n",
      "Epoch 95, loss: 2.131147\n",
      "Epoch 96, loss: 2.131088\n",
      "Epoch 97, loss: 2.131061\n",
      "Epoch 98, loss: 2.131014\n",
      "Epoch 99, loss: 2.130965\n",
      "Epoch 100, loss: 2.130917\n",
      "Epoch 101, loss: 2.130883\n",
      "Epoch 102, loss: 2.130811\n",
      "Epoch 103, loss: 2.130769\n",
      "Epoch 104, loss: 2.130742\n",
      "Epoch 105, loss: 2.130708\n",
      "Epoch 106, loss: 2.130647\n",
      "Epoch 107, loss: 2.130602\n",
      "Epoch 108, loss: 2.130563\n",
      "Epoch 109, loss: 2.130512\n",
      "Epoch 110, loss: 2.130461\n",
      "Epoch 111, loss: 2.130418\n",
      "Epoch 112, loss: 2.130403\n",
      "Epoch 113, loss: 2.130323\n",
      "Epoch 114, loss: 2.130284\n",
      "Epoch 115, loss: 2.130245\n",
      "Epoch 116, loss: 2.130195\n",
      "Epoch 117, loss: 2.130147\n",
      "Epoch 118, loss: 2.130121\n",
      "Epoch 119, loss: 2.130061\n",
      "Epoch 120, loss: 2.130035\n",
      "Epoch 121, loss: 2.129975\n",
      "Epoch 122, loss: 2.129930\n",
      "Epoch 123, loss: 2.129888\n",
      "Epoch 124, loss: 2.129849\n",
      "Epoch 125, loss: 2.129817\n",
      "Epoch 126, loss: 2.129742\n",
      "Epoch 127, loss: 2.129713\n",
      "Epoch 128, loss: 2.129673\n",
      "Epoch 129, loss: 2.129624\n",
      "Epoch 130, loss: 2.129574\n",
      "Epoch 131, loss: 2.129531\n",
      "Epoch 132, loss: 2.129509\n",
      "Epoch 133, loss: 2.129438\n",
      "Epoch 134, loss: 2.129392\n",
      "Epoch 135, loss: 2.129356\n",
      "Epoch 136, loss: 2.129313\n",
      "Epoch 137, loss: 2.129252\n",
      "Epoch 138, loss: 2.129216\n",
      "Epoch 139, loss: 2.129184\n",
      "Epoch 140, loss: 2.129141\n",
      "Epoch 141, loss: 2.129108\n",
      "Epoch 142, loss: 2.129071\n",
      "Epoch 143, loss: 2.128990\n",
      "Epoch 144, loss: 2.128966\n",
      "Epoch 145, loss: 2.128912\n",
      "Epoch 146, loss: 2.128871\n",
      "Epoch 147, loss: 2.128832\n",
      "Epoch 148, loss: 2.128801\n",
      "Epoch 149, loss: 2.128739\n",
      "Epoch 150, loss: 2.128726\n",
      "Epoch 151, loss: 2.128675\n",
      "Epoch 152, loss: 2.128640\n",
      "Epoch 153, loss: 2.128589\n",
      "Epoch 154, loss: 2.128536\n",
      "Epoch 155, loss: 2.128514\n",
      "Epoch 156, loss: 2.128452\n",
      "Epoch 157, loss: 2.128410\n",
      "Epoch 158, loss: 2.128386\n",
      "Epoch 159, loss: 2.128329\n",
      "Epoch 160, loss: 2.128291\n",
      "Epoch 161, loss: 2.128247\n",
      "Epoch 162, loss: 2.128204\n",
      "Epoch 163, loss: 2.128157\n",
      "Epoch 164, loss: 2.128108\n",
      "Epoch 165, loss: 2.128109\n",
      "Epoch 166, loss: 2.128028\n",
      "Epoch 167, loss: 2.128009\n",
      "Epoch 168, loss: 2.127940\n",
      "Epoch 169, loss: 2.127916\n",
      "Epoch 170, loss: 2.127874\n",
      "Epoch 171, loss: 2.127831\n",
      "Epoch 172, loss: 2.127800\n",
      "Epoch 173, loss: 2.127748\n",
      "Epoch 174, loss: 2.127715\n",
      "Epoch 175, loss: 2.127668\n",
      "Epoch 176, loss: 2.127617\n",
      "Epoch 177, loss: 2.127578\n",
      "Epoch 178, loss: 2.127537\n",
      "Epoch 179, loss: 2.127500\n",
      "Epoch 180, loss: 2.127449\n",
      "Epoch 181, loss: 2.127434\n",
      "Epoch 182, loss: 2.127361\n",
      "Epoch 183, loss: 2.127331\n",
      "Epoch 184, loss: 2.127298\n",
      "Epoch 185, loss: 2.127259\n",
      "Epoch 186, loss: 2.127226\n",
      "Epoch 187, loss: 2.127178\n",
      "Epoch 188, loss: 2.127150\n",
      "Epoch 189, loss: 2.127094\n",
      "Epoch 190, loss: 2.127039\n",
      "Epoch 191, loss: 2.127006\n",
      "Epoch 192, loss: 2.126981\n",
      "Epoch 193, loss: 2.126927\n",
      "Epoch 194, loss: 2.126905\n",
      "Epoch 195, loss: 2.126841\n",
      "Epoch 196, loss: 2.126799\n",
      "Epoch 197, loss: 2.126777\n",
      "Epoch 198, loss: 2.126725\n",
      "Epoch 199, loss: 2.126679\n",
      "Epoch 0, loss: 2.126729\n",
      "Epoch 1, loss: 2.126631\n",
      "Epoch 2, loss: 2.126576\n",
      "Epoch 3, loss: 2.126500\n",
      "Epoch 4, loss: 2.126436\n",
      "Epoch 5, loss: 2.126393\n",
      "Epoch 6, loss: 2.126306\n",
      "Epoch 7, loss: 2.126244\n",
      "Epoch 8, loss: 2.126181\n",
      "Epoch 9, loss: 2.126120\n",
      "Epoch 10, loss: 2.126081\n",
      "Epoch 11, loss: 2.126002\n",
      "Epoch 12, loss: 2.125966\n",
      "Epoch 13, loss: 2.125905\n",
      "Epoch 14, loss: 2.125843\n",
      "Epoch 15, loss: 2.125794\n",
      "Epoch 16, loss: 2.125744\n",
      "Epoch 17, loss: 2.125703\n",
      "Epoch 18, loss: 2.125628\n",
      "Epoch 19, loss: 2.125560\n",
      "Epoch 20, loss: 2.125517\n",
      "Epoch 21, loss: 2.125459\n",
      "Epoch 22, loss: 2.125395\n",
      "Epoch 23, loss: 2.125372\n",
      "Epoch 24, loss: 2.125306\n",
      "Epoch 25, loss: 2.125243\n",
      "Epoch 26, loss: 2.125164\n",
      "Epoch 27, loss: 2.125148\n",
      "Epoch 28, loss: 2.125080\n",
      "Epoch 29, loss: 2.125035\n",
      "Epoch 30, loss: 2.124961\n",
      "Epoch 31, loss: 2.124929\n",
      "Epoch 32, loss: 2.124878\n",
      "Epoch 33, loss: 2.124836\n",
      "Epoch 34, loss: 2.124781\n",
      "Epoch 35, loss: 2.124712\n",
      "Epoch 36, loss: 2.124672\n",
      "Epoch 37, loss: 2.124628\n",
      "Epoch 38, loss: 2.124587\n",
      "Epoch 39, loss: 2.124547\n",
      "Epoch 40, loss: 2.124474\n",
      "Epoch 41, loss: 2.124419\n",
      "Epoch 42, loss: 2.124396\n",
      "Epoch 43, loss: 2.124338\n",
      "Epoch 44, loss: 2.124299\n",
      "Epoch 45, loss: 2.124225\n",
      "Epoch 46, loss: 2.124181\n",
      "Epoch 47, loss: 2.124129\n",
      "Epoch 48, loss: 2.124095\n",
      "Epoch 49, loss: 2.124063\n",
      "Epoch 50, loss: 2.123990\n",
      "Epoch 51, loss: 2.123942\n",
      "Epoch 52, loss: 2.123901\n",
      "Epoch 53, loss: 2.123854\n",
      "Epoch 54, loss: 2.123806\n",
      "Epoch 55, loss: 2.123744\n",
      "Epoch 56, loss: 2.123718\n",
      "Epoch 57, loss: 2.123654\n",
      "Epoch 58, loss: 2.123624\n",
      "Epoch 59, loss: 2.123553\n",
      "Epoch 60, loss: 2.123520\n",
      "Epoch 61, loss: 2.123476\n",
      "Epoch 62, loss: 2.123425\n",
      "Epoch 63, loss: 2.123389\n",
      "Epoch 64, loss: 2.123347\n",
      "Epoch 65, loss: 2.123309\n",
      "Epoch 66, loss: 2.123239\n",
      "Epoch 67, loss: 2.123200\n",
      "Epoch 68, loss: 2.123141\n",
      "Epoch 69, loss: 2.123128\n",
      "Epoch 70, loss: 2.123064\n",
      "Epoch 71, loss: 2.123022\n",
      "Epoch 72, loss: 2.122973\n",
      "Epoch 73, loss: 2.122927\n",
      "Epoch 74, loss: 2.122877\n",
      "Epoch 75, loss: 2.122833\n",
      "Epoch 76, loss: 2.122776\n",
      "Epoch 77, loss: 2.122741\n",
      "Epoch 78, loss: 2.122698\n",
      "Epoch 79, loss: 2.122669\n",
      "Epoch 80, loss: 2.122611\n",
      "Epoch 81, loss: 2.122587\n",
      "Epoch 82, loss: 2.122531\n",
      "Epoch 83, loss: 2.122507\n",
      "Epoch 84, loss: 2.122446\n",
      "Epoch 85, loss: 2.122407\n",
      "Epoch 86, loss: 2.122342\n",
      "Epoch 87, loss: 2.122313\n",
      "Epoch 88, loss: 2.122271\n",
      "Epoch 89, loss: 2.122213\n",
      "Epoch 90, loss: 2.122190\n",
      "Epoch 91, loss: 2.122149\n",
      "Epoch 92, loss: 2.122104\n",
      "Epoch 93, loss: 2.122043\n",
      "Epoch 94, loss: 2.122026\n",
      "Epoch 95, loss: 2.121970\n",
      "Epoch 96, loss: 2.121928\n",
      "Epoch 97, loss: 2.121896\n",
      "Epoch 98, loss: 2.121831\n",
      "Epoch 99, loss: 2.121783\n",
      "Epoch 100, loss: 2.121772\n",
      "Epoch 101, loss: 2.121713\n",
      "Epoch 102, loss: 2.121687\n",
      "Epoch 103, loss: 2.121626\n",
      "Epoch 104, loss: 2.121596\n",
      "Epoch 105, loss: 2.121542\n",
      "Epoch 106, loss: 2.121513\n",
      "Epoch 107, loss: 2.121462\n",
      "Epoch 108, loss: 2.121433\n",
      "Epoch 109, loss: 2.121383\n",
      "Epoch 110, loss: 2.121333\n",
      "Epoch 111, loss: 2.121314\n",
      "Epoch 112, loss: 2.121243\n",
      "Epoch 113, loss: 2.121198\n",
      "Epoch 114, loss: 2.121162\n",
      "Epoch 115, loss: 2.121131\n",
      "Epoch 116, loss: 2.121113\n",
      "Epoch 117, loss: 2.121063\n",
      "Epoch 118, loss: 2.121010\n",
      "Epoch 119, loss: 2.120980\n",
      "Epoch 120, loss: 2.120933\n",
      "Epoch 121, loss: 2.120887\n",
      "Epoch 122, loss: 2.120846\n",
      "Epoch 123, loss: 2.120825\n",
      "Epoch 124, loss: 2.120773\n",
      "Epoch 125, loss: 2.120732\n",
      "Epoch 126, loss: 2.120696\n",
      "Epoch 127, loss: 2.120649\n",
      "Epoch 128, loss: 2.120589\n",
      "Epoch 129, loss: 2.120570\n",
      "Epoch 130, loss: 2.120507\n",
      "Epoch 131, loss: 2.120501\n",
      "Epoch 132, loss: 2.120470\n",
      "Epoch 133, loss: 2.120403\n",
      "Epoch 134, loss: 2.120376\n",
      "Epoch 135, loss: 2.120328\n",
      "Epoch 136, loss: 2.120291\n",
      "Epoch 137, loss: 2.120255\n",
      "Epoch 138, loss: 2.120207\n",
      "Epoch 139, loss: 2.120181\n",
      "Epoch 140, loss: 2.120140\n",
      "Epoch 141, loss: 2.120103\n",
      "Epoch 142, loss: 2.120069\n",
      "Epoch 143, loss: 2.120017\n",
      "Epoch 144, loss: 2.119986\n",
      "Epoch 145, loss: 2.119948\n",
      "Epoch 146, loss: 2.119898\n",
      "Epoch 147, loss: 2.119850\n",
      "Epoch 148, loss: 2.119821\n",
      "Epoch 149, loss: 2.119805\n",
      "Epoch 150, loss: 2.119735\n",
      "Epoch 151, loss: 2.119704\n",
      "Epoch 152, loss: 2.119673\n",
      "Epoch 153, loss: 2.119639\n",
      "Epoch 154, loss: 2.119605\n",
      "Epoch 155, loss: 2.119550\n",
      "Epoch 156, loss: 2.119521\n",
      "Epoch 157, loss: 2.119475\n",
      "Epoch 158, loss: 2.119444\n",
      "Epoch 159, loss: 2.119420\n",
      "Epoch 160, loss: 2.119359\n",
      "Epoch 161, loss: 2.119330\n",
      "Epoch 162, loss: 2.119290\n",
      "Epoch 163, loss: 2.119243\n",
      "Epoch 164, loss: 2.119210\n",
      "Epoch 165, loss: 2.119176\n",
      "Epoch 166, loss: 2.119154\n",
      "Epoch 167, loss: 2.119090\n",
      "Epoch 168, loss: 2.119073\n",
      "Epoch 169, loss: 2.119030\n",
      "Epoch 170, loss: 2.118977\n",
      "Epoch 171, loss: 2.118967\n",
      "Epoch 172, loss: 2.118911\n",
      "Epoch 173, loss: 2.118862\n",
      "Epoch 174, loss: 2.118826\n",
      "Epoch 175, loss: 2.118808\n",
      "Epoch 176, loss: 2.118784\n",
      "Epoch 177, loss: 2.118734\n",
      "Epoch 178, loss: 2.118689\n",
      "Epoch 179, loss: 2.118645\n",
      "Epoch 180, loss: 2.118612\n",
      "Epoch 181, loss: 2.118572\n",
      "Epoch 182, loss: 2.118533\n",
      "Epoch 183, loss: 2.118489\n",
      "Epoch 184, loss: 2.118485\n",
      "Epoch 185, loss: 2.118428\n",
      "Epoch 186, loss: 2.118405\n",
      "Epoch 187, loss: 2.118351\n",
      "Epoch 188, loss: 2.118318\n",
      "Epoch 189, loss: 2.118278\n",
      "Epoch 190, loss: 2.118241\n",
      "Epoch 191, loss: 2.118201\n",
      "Epoch 192, loss: 2.118169\n",
      "Epoch 193, loss: 2.118165\n",
      "Epoch 194, loss: 2.118092\n",
      "Epoch 195, loss: 2.118068\n",
      "Epoch 196, loss: 2.118029\n",
      "Epoch 197, loss: 2.117987\n",
      "Epoch 198, loss: 2.117953\n",
      "Epoch 199, loss: 2.117921\n",
      "Epoch 0, loss: 2.121457\n",
      "Epoch 1, loss: 2.121301\n",
      "Epoch 2, loss: 2.121150\n",
      "Epoch 3, loss: 2.121062\n",
      "Epoch 4, loss: 2.120993\n",
      "Epoch 5, loss: 2.120859\n",
      "Epoch 6, loss: 2.120772\n",
      "Epoch 7, loss: 2.120690\n",
      "Epoch 8, loss: 2.120591\n",
      "Epoch 9, loss: 2.120536\n",
      "Epoch 10, loss: 2.120456\n",
      "Epoch 11, loss: 2.120360\n",
      "Epoch 12, loss: 2.120317\n",
      "Epoch 13, loss: 2.120220\n",
      "Epoch 14, loss: 2.120150\n",
      "Epoch 15, loss: 2.120056\n",
      "Epoch 16, loss: 2.120002\n",
      "Epoch 17, loss: 2.119910\n",
      "Epoch 18, loss: 2.119855\n",
      "Epoch 19, loss: 2.119787\n",
      "Epoch 20, loss: 2.119711\n",
      "Epoch 21, loss: 2.119643\n",
      "Epoch 22, loss: 2.119572\n",
      "Epoch 23, loss: 2.119512\n",
      "Epoch 24, loss: 2.119436\n",
      "Epoch 25, loss: 2.119369\n",
      "Epoch 26, loss: 2.119312\n",
      "Epoch 27, loss: 2.119234\n",
      "Epoch 28, loss: 2.119187\n",
      "Epoch 29, loss: 2.119101\n",
      "Epoch 30, loss: 2.119038\n",
      "Epoch 31, loss: 2.118997\n",
      "Epoch 32, loss: 2.118922\n",
      "Epoch 33, loss: 2.118853\n",
      "Epoch 34, loss: 2.118797\n",
      "Epoch 35, loss: 2.118728\n",
      "Epoch 36, loss: 2.118680\n",
      "Epoch 37, loss: 2.118622\n",
      "Epoch 38, loss: 2.118538\n",
      "Epoch 39, loss: 2.118475\n",
      "Epoch 40, loss: 2.118423\n",
      "Epoch 41, loss: 2.118356\n",
      "Epoch 42, loss: 2.118290\n",
      "Epoch 43, loss: 2.118232\n",
      "Epoch 44, loss: 2.118194\n",
      "Epoch 45, loss: 2.118119\n",
      "Epoch 46, loss: 2.118058\n",
      "Epoch 47, loss: 2.118003\n",
      "Epoch 48, loss: 2.117930\n",
      "Epoch 49, loss: 2.117880\n",
      "Epoch 50, loss: 2.117847\n",
      "Epoch 51, loss: 2.117793\n",
      "Epoch 52, loss: 2.117728\n",
      "Epoch 53, loss: 2.117670\n",
      "Epoch 54, loss: 2.117596\n",
      "Epoch 55, loss: 2.117557\n",
      "Epoch 56, loss: 2.117507\n",
      "Epoch 57, loss: 2.117442\n",
      "Epoch 58, loss: 2.117384\n",
      "Epoch 59, loss: 2.117348\n",
      "Epoch 60, loss: 2.117283\n",
      "Epoch 61, loss: 2.117230\n",
      "Epoch 62, loss: 2.117171\n",
      "Epoch 63, loss: 2.117116\n",
      "Epoch 64, loss: 2.117076\n",
      "Epoch 65, loss: 2.117028\n",
      "Epoch 66, loss: 2.116977\n",
      "Epoch 67, loss: 2.116925\n",
      "Epoch 68, loss: 2.116868\n",
      "Epoch 69, loss: 2.116799\n",
      "Epoch 70, loss: 2.116749\n",
      "Epoch 71, loss: 2.116710\n",
      "Epoch 72, loss: 2.116652\n",
      "Epoch 73, loss: 2.116597\n",
      "Epoch 74, loss: 2.116561\n",
      "Epoch 75, loss: 2.116503\n",
      "Epoch 76, loss: 2.116458\n",
      "Epoch 77, loss: 2.116404\n",
      "Epoch 78, loss: 2.116365\n",
      "Epoch 79, loss: 2.116311\n",
      "Epoch 80, loss: 2.116250\n",
      "Epoch 81, loss: 2.116205\n",
      "Epoch 82, loss: 2.116134\n",
      "Epoch 83, loss: 2.116105\n",
      "Epoch 84, loss: 2.116066\n",
      "Epoch 85, loss: 2.116021\n",
      "Epoch 86, loss: 2.115947\n",
      "Epoch 87, loss: 2.115919\n",
      "Epoch 88, loss: 2.115885\n",
      "Epoch 89, loss: 2.115811\n",
      "Epoch 90, loss: 2.115762\n",
      "Epoch 91, loss: 2.115735\n",
      "Epoch 92, loss: 2.115678\n",
      "Epoch 93, loss: 2.115619\n",
      "Epoch 94, loss: 2.115594\n",
      "Epoch 95, loss: 2.115540\n",
      "Epoch 96, loss: 2.115512\n",
      "Epoch 97, loss: 2.115456\n",
      "Epoch 98, loss: 2.115390\n",
      "Epoch 99, loss: 2.115355\n",
      "Epoch 100, loss: 2.115321\n",
      "Epoch 101, loss: 2.115272\n",
      "Epoch 102, loss: 2.115225\n",
      "Epoch 103, loss: 2.115163\n",
      "Epoch 104, loss: 2.115125\n",
      "Epoch 105, loss: 2.115075\n",
      "Epoch 106, loss: 2.115034\n",
      "Epoch 107, loss: 2.114982\n",
      "Epoch 108, loss: 2.114946\n",
      "Epoch 109, loss: 2.114916\n",
      "Epoch 110, loss: 2.114851\n",
      "Epoch 111, loss: 2.114808\n",
      "Epoch 112, loss: 2.114766\n",
      "Epoch 113, loss: 2.114726\n",
      "Epoch 114, loss: 2.114670\n",
      "Epoch 115, loss: 2.114635\n",
      "Epoch 116, loss: 2.114595\n",
      "Epoch 117, loss: 2.114551\n",
      "Epoch 118, loss: 2.114512\n",
      "Epoch 119, loss: 2.114471\n",
      "Epoch 120, loss: 2.114414\n",
      "Epoch 121, loss: 2.114384\n",
      "Epoch 122, loss: 2.114326\n",
      "Epoch 123, loss: 2.114287\n",
      "Epoch 124, loss: 2.114237\n",
      "Epoch 125, loss: 2.114214\n",
      "Epoch 126, loss: 2.114164\n",
      "Epoch 127, loss: 2.114136\n",
      "Epoch 128, loss: 2.114079\n",
      "Epoch 129, loss: 2.114028\n",
      "Epoch 130, loss: 2.113980\n",
      "Epoch 131, loss: 2.113960\n",
      "Epoch 132, loss: 2.113910\n",
      "Epoch 133, loss: 2.113861\n",
      "Epoch 134, loss: 2.113822\n",
      "Epoch 135, loss: 2.113777\n",
      "Epoch 136, loss: 2.113739\n",
      "Epoch 137, loss: 2.113700\n",
      "Epoch 138, loss: 2.113642\n",
      "Epoch 139, loss: 2.113616\n",
      "Epoch 140, loss: 2.113561\n",
      "Epoch 141, loss: 2.113528\n",
      "Epoch 142, loss: 2.113490\n",
      "Epoch 143, loss: 2.113445\n",
      "Epoch 144, loss: 2.113390\n",
      "Epoch 145, loss: 2.113374\n",
      "Epoch 146, loss: 2.113330\n",
      "Epoch 147, loss: 2.113288\n",
      "Epoch 148, loss: 2.113244\n",
      "Epoch 149, loss: 2.113199\n",
      "Epoch 150, loss: 2.113152\n",
      "Epoch 151, loss: 2.113128\n",
      "Epoch 152, loss: 2.113085\n",
      "Epoch 153, loss: 2.113024\n",
      "Epoch 154, loss: 2.113010\n",
      "Epoch 155, loss: 2.112950\n",
      "Epoch 156, loss: 2.112931\n",
      "Epoch 157, loss: 2.112879\n",
      "Epoch 158, loss: 2.112834\n",
      "Epoch 159, loss: 2.112789\n",
      "Epoch 160, loss: 2.112797\n",
      "Epoch 161, loss: 2.112727\n",
      "Epoch 162, loss: 2.112669\n",
      "Epoch 163, loss: 2.112645\n",
      "Epoch 164, loss: 2.112605\n",
      "Epoch 165, loss: 2.112564\n",
      "Epoch 166, loss: 2.112523\n",
      "Epoch 167, loss: 2.112497\n",
      "Epoch 168, loss: 2.112446\n",
      "Epoch 169, loss: 2.112423\n",
      "Epoch 170, loss: 2.112380\n",
      "Epoch 171, loss: 2.112318\n",
      "Epoch 172, loss: 2.112310\n",
      "Epoch 173, loss: 2.112249\n",
      "Epoch 174, loss: 2.112222\n",
      "Epoch 175, loss: 2.112175\n",
      "Epoch 176, loss: 2.112149\n",
      "Epoch 177, loss: 2.112107\n",
      "Epoch 178, loss: 2.112066\n",
      "Epoch 179, loss: 2.112008\n",
      "Epoch 180, loss: 2.111992\n",
      "Epoch 181, loss: 2.111944\n",
      "Epoch 182, loss: 2.111909\n",
      "Epoch 183, loss: 2.111864\n",
      "Epoch 184, loss: 2.111826\n",
      "Epoch 185, loss: 2.111793\n",
      "Epoch 186, loss: 2.111773\n",
      "Epoch 187, loss: 2.111717\n",
      "Epoch 188, loss: 2.111703\n",
      "Epoch 189, loss: 2.111640\n",
      "Epoch 190, loss: 2.111606\n",
      "Epoch 191, loss: 2.111585\n",
      "Epoch 192, loss: 2.111527\n",
      "Epoch 193, loss: 2.111502\n",
      "Epoch 194, loss: 2.111460\n",
      "Epoch 195, loss: 2.111420\n",
      "Epoch 196, loss: 2.111377\n",
      "Epoch 197, loss: 2.111344\n",
      "Epoch 198, loss: 2.111301\n",
      "Epoch 199, loss: 2.111275\n",
      "Epoch 0, loss: 2.121378\n",
      "Epoch 1, loss: 2.121210\n",
      "Epoch 2, loss: 2.121070\n",
      "Epoch 3, loss: 2.120961\n",
      "Epoch 4, loss: 2.120841\n",
      "Epoch 5, loss: 2.120744\n",
      "Epoch 6, loss: 2.120656\n",
      "Epoch 7, loss: 2.120587\n",
      "Epoch 8, loss: 2.120494\n",
      "Epoch 9, loss: 2.120421\n",
      "Epoch 10, loss: 2.120345\n",
      "Epoch 11, loss: 2.120289\n",
      "Epoch 12, loss: 2.120224\n",
      "Epoch 13, loss: 2.120139\n",
      "Epoch 14, loss: 2.120087\n",
      "Epoch 15, loss: 2.120044\n",
      "Epoch 16, loss: 2.119969\n",
      "Epoch 17, loss: 2.119903\n",
      "Epoch 18, loss: 2.119832\n",
      "Epoch 19, loss: 2.119793\n",
      "Epoch 20, loss: 2.119742\n",
      "Epoch 21, loss: 2.119662\n",
      "Epoch 22, loss: 2.119636\n",
      "Epoch 23, loss: 2.119570\n",
      "Epoch 24, loss: 2.119507\n",
      "Epoch 25, loss: 2.119459\n",
      "Epoch 26, loss: 2.119403\n",
      "Epoch 27, loss: 2.119334\n",
      "Epoch 28, loss: 2.119324\n",
      "Epoch 29, loss: 2.119234\n",
      "Epoch 30, loss: 2.119187\n",
      "Epoch 31, loss: 2.119127\n",
      "Epoch 32, loss: 2.119075\n",
      "Epoch 33, loss: 2.119032\n",
      "Epoch 34, loss: 2.118978\n",
      "Epoch 35, loss: 2.118941\n",
      "Epoch 36, loss: 2.118879\n",
      "Epoch 37, loss: 2.118848\n",
      "Epoch 38, loss: 2.118782\n",
      "Epoch 39, loss: 2.118718\n",
      "Epoch 40, loss: 2.118689\n",
      "Epoch 41, loss: 2.118633\n",
      "Epoch 42, loss: 2.118593\n",
      "Epoch 43, loss: 2.118532\n",
      "Epoch 44, loss: 2.118491\n",
      "Epoch 45, loss: 2.118434\n",
      "Epoch 46, loss: 2.118389\n",
      "Epoch 47, loss: 2.118355\n",
      "Epoch 48, loss: 2.118295\n",
      "Epoch 49, loss: 2.118256\n",
      "Epoch 50, loss: 2.118199\n",
      "Epoch 51, loss: 2.118170\n",
      "Epoch 52, loss: 2.118116\n",
      "Epoch 53, loss: 2.118066\n",
      "Epoch 54, loss: 2.118026\n",
      "Epoch 55, loss: 2.117992\n",
      "Epoch 56, loss: 2.117916\n",
      "Epoch 57, loss: 2.117891\n",
      "Epoch 58, loss: 2.117854\n",
      "Epoch 59, loss: 2.117805\n",
      "Epoch 60, loss: 2.117762\n",
      "Epoch 61, loss: 2.117709\n",
      "Epoch 62, loss: 2.117659\n",
      "Epoch 63, loss: 2.117619\n",
      "Epoch 64, loss: 2.117570\n",
      "Epoch 65, loss: 2.117552\n",
      "Epoch 66, loss: 2.117479\n",
      "Epoch 67, loss: 2.117442\n",
      "Epoch 68, loss: 2.117414\n",
      "Epoch 69, loss: 2.117347\n",
      "Epoch 70, loss: 2.117326\n",
      "Epoch 71, loss: 2.117281\n",
      "Epoch 72, loss: 2.117233\n",
      "Epoch 73, loss: 2.117179\n",
      "Epoch 74, loss: 2.117142\n",
      "Epoch 75, loss: 2.117110\n",
      "Epoch 76, loss: 2.117064\n",
      "Epoch 77, loss: 2.117007\n",
      "Epoch 78, loss: 2.116988\n",
      "Epoch 79, loss: 2.116951\n",
      "Epoch 80, loss: 2.116889\n",
      "Epoch 81, loss: 2.116844\n",
      "Epoch 82, loss: 2.116799\n",
      "Epoch 83, loss: 2.116777\n",
      "Epoch 84, loss: 2.116725\n",
      "Epoch 85, loss: 2.116700\n",
      "Epoch 86, loss: 2.116659\n",
      "Epoch 87, loss: 2.116617\n",
      "Epoch 88, loss: 2.116568\n",
      "Epoch 89, loss: 2.116546\n",
      "Epoch 90, loss: 2.116478\n",
      "Epoch 91, loss: 2.116439\n",
      "Epoch 92, loss: 2.116411\n",
      "Epoch 93, loss: 2.116348\n",
      "Epoch 94, loss: 2.116317\n",
      "Epoch 95, loss: 2.116296\n",
      "Epoch 96, loss: 2.116234\n",
      "Epoch 97, loss: 2.116208\n",
      "Epoch 98, loss: 2.116181\n",
      "Epoch 99, loss: 2.116122\n",
      "Epoch 100, loss: 2.116093\n",
      "Epoch 101, loss: 2.116054\n",
      "Epoch 102, loss: 2.116013\n",
      "Epoch 103, loss: 2.115966\n",
      "Epoch 104, loss: 2.115926\n",
      "Epoch 105, loss: 2.115882\n",
      "Epoch 106, loss: 2.115860\n",
      "Epoch 107, loss: 2.115811\n",
      "Epoch 108, loss: 2.115768\n",
      "Epoch 109, loss: 2.115747\n",
      "Epoch 110, loss: 2.115701\n",
      "Epoch 111, loss: 2.115649\n",
      "Epoch 112, loss: 2.115618\n",
      "Epoch 113, loss: 2.115592\n",
      "Epoch 114, loss: 2.115526\n",
      "Epoch 115, loss: 2.115510\n",
      "Epoch 116, loss: 2.115478\n",
      "Epoch 117, loss: 2.115414\n",
      "Epoch 118, loss: 2.115385\n",
      "Epoch 119, loss: 2.115332\n",
      "Epoch 120, loss: 2.115320\n",
      "Epoch 121, loss: 2.115287\n",
      "Epoch 122, loss: 2.115231\n",
      "Epoch 123, loss: 2.115211\n",
      "Epoch 124, loss: 2.115150\n",
      "Epoch 125, loss: 2.115130\n",
      "Epoch 126, loss: 2.115090\n",
      "Epoch 127, loss: 2.115047\n",
      "Epoch 128, loss: 2.115013\n",
      "Epoch 129, loss: 2.114972\n",
      "Epoch 130, loss: 2.114947\n",
      "Epoch 131, loss: 2.114907\n",
      "Epoch 132, loss: 2.114870\n",
      "Epoch 133, loss: 2.114821\n",
      "Epoch 134, loss: 2.114779\n",
      "Epoch 135, loss: 2.114766\n",
      "Epoch 136, loss: 2.114728\n",
      "Epoch 137, loss: 2.114677\n",
      "Epoch 138, loss: 2.114657\n",
      "Epoch 139, loss: 2.114610\n",
      "Epoch 140, loss: 2.114566\n",
      "Epoch 141, loss: 2.114543\n",
      "Epoch 142, loss: 2.114489\n",
      "Epoch 143, loss: 2.114475\n",
      "Epoch 144, loss: 2.114430\n",
      "Epoch 145, loss: 2.114406\n",
      "Epoch 146, loss: 2.114362\n",
      "Epoch 147, loss: 2.114309\n",
      "Epoch 148, loss: 2.114260\n",
      "Epoch 149, loss: 2.114234\n",
      "Epoch 150, loss: 2.114232\n",
      "Epoch 151, loss: 2.114168\n",
      "Epoch 152, loss: 2.114138\n",
      "Epoch 153, loss: 2.114102\n",
      "Epoch 154, loss: 2.114071\n",
      "Epoch 155, loss: 2.114035\n",
      "Epoch 156, loss: 2.113996\n",
      "Epoch 157, loss: 2.113950\n",
      "Epoch 158, loss: 2.113919\n",
      "Epoch 159, loss: 2.113888\n",
      "Epoch 160, loss: 2.113855\n",
      "Epoch 161, loss: 2.113819\n",
      "Epoch 162, loss: 2.113786\n",
      "Epoch 163, loss: 2.113733\n",
      "Epoch 164, loss: 2.113726\n",
      "Epoch 165, loss: 2.113685\n",
      "Epoch 166, loss: 2.113652\n",
      "Epoch 167, loss: 2.113595\n",
      "Epoch 168, loss: 2.113578\n",
      "Epoch 169, loss: 2.113546\n",
      "Epoch 170, loss: 2.113515\n",
      "Epoch 171, loss: 2.113472\n",
      "Epoch 172, loss: 2.113437\n",
      "Epoch 173, loss: 2.113411\n",
      "Epoch 174, loss: 2.113377\n",
      "Epoch 175, loss: 2.113324\n",
      "Epoch 176, loss: 2.113286\n",
      "Epoch 177, loss: 2.113265\n",
      "Epoch 178, loss: 2.113215\n",
      "Epoch 179, loss: 2.113201\n",
      "Epoch 180, loss: 2.113173\n",
      "Epoch 181, loss: 2.113130\n",
      "Epoch 182, loss: 2.113092\n",
      "Epoch 183, loss: 2.113059\n",
      "Epoch 184, loss: 2.113033\n",
      "Epoch 185, loss: 2.112992\n",
      "Epoch 186, loss: 2.112966\n",
      "Epoch 187, loss: 2.112936\n",
      "Epoch 188, loss: 2.112881\n",
      "Epoch 189, loss: 2.112869\n",
      "Epoch 190, loss: 2.112815\n",
      "Epoch 191, loss: 2.112800\n",
      "Epoch 192, loss: 2.112783\n",
      "Epoch 193, loss: 2.112725\n",
      "Epoch 194, loss: 2.112697\n",
      "Epoch 195, loss: 2.112650\n",
      "Epoch 196, loss: 2.112604\n",
      "Epoch 197, loss: 2.112590\n",
      "Epoch 198, loss: 2.112563\n",
      "Epoch 199, loss: 2.112527\n",
      "Epoch 0, loss: 2.110848\n",
      "Epoch 1, loss: 2.110703\n",
      "Epoch 2, loss: 2.110592\n",
      "Epoch 3, loss: 2.110456\n",
      "Epoch 4, loss: 2.110344\n",
      "Epoch 5, loss: 2.110262\n",
      "Epoch 6, loss: 2.110165\n",
      "Epoch 7, loss: 2.110074\n",
      "Epoch 8, loss: 2.110014\n",
      "Epoch 9, loss: 2.109943\n",
      "Epoch 10, loss: 2.109879\n",
      "Epoch 11, loss: 2.109816\n",
      "Epoch 12, loss: 2.109757\n",
      "Epoch 13, loss: 2.109694\n",
      "Epoch 14, loss: 2.109623\n",
      "Epoch 15, loss: 2.109581\n",
      "Epoch 16, loss: 2.109506\n",
      "Epoch 17, loss: 2.109439\n",
      "Epoch 18, loss: 2.109397\n",
      "Epoch 19, loss: 2.109343\n",
      "Epoch 20, loss: 2.109273\n",
      "Epoch 21, loss: 2.109238\n",
      "Epoch 22, loss: 2.109188\n",
      "Epoch 23, loss: 2.109110\n",
      "Epoch 24, loss: 2.109059\n",
      "Epoch 25, loss: 2.108999\n",
      "Epoch 26, loss: 2.108963\n",
      "Epoch 27, loss: 2.108893\n",
      "Epoch 28, loss: 2.108859\n",
      "Epoch 29, loss: 2.108790\n",
      "Epoch 30, loss: 2.108740\n",
      "Epoch 31, loss: 2.108698\n",
      "Epoch 32, loss: 2.108654\n",
      "Epoch 33, loss: 2.108585\n",
      "Epoch 34, loss: 2.108547\n",
      "Epoch 35, loss: 2.108516\n",
      "Epoch 36, loss: 2.108436\n",
      "Epoch 37, loss: 2.108396\n",
      "Epoch 38, loss: 2.108336\n",
      "Epoch 39, loss: 2.108289\n",
      "Epoch 40, loss: 2.108242\n",
      "Epoch 41, loss: 2.108200\n",
      "Epoch 42, loss: 2.108165\n",
      "Epoch 43, loss: 2.108102\n",
      "Epoch 44, loss: 2.108046\n",
      "Epoch 45, loss: 2.108005\n",
      "Epoch 46, loss: 2.107959\n",
      "Epoch 47, loss: 2.107933\n",
      "Epoch 48, loss: 2.107872\n",
      "Epoch 49, loss: 2.107803\n",
      "Epoch 50, loss: 2.107776\n",
      "Epoch 51, loss: 2.107723\n",
      "Epoch 52, loss: 2.107685\n",
      "Epoch 53, loss: 2.107628\n",
      "Epoch 54, loss: 2.107602\n",
      "Epoch 55, loss: 2.107563\n",
      "Epoch 56, loss: 2.107512\n",
      "Epoch 57, loss: 2.107459\n",
      "Epoch 58, loss: 2.107410\n",
      "Epoch 59, loss: 2.107384\n",
      "Epoch 60, loss: 2.107315\n",
      "Epoch 61, loss: 2.107279\n",
      "Epoch 62, loss: 2.107259\n",
      "Epoch 63, loss: 2.107204\n",
      "Epoch 64, loss: 2.107132\n",
      "Epoch 65, loss: 2.107147\n",
      "Epoch 66, loss: 2.107066\n",
      "Epoch 67, loss: 2.107023\n",
      "Epoch 68, loss: 2.107007\n",
      "Epoch 69, loss: 2.106923\n",
      "Epoch 70, loss: 2.106895\n",
      "Epoch 71, loss: 2.106860\n",
      "Epoch 72, loss: 2.106818\n",
      "Epoch 73, loss: 2.106764\n",
      "Epoch 74, loss: 2.106732\n",
      "Epoch 75, loss: 2.106694\n",
      "Epoch 76, loss: 2.106654\n",
      "Epoch 77, loss: 2.106608\n",
      "Epoch 78, loss: 2.106553\n",
      "Epoch 79, loss: 2.106524\n",
      "Epoch 80, loss: 2.106482\n",
      "Epoch 81, loss: 2.106446\n",
      "Epoch 82, loss: 2.106404\n",
      "Epoch 83, loss: 2.106353\n",
      "Epoch 84, loss: 2.106323\n",
      "Epoch 85, loss: 2.106283\n",
      "Epoch 86, loss: 2.106220\n",
      "Epoch 87, loss: 2.106210\n",
      "Epoch 88, loss: 2.106173\n",
      "Epoch 89, loss: 2.106118\n",
      "Epoch 90, loss: 2.106090\n",
      "Epoch 91, loss: 2.106020\n",
      "Epoch 92, loss: 2.106008\n",
      "Epoch 93, loss: 2.105965\n",
      "Epoch 94, loss: 2.105914\n",
      "Epoch 95, loss: 2.105881\n",
      "Epoch 96, loss: 2.105842\n",
      "Epoch 97, loss: 2.105802\n",
      "Epoch 98, loss: 2.105765\n",
      "Epoch 99, loss: 2.105729\n",
      "Epoch 100, loss: 2.105695\n",
      "Epoch 101, loss: 2.105645\n",
      "Epoch 102, loss: 2.105619\n",
      "Epoch 103, loss: 2.105589\n",
      "Epoch 104, loss: 2.105530\n",
      "Epoch 105, loss: 2.105495\n",
      "Epoch 106, loss: 2.105465\n",
      "Epoch 107, loss: 2.105432\n",
      "Epoch 108, loss: 2.105377\n",
      "Epoch 109, loss: 2.105350\n",
      "Epoch 110, loss: 2.105293\n",
      "Epoch 111, loss: 2.105271\n",
      "Epoch 112, loss: 2.105227\n",
      "Epoch 113, loss: 2.105205\n",
      "Epoch 114, loss: 2.105173\n",
      "Epoch 115, loss: 2.105126\n",
      "Epoch 116, loss: 2.105120\n",
      "Epoch 117, loss: 2.105067\n",
      "Epoch 118, loss: 2.105011\n",
      "Epoch 119, loss: 2.104968\n",
      "Epoch 120, loss: 2.104940\n",
      "Epoch 121, loss: 2.104940\n",
      "Epoch 122, loss: 2.104855\n",
      "Epoch 123, loss: 2.104843\n",
      "Epoch 124, loss: 2.104795\n",
      "Epoch 125, loss: 2.104767\n",
      "Epoch 126, loss: 2.104742\n",
      "Epoch 127, loss: 2.104702\n",
      "Epoch 128, loss: 2.104657\n",
      "Epoch 129, loss: 2.104621\n",
      "Epoch 130, loss: 2.104590\n",
      "Epoch 131, loss: 2.104554\n",
      "Epoch 132, loss: 2.104523\n",
      "Epoch 133, loss: 2.104471\n",
      "Epoch 134, loss: 2.104456\n",
      "Epoch 135, loss: 2.104399\n",
      "Epoch 136, loss: 2.104378\n",
      "Epoch 137, loss: 2.104320\n",
      "Epoch 138, loss: 2.104312\n",
      "Epoch 139, loss: 2.104270\n",
      "Epoch 140, loss: 2.104233\n",
      "Epoch 141, loss: 2.104191\n",
      "Epoch 142, loss: 2.104159\n",
      "Epoch 143, loss: 2.104131\n",
      "Epoch 144, loss: 2.104092\n",
      "Epoch 145, loss: 2.104061\n",
      "Epoch 146, loss: 2.104016\n",
      "Epoch 147, loss: 2.103989\n",
      "Epoch 148, loss: 2.103966\n",
      "Epoch 149, loss: 2.103947\n",
      "Epoch 150, loss: 2.103890\n",
      "Epoch 151, loss: 2.103838\n",
      "Epoch 152, loss: 2.103808\n",
      "Epoch 153, loss: 2.103777\n",
      "Epoch 154, loss: 2.103752\n",
      "Epoch 155, loss: 2.103706\n",
      "Epoch 156, loss: 2.103700\n",
      "Epoch 157, loss: 2.103662\n",
      "Epoch 158, loss: 2.103617\n",
      "Epoch 159, loss: 2.103566\n",
      "Epoch 160, loss: 2.103552\n",
      "Epoch 161, loss: 2.103508\n",
      "Epoch 162, loss: 2.103490\n",
      "Epoch 163, loss: 2.103452\n",
      "Epoch 164, loss: 2.103419\n",
      "Epoch 165, loss: 2.103368\n",
      "Epoch 166, loss: 2.103361\n",
      "Epoch 167, loss: 2.103300\n",
      "Epoch 168, loss: 2.103292\n",
      "Epoch 169, loss: 2.103240\n",
      "Epoch 170, loss: 2.103222\n",
      "Epoch 171, loss: 2.103169\n",
      "Epoch 172, loss: 2.103165\n",
      "Epoch 173, loss: 2.103134\n",
      "Epoch 174, loss: 2.103083\n",
      "Epoch 175, loss: 2.103071\n",
      "Epoch 176, loss: 2.102997\n",
      "Epoch 177, loss: 2.102986\n",
      "Epoch 178, loss: 2.102937\n",
      "Epoch 179, loss: 2.102915\n",
      "Epoch 180, loss: 2.102872\n",
      "Epoch 181, loss: 2.102854\n",
      "Epoch 182, loss: 2.102843\n",
      "Epoch 183, loss: 2.102781\n",
      "Epoch 184, loss: 2.102753\n",
      "Epoch 185, loss: 2.102755\n",
      "Epoch 186, loss: 2.102694\n",
      "Epoch 187, loss: 2.102666\n",
      "Epoch 188, loss: 2.102631\n",
      "Epoch 189, loss: 2.102609\n",
      "Epoch 190, loss: 2.102572\n",
      "Epoch 191, loss: 2.102534\n",
      "Epoch 192, loss: 2.102504\n",
      "Epoch 193, loss: 2.102464\n",
      "Epoch 194, loss: 2.102441\n",
      "Epoch 195, loss: 2.102413\n",
      "Epoch 196, loss: 2.102387\n",
      "Epoch 197, loss: 2.102359\n",
      "Epoch 198, loss: 2.102321\n",
      "Epoch 199, loss: 2.102283\n",
      "Epoch 0, loss: 2.108753\n",
      "Epoch 1, loss: 2.108617\n",
      "Epoch 2, loss: 2.108506\n",
      "Epoch 3, loss: 2.108421\n",
      "Epoch 4, loss: 2.108352\n",
      "Epoch 5, loss: 2.108258\n",
      "Epoch 6, loss: 2.108189\n",
      "Epoch 7, loss: 2.108114\n",
      "Epoch 8, loss: 2.108068\n",
      "Epoch 9, loss: 2.108004\n",
      "Epoch 10, loss: 2.107954\n",
      "Epoch 11, loss: 2.107881\n",
      "Epoch 12, loss: 2.107836\n",
      "Epoch 13, loss: 2.107773\n",
      "Epoch 14, loss: 2.107732\n",
      "Epoch 15, loss: 2.107672\n",
      "Epoch 16, loss: 2.107604\n",
      "Epoch 17, loss: 2.107570\n",
      "Epoch 18, loss: 2.107503\n",
      "Epoch 19, loss: 2.107446\n",
      "Epoch 20, loss: 2.107380\n",
      "Epoch 21, loss: 2.107354\n",
      "Epoch 22, loss: 2.107292\n",
      "Epoch 23, loss: 2.107239\n",
      "Epoch 24, loss: 2.107190\n",
      "Epoch 25, loss: 2.107148\n",
      "Epoch 26, loss: 2.107090\n",
      "Epoch 27, loss: 2.107045\n",
      "Epoch 28, loss: 2.107009\n",
      "Epoch 29, loss: 2.106942\n",
      "Epoch 30, loss: 2.106883\n",
      "Epoch 31, loss: 2.106855\n",
      "Epoch 32, loss: 2.106776\n",
      "Epoch 33, loss: 2.106764\n",
      "Epoch 34, loss: 2.106711\n",
      "Epoch 35, loss: 2.106654\n",
      "Epoch 36, loss: 2.106603\n",
      "Epoch 37, loss: 2.106579\n",
      "Epoch 38, loss: 2.106511\n",
      "Epoch 39, loss: 2.106458\n",
      "Epoch 40, loss: 2.106436\n",
      "Epoch 41, loss: 2.106390\n",
      "Epoch 42, loss: 2.106333\n",
      "Epoch 43, loss: 2.106287\n",
      "Epoch 44, loss: 2.106236\n",
      "Epoch 45, loss: 2.106194\n",
      "Epoch 46, loss: 2.106147\n",
      "Epoch 47, loss: 2.106097\n",
      "Epoch 48, loss: 2.106071\n",
      "Epoch 49, loss: 2.106039\n",
      "Epoch 50, loss: 2.105979\n",
      "Epoch 51, loss: 2.105922\n",
      "Epoch 52, loss: 2.105881\n",
      "Epoch 53, loss: 2.105853\n",
      "Epoch 54, loss: 2.105826\n",
      "Epoch 55, loss: 2.105778\n",
      "Epoch 56, loss: 2.105705\n",
      "Epoch 57, loss: 2.105658\n",
      "Epoch 58, loss: 2.105629\n",
      "Epoch 59, loss: 2.105608\n",
      "Epoch 60, loss: 2.105548\n",
      "Epoch 61, loss: 2.105538\n",
      "Epoch 62, loss: 2.105483\n",
      "Epoch 63, loss: 2.105448\n",
      "Epoch 64, loss: 2.105386\n",
      "Epoch 65, loss: 2.105363\n",
      "Epoch 66, loss: 2.105316\n",
      "Epoch 67, loss: 2.105252\n",
      "Epoch 68, loss: 2.105218\n",
      "Epoch 69, loss: 2.105186\n",
      "Epoch 70, loss: 2.105153\n",
      "Epoch 71, loss: 2.105100\n",
      "Epoch 72, loss: 2.105071\n",
      "Epoch 73, loss: 2.105021\n",
      "Epoch 74, loss: 2.104983\n",
      "Epoch 75, loss: 2.104956\n",
      "Epoch 76, loss: 2.104916\n",
      "Epoch 77, loss: 2.104862\n",
      "Epoch 78, loss: 2.104848\n",
      "Epoch 79, loss: 2.104800\n",
      "Epoch 80, loss: 2.104753\n",
      "Epoch 81, loss: 2.104713\n",
      "Epoch 82, loss: 2.104692\n",
      "Epoch 83, loss: 2.104640\n",
      "Epoch 84, loss: 2.104618\n",
      "Epoch 85, loss: 2.104556\n",
      "Epoch 86, loss: 2.104527\n",
      "Epoch 87, loss: 2.104490\n",
      "Epoch 88, loss: 2.104455\n",
      "Epoch 89, loss: 2.104440\n",
      "Epoch 90, loss: 2.104374\n",
      "Epoch 91, loss: 2.104337\n",
      "Epoch 92, loss: 2.104317\n",
      "Epoch 93, loss: 2.104273\n",
      "Epoch 94, loss: 2.104240\n",
      "Epoch 95, loss: 2.104193\n",
      "Epoch 96, loss: 2.104182\n",
      "Epoch 97, loss: 2.104111\n",
      "Epoch 98, loss: 2.104079\n",
      "Epoch 99, loss: 2.104058\n",
      "Epoch 100, loss: 2.104008\n",
      "Epoch 101, loss: 2.103988\n",
      "Epoch 102, loss: 2.103951\n",
      "Epoch 103, loss: 2.103919\n",
      "Epoch 104, loss: 2.103862\n",
      "Epoch 105, loss: 2.103845\n",
      "Epoch 106, loss: 2.103813\n",
      "Epoch 107, loss: 2.103772\n",
      "Epoch 108, loss: 2.103732\n",
      "Epoch 109, loss: 2.103687\n",
      "Epoch 110, loss: 2.103649\n",
      "Epoch 111, loss: 2.103629\n",
      "Epoch 112, loss: 2.103607\n",
      "Epoch 113, loss: 2.103567\n",
      "Epoch 114, loss: 2.103530\n",
      "Epoch 115, loss: 2.103491\n",
      "Epoch 116, loss: 2.103449\n",
      "Epoch 117, loss: 2.103397\n",
      "Epoch 118, loss: 2.103396\n",
      "Epoch 119, loss: 2.103352\n",
      "Epoch 120, loss: 2.103325\n",
      "Epoch 121, loss: 2.103276\n",
      "Epoch 122, loss: 2.103257\n",
      "Epoch 123, loss: 2.103217\n",
      "Epoch 124, loss: 2.103180\n",
      "Epoch 125, loss: 2.103142\n",
      "Epoch 126, loss: 2.103125\n",
      "Epoch 127, loss: 2.103077\n",
      "Epoch 128, loss: 2.103054\n",
      "Epoch 129, loss: 2.103006\n",
      "Epoch 130, loss: 2.102966\n",
      "Epoch 131, loss: 2.102951\n",
      "Epoch 132, loss: 2.102909\n",
      "Epoch 133, loss: 2.102878\n",
      "Epoch 134, loss: 2.102852\n",
      "Epoch 135, loss: 2.102824\n",
      "Epoch 136, loss: 2.102773\n",
      "Epoch 137, loss: 2.102747\n",
      "Epoch 138, loss: 2.102725\n",
      "Epoch 139, loss: 2.102683\n",
      "Epoch 140, loss: 2.102642\n",
      "Epoch 141, loss: 2.102615\n",
      "Epoch 142, loss: 2.102576\n",
      "Epoch 143, loss: 2.102550\n",
      "Epoch 144, loss: 2.102516\n",
      "Epoch 145, loss: 2.102491\n",
      "Epoch 146, loss: 2.102458\n",
      "Epoch 147, loss: 2.102413\n",
      "Epoch 148, loss: 2.102380\n",
      "Epoch 149, loss: 2.102359\n",
      "Epoch 150, loss: 2.102329\n",
      "Epoch 151, loss: 2.102303\n",
      "Epoch 152, loss: 2.102235\n",
      "Epoch 153, loss: 2.102223\n",
      "Epoch 154, loss: 2.102199\n",
      "Epoch 155, loss: 2.102177\n",
      "Epoch 156, loss: 2.102108\n",
      "Epoch 157, loss: 2.102100\n",
      "Epoch 158, loss: 2.102074\n",
      "Epoch 159, loss: 2.102045\n",
      "Epoch 160, loss: 2.101996\n",
      "Epoch 161, loss: 2.101979\n",
      "Epoch 162, loss: 2.101950\n",
      "Epoch 163, loss: 2.101918\n",
      "Epoch 164, loss: 2.101912\n",
      "Epoch 165, loss: 2.101853\n",
      "Epoch 166, loss: 2.101806\n",
      "Epoch 167, loss: 2.101788\n",
      "Epoch 168, loss: 2.101747\n",
      "Epoch 169, loss: 2.101734\n",
      "Epoch 170, loss: 2.101708\n",
      "Epoch 171, loss: 2.101670\n",
      "Epoch 172, loss: 2.101628\n",
      "Epoch 173, loss: 2.101602\n",
      "Epoch 174, loss: 2.101559\n",
      "Epoch 175, loss: 2.101554\n",
      "Epoch 176, loss: 2.101511\n",
      "Epoch 177, loss: 2.101514\n",
      "Epoch 178, loss: 2.101438\n",
      "Epoch 179, loss: 2.101415\n",
      "Epoch 180, loss: 2.101399\n",
      "Epoch 181, loss: 2.101341\n",
      "Epoch 182, loss: 2.101336\n",
      "Epoch 183, loss: 2.101299\n",
      "Epoch 184, loss: 2.101254\n",
      "Epoch 185, loss: 2.101222\n",
      "Epoch 186, loss: 2.101205\n",
      "Epoch 187, loss: 2.101168\n",
      "Epoch 188, loss: 2.101133\n",
      "Epoch 189, loss: 2.101119\n",
      "Epoch 190, loss: 2.101077\n",
      "Epoch 191, loss: 2.101056\n",
      "Epoch 192, loss: 2.101013\n",
      "Epoch 193, loss: 2.101001\n",
      "Epoch 194, loss: 2.100959\n",
      "Epoch 195, loss: 2.100934\n",
      "Epoch 196, loss: 2.100893\n",
      "Epoch 197, loss: 2.100885\n",
      "Epoch 198, loss: 2.100861\n",
      "Epoch 199, loss: 2.100802\n",
      "Epoch 0, loss: 2.103432\n",
      "Epoch 1, loss: 2.103371\n",
      "Epoch 2, loss: 2.103330\n",
      "Epoch 3, loss: 2.103286\n",
      "Epoch 4, loss: 2.103266\n",
      "Epoch 5, loss: 2.103211\n",
      "Epoch 6, loss: 2.103190\n",
      "Epoch 7, loss: 2.103161\n",
      "Epoch 8, loss: 2.103127\n",
      "Epoch 9, loss: 2.103087\n",
      "Epoch 10, loss: 2.103066\n",
      "Epoch 11, loss: 2.103021\n",
      "Epoch 12, loss: 2.103000\n",
      "Epoch 13, loss: 2.102968\n",
      "Epoch 14, loss: 2.102948\n",
      "Epoch 15, loss: 2.102910\n",
      "Epoch 16, loss: 2.102905\n",
      "Epoch 17, loss: 2.102865\n",
      "Epoch 18, loss: 2.102826\n",
      "Epoch 19, loss: 2.102802\n",
      "Epoch 20, loss: 2.102782\n",
      "Epoch 21, loss: 2.102753\n",
      "Epoch 22, loss: 2.102720\n",
      "Epoch 23, loss: 2.102703\n",
      "Epoch 24, loss: 2.102682\n",
      "Epoch 25, loss: 2.102653\n",
      "Epoch 26, loss: 2.102610\n",
      "Epoch 27, loss: 2.102589\n",
      "Epoch 28, loss: 2.102571\n",
      "Epoch 29, loss: 2.102542\n",
      "Epoch 30, loss: 2.102516\n",
      "Epoch 31, loss: 2.102496\n",
      "Epoch 32, loss: 2.102459\n",
      "Epoch 33, loss: 2.102440\n",
      "Epoch 34, loss: 2.102424\n",
      "Epoch 35, loss: 2.102394\n",
      "Epoch 36, loss: 2.102372\n",
      "Epoch 37, loss: 2.102338\n",
      "Epoch 38, loss: 2.102324\n",
      "Epoch 39, loss: 2.102288\n",
      "Epoch 40, loss: 2.102282\n",
      "Epoch 41, loss: 2.102243\n",
      "Epoch 42, loss: 2.102210\n",
      "Epoch 43, loss: 2.102199\n",
      "Epoch 44, loss: 2.102176\n",
      "Epoch 45, loss: 2.102148\n",
      "Epoch 46, loss: 2.102118\n",
      "Epoch 47, loss: 2.102112\n",
      "Epoch 48, loss: 2.102072\n",
      "Epoch 49, loss: 2.102056\n",
      "Epoch 50, loss: 2.102024\n",
      "Epoch 51, loss: 2.102000\n",
      "Epoch 52, loss: 2.101961\n",
      "Epoch 53, loss: 2.101950\n",
      "Epoch 54, loss: 2.101917\n",
      "Epoch 55, loss: 2.101897\n",
      "Epoch 56, loss: 2.101882\n",
      "Epoch 57, loss: 2.101844\n",
      "Epoch 58, loss: 2.101853\n",
      "Epoch 59, loss: 2.101817\n",
      "Epoch 60, loss: 2.101783\n",
      "Epoch 61, loss: 2.101771\n",
      "Epoch 62, loss: 2.101741\n",
      "Epoch 63, loss: 2.101721\n",
      "Epoch 64, loss: 2.101696\n",
      "Epoch 65, loss: 2.101674\n",
      "Epoch 66, loss: 2.101645\n",
      "Epoch 67, loss: 2.101625\n",
      "Epoch 68, loss: 2.101614\n",
      "Epoch 69, loss: 2.101575\n",
      "Epoch 70, loss: 2.101576\n",
      "Epoch 71, loss: 2.101556\n",
      "Epoch 72, loss: 2.101517\n",
      "Epoch 73, loss: 2.101495\n",
      "Epoch 74, loss: 2.101468\n",
      "Epoch 75, loss: 2.101454\n",
      "Epoch 76, loss: 2.101421\n",
      "Epoch 77, loss: 2.101409\n",
      "Epoch 78, loss: 2.101390\n",
      "Epoch 79, loss: 2.101362\n",
      "Epoch 80, loss: 2.101355\n",
      "Epoch 81, loss: 2.101317\n",
      "Epoch 82, loss: 2.101289\n",
      "Epoch 83, loss: 2.101274\n",
      "Epoch 84, loss: 2.101246\n",
      "Epoch 85, loss: 2.101226\n",
      "Epoch 86, loss: 2.101229\n",
      "Epoch 87, loss: 2.101176\n",
      "Epoch 88, loss: 2.101169\n",
      "Epoch 89, loss: 2.101133\n",
      "Epoch 90, loss: 2.101127\n",
      "Epoch 91, loss: 2.101105\n",
      "Epoch 92, loss: 2.101069\n",
      "Epoch 93, loss: 2.101049\n",
      "Epoch 94, loss: 2.101033\n",
      "Epoch 95, loss: 2.101021\n",
      "Epoch 96, loss: 2.100975\n",
      "Epoch 97, loss: 2.100975\n",
      "Epoch 98, loss: 2.100941\n",
      "Epoch 99, loss: 2.100912\n",
      "Epoch 100, loss: 2.100902\n",
      "Epoch 101, loss: 2.100862\n",
      "Epoch 102, loss: 2.100849\n",
      "Epoch 103, loss: 2.100839\n",
      "Epoch 104, loss: 2.100817\n",
      "Epoch 105, loss: 2.100795\n",
      "Epoch 106, loss: 2.100773\n",
      "Epoch 107, loss: 2.100749\n",
      "Epoch 108, loss: 2.100718\n",
      "Epoch 109, loss: 2.100722\n",
      "Epoch 110, loss: 2.100687\n",
      "Epoch 111, loss: 2.100674\n",
      "Epoch 112, loss: 2.100634\n",
      "Epoch 113, loss: 2.100614\n",
      "Epoch 114, loss: 2.100600\n",
      "Epoch 115, loss: 2.100578\n",
      "Epoch 116, loss: 2.100554\n",
      "Epoch 117, loss: 2.100525\n",
      "Epoch 118, loss: 2.100511\n",
      "Epoch 119, loss: 2.100511\n",
      "Epoch 120, loss: 2.100464\n",
      "Epoch 121, loss: 2.100467\n",
      "Epoch 122, loss: 2.100449\n",
      "Epoch 123, loss: 2.100408\n",
      "Epoch 124, loss: 2.100389\n",
      "Epoch 125, loss: 2.100361\n",
      "Epoch 126, loss: 2.100348\n",
      "Epoch 127, loss: 2.100308\n",
      "Epoch 128, loss: 2.100297\n",
      "Epoch 129, loss: 2.100291\n",
      "Epoch 130, loss: 2.100260\n",
      "Epoch 131, loss: 2.100227\n",
      "Epoch 132, loss: 2.100207\n",
      "Epoch 133, loss: 2.100191\n",
      "Epoch 134, loss: 2.100176\n",
      "Epoch 135, loss: 2.100158\n",
      "Epoch 136, loss: 2.100130\n",
      "Epoch 137, loss: 2.100118\n",
      "Epoch 138, loss: 2.100086\n",
      "Epoch 139, loss: 2.100071\n",
      "Epoch 140, loss: 2.100054\n",
      "Epoch 141, loss: 2.100033\n",
      "Epoch 142, loss: 2.100011\n",
      "Epoch 143, loss: 2.099978\n",
      "Epoch 144, loss: 2.099981\n",
      "Epoch 145, loss: 2.099937\n",
      "Epoch 146, loss: 2.099930\n",
      "Epoch 147, loss: 2.099919\n",
      "Epoch 148, loss: 2.099892\n",
      "Epoch 149, loss: 2.099862\n",
      "Epoch 150, loss: 2.099846\n",
      "Epoch 151, loss: 2.099823\n",
      "Epoch 152, loss: 2.099786\n",
      "Epoch 153, loss: 2.099795\n",
      "Epoch 154, loss: 2.099763\n",
      "Epoch 155, loss: 2.099726\n",
      "Epoch 156, loss: 2.099724\n",
      "Epoch 157, loss: 2.099715\n",
      "Epoch 158, loss: 2.099688\n",
      "Epoch 159, loss: 2.099645\n",
      "Epoch 160, loss: 2.099644\n",
      "Epoch 161, loss: 2.099612\n",
      "Epoch 162, loss: 2.099590\n",
      "Epoch 163, loss: 2.099579\n",
      "Epoch 164, loss: 2.099555\n",
      "Epoch 165, loss: 2.099535\n",
      "Epoch 166, loss: 2.099516\n",
      "Epoch 167, loss: 2.099497\n",
      "Epoch 168, loss: 2.099464\n",
      "Epoch 169, loss: 2.099457\n",
      "Epoch 170, loss: 2.099438\n",
      "Epoch 171, loss: 2.099428\n",
      "Epoch 172, loss: 2.099401\n",
      "Epoch 173, loss: 2.099383\n",
      "Epoch 174, loss: 2.099365\n",
      "Epoch 175, loss: 2.099327\n",
      "Epoch 176, loss: 2.099313\n",
      "Epoch 177, loss: 2.099305\n",
      "Epoch 178, loss: 2.099273\n",
      "Epoch 179, loss: 2.099246\n",
      "Epoch 180, loss: 2.099238\n",
      "Epoch 181, loss: 2.099214\n",
      "Epoch 182, loss: 2.099193\n",
      "Epoch 183, loss: 2.099176\n",
      "Epoch 184, loss: 2.099168\n",
      "Epoch 185, loss: 2.099128\n",
      "Epoch 186, loss: 2.099110\n",
      "Epoch 187, loss: 2.099080\n",
      "Epoch 188, loss: 2.099084\n",
      "Epoch 189, loss: 2.099057\n",
      "Epoch 190, loss: 2.099036\n",
      "Epoch 191, loss: 2.099008\n",
      "Epoch 192, loss: 2.098991\n",
      "Epoch 193, loss: 2.098974\n",
      "Epoch 194, loss: 2.098967\n",
      "Epoch 195, loss: 2.098931\n",
      "Epoch 196, loss: 2.098922\n",
      "Epoch 197, loss: 2.098895\n",
      "Epoch 198, loss: 2.098867\n",
      "Epoch 199, loss: 2.098830\n",
      "Epoch 0, loss: 2.100278\n",
      "Epoch 1, loss: 2.100235\n",
      "Epoch 2, loss: 2.100161\n",
      "Epoch 3, loss: 2.100121\n",
      "Epoch 4, loss: 2.100061\n",
      "Epoch 5, loss: 2.100026\n",
      "Epoch 6, loss: 2.099973\n",
      "Epoch 7, loss: 2.099912\n",
      "Epoch 8, loss: 2.099892\n",
      "Epoch 9, loss: 2.099837\n",
      "Epoch 10, loss: 2.099800\n",
      "Epoch 11, loss: 2.099750\n",
      "Epoch 12, loss: 2.099698\n",
      "Epoch 13, loss: 2.099669\n",
      "Epoch 14, loss: 2.099615\n",
      "Epoch 15, loss: 2.099578\n",
      "Epoch 16, loss: 2.099548\n",
      "Epoch 17, loss: 2.099503\n",
      "Epoch 18, loss: 2.099478\n",
      "Epoch 19, loss: 2.099432\n",
      "Epoch 20, loss: 2.099401\n",
      "Epoch 21, loss: 2.099332\n",
      "Epoch 22, loss: 2.099289\n",
      "Epoch 23, loss: 2.099276\n",
      "Epoch 24, loss: 2.099237\n",
      "Epoch 25, loss: 2.099197\n",
      "Epoch 26, loss: 2.099160\n",
      "Epoch 27, loss: 2.099115\n",
      "Epoch 28, loss: 2.099084\n",
      "Epoch 29, loss: 2.099048\n",
      "Epoch 30, loss: 2.099012\n",
      "Epoch 31, loss: 2.098974\n",
      "Epoch 32, loss: 2.098942\n",
      "Epoch 33, loss: 2.098899\n",
      "Epoch 34, loss: 2.098863\n",
      "Epoch 35, loss: 2.098840\n",
      "Epoch 36, loss: 2.098808\n",
      "Epoch 37, loss: 2.098758\n",
      "Epoch 38, loss: 2.098729\n",
      "Epoch 39, loss: 2.098702\n",
      "Epoch 40, loss: 2.098659\n",
      "Epoch 41, loss: 2.098620\n",
      "Epoch 42, loss: 2.098589\n",
      "Epoch 43, loss: 2.098559\n",
      "Epoch 44, loss: 2.098515\n",
      "Epoch 45, loss: 2.098477\n",
      "Epoch 46, loss: 2.098464\n",
      "Epoch 47, loss: 2.098424\n",
      "Epoch 48, loss: 2.098389\n",
      "Epoch 49, loss: 2.098352\n",
      "Epoch 50, loss: 2.098316\n",
      "Epoch 51, loss: 2.098319\n",
      "Epoch 52, loss: 2.098244\n",
      "Epoch 53, loss: 2.098243\n",
      "Epoch 54, loss: 2.098182\n",
      "Epoch 55, loss: 2.098154\n",
      "Epoch 56, loss: 2.098132\n",
      "Epoch 57, loss: 2.098088\n",
      "Epoch 58, loss: 2.098054\n",
      "Epoch 59, loss: 2.098041\n",
      "Epoch 60, loss: 2.097981\n",
      "Epoch 61, loss: 2.097980\n",
      "Epoch 62, loss: 2.097935\n",
      "Epoch 63, loss: 2.097920\n",
      "Epoch 64, loss: 2.097869\n",
      "Epoch 65, loss: 2.097835\n",
      "Epoch 66, loss: 2.097819\n",
      "Epoch 67, loss: 2.097778\n",
      "Epoch 68, loss: 2.097735\n",
      "Epoch 69, loss: 2.097712\n",
      "Epoch 70, loss: 2.097672\n",
      "Epoch 71, loss: 2.097669\n",
      "Epoch 72, loss: 2.097627\n",
      "Epoch 73, loss: 2.097592\n",
      "Epoch 74, loss: 2.097559\n",
      "Epoch 75, loss: 2.097525\n",
      "Epoch 76, loss: 2.097502\n",
      "Epoch 77, loss: 2.097461\n",
      "Epoch 78, loss: 2.097456\n",
      "Epoch 79, loss: 2.097397\n",
      "Epoch 80, loss: 2.097373\n",
      "Epoch 81, loss: 2.097346\n",
      "Epoch 82, loss: 2.097318\n",
      "Epoch 83, loss: 2.097281\n",
      "Epoch 84, loss: 2.097256\n",
      "Epoch 85, loss: 2.097223\n",
      "Epoch 86, loss: 2.097203\n",
      "Epoch 87, loss: 2.097180\n",
      "Epoch 88, loss: 2.097129\n",
      "Epoch 89, loss: 2.097105\n",
      "Epoch 90, loss: 2.097093\n",
      "Epoch 91, loss: 2.097049\n",
      "Epoch 92, loss: 2.097016\n",
      "Epoch 93, loss: 2.097003\n",
      "Epoch 94, loss: 2.096980\n",
      "Epoch 95, loss: 2.096929\n",
      "Epoch 96, loss: 2.096911\n",
      "Epoch 97, loss: 2.096884\n",
      "Epoch 98, loss: 2.096855\n",
      "Epoch 99, loss: 2.096835\n",
      "Epoch 100, loss: 2.096817\n",
      "Epoch 101, loss: 2.096779\n",
      "Epoch 102, loss: 2.096734\n",
      "Epoch 103, loss: 2.096720\n",
      "Epoch 104, loss: 2.096712\n",
      "Epoch 105, loss: 2.096649\n",
      "Epoch 106, loss: 2.096636\n",
      "Epoch 107, loss: 2.096582\n",
      "Epoch 108, loss: 2.096576\n",
      "Epoch 109, loss: 2.096540\n",
      "Epoch 110, loss: 2.096526\n",
      "Epoch 111, loss: 2.096478\n",
      "Epoch 112, loss: 2.096458\n",
      "Epoch 113, loss: 2.096430\n",
      "Epoch 114, loss: 2.096415\n",
      "Epoch 115, loss: 2.096376\n",
      "Epoch 116, loss: 2.096346\n",
      "Epoch 117, loss: 2.096336\n",
      "Epoch 118, loss: 2.096298\n",
      "Epoch 119, loss: 2.096259\n",
      "Epoch 120, loss: 2.096229\n",
      "Epoch 121, loss: 2.096214\n",
      "Epoch 122, loss: 2.096187\n",
      "Epoch 123, loss: 2.096176\n",
      "Epoch 124, loss: 2.096151\n",
      "Epoch 125, loss: 2.096108\n",
      "Epoch 126, loss: 2.096088\n",
      "Epoch 127, loss: 2.096061\n",
      "Epoch 128, loss: 2.096032\n",
      "Epoch 129, loss: 2.096020\n",
      "Epoch 130, loss: 2.095972\n",
      "Epoch 131, loss: 2.095940\n",
      "Epoch 132, loss: 2.095924\n",
      "Epoch 133, loss: 2.095902\n",
      "Epoch 134, loss: 2.095861\n",
      "Epoch 135, loss: 2.095838\n",
      "Epoch 136, loss: 2.095813\n",
      "Epoch 137, loss: 2.095779\n",
      "Epoch 138, loss: 2.095765\n",
      "Epoch 139, loss: 2.095725\n",
      "Epoch 140, loss: 2.095713\n",
      "Epoch 141, loss: 2.095689\n",
      "Epoch 142, loss: 2.095660\n",
      "Epoch 143, loss: 2.095632\n",
      "Epoch 144, loss: 2.095614\n",
      "Epoch 145, loss: 2.095569\n",
      "Epoch 146, loss: 2.095556\n",
      "Epoch 147, loss: 2.095502\n",
      "Epoch 148, loss: 2.095517\n",
      "Epoch 149, loss: 2.095486\n",
      "Epoch 150, loss: 2.095456\n",
      "Epoch 151, loss: 2.095418\n",
      "Epoch 152, loss: 2.095394\n",
      "Epoch 153, loss: 2.095369\n",
      "Epoch 154, loss: 2.095345\n",
      "Epoch 155, loss: 2.095313\n",
      "Epoch 156, loss: 2.095292\n",
      "Epoch 157, loss: 2.095277\n",
      "Epoch 158, loss: 2.095248\n",
      "Epoch 159, loss: 2.095214\n",
      "Epoch 160, loss: 2.095176\n",
      "Epoch 161, loss: 2.095161\n",
      "Epoch 162, loss: 2.095148\n",
      "Epoch 163, loss: 2.095118\n",
      "Epoch 164, loss: 2.095086\n",
      "Epoch 165, loss: 2.095087\n",
      "Epoch 166, loss: 2.095036\n",
      "Epoch 167, loss: 2.095014\n",
      "Epoch 168, loss: 2.094986\n",
      "Epoch 169, loss: 2.094962\n",
      "Epoch 170, loss: 2.094941\n",
      "Epoch 171, loss: 2.094917\n",
      "Epoch 172, loss: 2.094873\n",
      "Epoch 173, loss: 2.094853\n",
      "Epoch 174, loss: 2.094845\n",
      "Epoch 175, loss: 2.094823\n",
      "Epoch 176, loss: 2.094794\n",
      "Epoch 177, loss: 2.094754\n",
      "Epoch 178, loss: 2.094743\n",
      "Epoch 179, loss: 2.094743\n",
      "Epoch 180, loss: 2.094695\n",
      "Epoch 181, loss: 2.094683\n",
      "Epoch 182, loss: 2.094651\n",
      "Epoch 183, loss: 2.094625\n",
      "Epoch 184, loss: 2.094578\n",
      "Epoch 185, loss: 2.094579\n",
      "Epoch 186, loss: 2.094555\n",
      "Epoch 187, loss: 2.094522\n",
      "Epoch 188, loss: 2.094502\n",
      "Epoch 189, loss: 2.094480\n",
      "Epoch 190, loss: 2.094455\n",
      "Epoch 191, loss: 2.094421\n",
      "Epoch 192, loss: 2.094387\n",
      "Epoch 193, loss: 2.094372\n",
      "Epoch 194, loss: 2.094367\n",
      "Epoch 195, loss: 2.094350\n",
      "Epoch 196, loss: 2.094299\n",
      "Epoch 197, loss: 2.094293\n",
      "Epoch 198, loss: 2.094256\n",
      "Epoch 199, loss: 2.094224\n",
      "Epoch 0, loss: 2.096626\n",
      "Epoch 1, loss: 2.096507\n",
      "Epoch 2, loss: 2.096386\n",
      "Epoch 3, loss: 2.096276\n",
      "Epoch 4, loss: 2.096192\n",
      "Epoch 5, loss: 2.096111\n",
      "Epoch 6, loss: 2.096047\n",
      "Epoch 7, loss: 2.095974\n",
      "Epoch 8, loss: 2.095895\n",
      "Epoch 9, loss: 2.095826\n",
      "Epoch 10, loss: 2.095773\n",
      "Epoch 11, loss: 2.095697\n",
      "Epoch 12, loss: 2.095641\n",
      "Epoch 13, loss: 2.095583\n",
      "Epoch 14, loss: 2.095513\n",
      "Epoch 15, loss: 2.095446\n",
      "Epoch 16, loss: 2.095403\n",
      "Epoch 17, loss: 2.095342\n",
      "Epoch 18, loss: 2.095273\n",
      "Epoch 19, loss: 2.095210\n",
      "Epoch 20, loss: 2.095154\n",
      "Epoch 21, loss: 2.095121\n",
      "Epoch 22, loss: 2.095049\n",
      "Epoch 23, loss: 2.094993\n",
      "Epoch 24, loss: 2.094927\n",
      "Epoch 25, loss: 2.094881\n",
      "Epoch 26, loss: 2.094825\n",
      "Epoch 27, loss: 2.094784\n",
      "Epoch 28, loss: 2.094734\n",
      "Epoch 29, loss: 2.094677\n",
      "Epoch 30, loss: 2.094619\n",
      "Epoch 31, loss: 2.094572\n",
      "Epoch 32, loss: 2.094540\n",
      "Epoch 33, loss: 2.094487\n",
      "Epoch 34, loss: 2.094409\n",
      "Epoch 35, loss: 2.094385\n",
      "Epoch 36, loss: 2.094326\n",
      "Epoch 37, loss: 2.094268\n",
      "Epoch 38, loss: 2.094237\n",
      "Epoch 39, loss: 2.094185\n",
      "Epoch 40, loss: 2.094134\n",
      "Epoch 41, loss: 2.094091\n",
      "Epoch 42, loss: 2.094040\n",
      "Epoch 43, loss: 2.093997\n",
      "Epoch 44, loss: 2.093947\n",
      "Epoch 45, loss: 2.093902\n",
      "Epoch 46, loss: 2.093856\n",
      "Epoch 47, loss: 2.093804\n",
      "Epoch 48, loss: 2.093770\n",
      "Epoch 49, loss: 2.093729\n",
      "Epoch 50, loss: 2.093669\n",
      "Epoch 51, loss: 2.093631\n",
      "Epoch 52, loss: 2.093597\n",
      "Epoch 53, loss: 2.093546\n",
      "Epoch 54, loss: 2.093485\n",
      "Epoch 55, loss: 2.093466\n",
      "Epoch 56, loss: 2.093420\n",
      "Epoch 57, loss: 2.093376\n",
      "Epoch 58, loss: 2.093330\n",
      "Epoch 59, loss: 2.093298\n",
      "Epoch 60, loss: 2.093254\n",
      "Epoch 61, loss: 2.093200\n",
      "Epoch 62, loss: 2.093153\n",
      "Epoch 63, loss: 2.093114\n",
      "Epoch 64, loss: 2.093093\n",
      "Epoch 65, loss: 2.093044\n",
      "Epoch 66, loss: 2.093012\n",
      "Epoch 67, loss: 2.092960\n",
      "Epoch 68, loss: 2.092916\n",
      "Epoch 69, loss: 2.092874\n",
      "Epoch 70, loss: 2.092872\n",
      "Epoch 71, loss: 2.092803\n",
      "Epoch 72, loss: 2.092789\n",
      "Epoch 73, loss: 2.092720\n",
      "Epoch 74, loss: 2.092692\n",
      "Epoch 75, loss: 2.092651\n",
      "Epoch 76, loss: 2.092609\n",
      "Epoch 77, loss: 2.092574\n",
      "Epoch 78, loss: 2.092537\n",
      "Epoch 79, loss: 2.092497\n",
      "Epoch 80, loss: 2.092461\n",
      "Epoch 81, loss: 2.092440\n",
      "Epoch 82, loss: 2.092377\n",
      "Epoch 83, loss: 2.092349\n",
      "Epoch 84, loss: 2.092310\n",
      "Epoch 85, loss: 2.092288\n",
      "Epoch 86, loss: 2.092241\n",
      "Epoch 87, loss: 2.092222\n",
      "Epoch 88, loss: 2.092168\n",
      "Epoch 89, loss: 2.092136\n",
      "Epoch 90, loss: 2.092101\n",
      "Epoch 91, loss: 2.092052\n",
      "Epoch 92, loss: 2.092032\n",
      "Epoch 93, loss: 2.091987\n",
      "Epoch 94, loss: 2.091964\n",
      "Epoch 95, loss: 2.091942\n",
      "Epoch 96, loss: 2.091882\n",
      "Epoch 97, loss: 2.091869\n",
      "Epoch 98, loss: 2.091835\n",
      "Epoch 99, loss: 2.091803\n",
      "Epoch 100, loss: 2.091754\n",
      "Epoch 101, loss: 2.091723\n",
      "Epoch 102, loss: 2.091684\n",
      "Epoch 103, loss: 2.091667\n",
      "Epoch 104, loss: 2.091626\n",
      "Epoch 105, loss: 2.091583\n",
      "Epoch 106, loss: 2.091556\n",
      "Epoch 107, loss: 2.091523\n",
      "Epoch 108, loss: 2.091496\n",
      "Epoch 109, loss: 2.091455\n",
      "Epoch 110, loss: 2.091429\n",
      "Epoch 111, loss: 2.091403\n",
      "Epoch 112, loss: 2.091365\n",
      "Epoch 113, loss: 2.091324\n",
      "Epoch 114, loss: 2.091299\n",
      "Epoch 115, loss: 2.091255\n",
      "Epoch 116, loss: 2.091230\n",
      "Epoch 117, loss: 2.091202\n",
      "Epoch 118, loss: 2.091169\n",
      "Epoch 119, loss: 2.091127\n",
      "Epoch 120, loss: 2.091098\n",
      "Epoch 121, loss: 2.091064\n",
      "Epoch 122, loss: 2.091036\n",
      "Epoch 123, loss: 2.091004\n",
      "Epoch 124, loss: 2.090973\n",
      "Epoch 125, loss: 2.090943\n",
      "Epoch 126, loss: 2.090919\n",
      "Epoch 127, loss: 2.090878\n",
      "Epoch 128, loss: 2.090849\n",
      "Epoch 129, loss: 2.090829\n",
      "Epoch 130, loss: 2.090792\n",
      "Epoch 131, loss: 2.090762\n",
      "Epoch 132, loss: 2.090716\n",
      "Epoch 133, loss: 2.090697\n",
      "Epoch 134, loss: 2.090683\n",
      "Epoch 135, loss: 2.090664\n",
      "Epoch 136, loss: 2.090609\n",
      "Epoch 137, loss: 2.090576\n",
      "Epoch 138, loss: 2.090539\n",
      "Epoch 139, loss: 2.090527\n",
      "Epoch 140, loss: 2.090477\n",
      "Epoch 141, loss: 2.090443\n",
      "Epoch 142, loss: 2.090427\n",
      "Epoch 143, loss: 2.090401\n",
      "Epoch 144, loss: 2.090372\n",
      "Epoch 145, loss: 2.090336\n",
      "Epoch 146, loss: 2.090304\n",
      "Epoch 147, loss: 2.090265\n",
      "Epoch 148, loss: 2.090250\n",
      "Epoch 149, loss: 2.090227\n",
      "Epoch 150, loss: 2.090196\n",
      "Epoch 151, loss: 2.090165\n",
      "Epoch 152, loss: 2.090126\n",
      "Epoch 153, loss: 2.090102\n",
      "Epoch 154, loss: 2.090075\n",
      "Epoch 155, loss: 2.090047\n",
      "Epoch 156, loss: 2.089999\n",
      "Epoch 157, loss: 2.090001\n",
      "Epoch 158, loss: 2.089950\n",
      "Epoch 159, loss: 2.089938\n",
      "Epoch 160, loss: 2.089899\n",
      "Epoch 161, loss: 2.089863\n",
      "Epoch 162, loss: 2.089831\n",
      "Epoch 163, loss: 2.089815\n",
      "Epoch 164, loss: 2.089788\n",
      "Epoch 165, loss: 2.089745\n",
      "Epoch 166, loss: 2.089735\n",
      "Epoch 167, loss: 2.089703\n",
      "Epoch 168, loss: 2.089681\n",
      "Epoch 169, loss: 2.089646\n",
      "Epoch 170, loss: 2.089625\n",
      "Epoch 171, loss: 2.089603\n",
      "Epoch 172, loss: 2.089555\n",
      "Epoch 173, loss: 2.089534\n",
      "Epoch 174, loss: 2.089515\n",
      "Epoch 175, loss: 2.089482\n",
      "Epoch 176, loss: 2.089466\n",
      "Epoch 177, loss: 2.089432\n",
      "Epoch 178, loss: 2.089414\n",
      "Epoch 179, loss: 2.089374\n",
      "Epoch 180, loss: 2.089351\n",
      "Epoch 181, loss: 2.089332\n",
      "Epoch 182, loss: 2.089291\n",
      "Epoch 183, loss: 2.089253\n",
      "Epoch 184, loss: 2.089240\n",
      "Epoch 185, loss: 2.089203\n",
      "Epoch 186, loss: 2.089201\n",
      "Epoch 187, loss: 2.089155\n",
      "Epoch 188, loss: 2.089142\n",
      "Epoch 189, loss: 2.089106\n",
      "Epoch 190, loss: 2.089068\n",
      "Epoch 191, loss: 2.089060\n",
      "Epoch 192, loss: 2.089034\n",
      "Epoch 193, loss: 2.088984\n",
      "Epoch 194, loss: 2.088980\n",
      "Epoch 195, loss: 2.088946\n",
      "Epoch 196, loss: 2.088922\n",
      "Epoch 197, loss: 2.088894\n",
      "Epoch 198, loss: 2.088862\n",
      "Epoch 199, loss: 2.088822\n",
      "Epoch 0, loss: 2.098594\n",
      "Epoch 1, loss: 2.098444\n",
      "Epoch 2, loss: 2.098312\n",
      "Epoch 3, loss: 2.098221\n",
      "Epoch 4, loss: 2.098123\n",
      "Epoch 5, loss: 2.098032\n",
      "Epoch 6, loss: 2.097947\n",
      "Epoch 7, loss: 2.097871\n",
      "Epoch 8, loss: 2.097805\n",
      "Epoch 9, loss: 2.097750\n",
      "Epoch 10, loss: 2.097693\n",
      "Epoch 11, loss: 2.097625\n",
      "Epoch 12, loss: 2.097571\n",
      "Epoch 13, loss: 2.097526\n",
      "Epoch 14, loss: 2.097461\n",
      "Epoch 15, loss: 2.097413\n",
      "Epoch 16, loss: 2.097360\n",
      "Epoch 17, loss: 2.097311\n",
      "Epoch 18, loss: 2.097272\n",
      "Epoch 19, loss: 2.097225\n",
      "Epoch 20, loss: 2.097163\n",
      "Epoch 21, loss: 2.097139\n",
      "Epoch 22, loss: 2.097089\n",
      "Epoch 23, loss: 2.097024\n",
      "Epoch 24, loss: 2.097021\n",
      "Epoch 25, loss: 2.096944\n",
      "Epoch 26, loss: 2.096902\n",
      "Epoch 27, loss: 2.096867\n",
      "Epoch 28, loss: 2.096846\n",
      "Epoch 29, loss: 2.096787\n",
      "Epoch 30, loss: 2.096735\n",
      "Epoch 31, loss: 2.096711\n",
      "Epoch 32, loss: 2.096665\n",
      "Epoch 33, loss: 2.096635\n",
      "Epoch 34, loss: 2.096574\n",
      "Epoch 35, loss: 2.096534\n",
      "Epoch 36, loss: 2.096489\n",
      "Epoch 37, loss: 2.096474\n",
      "Epoch 38, loss: 2.096429\n",
      "Epoch 39, loss: 2.096394\n",
      "Epoch 40, loss: 2.096359\n",
      "Epoch 41, loss: 2.096300\n",
      "Epoch 42, loss: 2.096284\n",
      "Epoch 43, loss: 2.096227\n",
      "Epoch 44, loss: 2.096190\n",
      "Epoch 45, loss: 2.096154\n",
      "Epoch 46, loss: 2.096114\n",
      "Epoch 47, loss: 2.096109\n",
      "Epoch 48, loss: 2.096056\n",
      "Epoch 49, loss: 2.096016\n",
      "Epoch 50, loss: 2.095984\n",
      "Epoch 51, loss: 2.095945\n",
      "Epoch 52, loss: 2.095916\n",
      "Epoch 53, loss: 2.095879\n",
      "Epoch 54, loss: 2.095844\n",
      "Epoch 55, loss: 2.095805\n",
      "Epoch 56, loss: 2.095749\n",
      "Epoch 57, loss: 2.095730\n",
      "Epoch 58, loss: 2.095707\n",
      "Epoch 59, loss: 2.095653\n",
      "Epoch 60, loss: 2.095636\n",
      "Epoch 61, loss: 2.095604\n",
      "Epoch 62, loss: 2.095557\n",
      "Epoch 63, loss: 2.095535\n",
      "Epoch 64, loss: 2.095490\n",
      "Epoch 65, loss: 2.095460\n",
      "Epoch 66, loss: 2.095419\n",
      "Epoch 67, loss: 2.095381\n",
      "Epoch 68, loss: 2.095352\n",
      "Epoch 69, loss: 2.095330\n",
      "Epoch 70, loss: 2.095282\n",
      "Epoch 71, loss: 2.095274\n",
      "Epoch 72, loss: 2.095242\n",
      "Epoch 73, loss: 2.095215\n",
      "Epoch 74, loss: 2.095175\n",
      "Epoch 75, loss: 2.095125\n",
      "Epoch 76, loss: 2.095082\n",
      "Epoch 77, loss: 2.095070\n",
      "Epoch 78, loss: 2.095041\n",
      "Epoch 79, loss: 2.095012\n",
      "Epoch 80, loss: 2.094963\n",
      "Epoch 81, loss: 2.094937\n",
      "Epoch 82, loss: 2.094897\n",
      "Epoch 83, loss: 2.094872\n",
      "Epoch 84, loss: 2.094863\n",
      "Epoch 85, loss: 2.094821\n",
      "Epoch 86, loss: 2.094798\n",
      "Epoch 87, loss: 2.094764\n",
      "Epoch 88, loss: 2.094721\n",
      "Epoch 89, loss: 2.094690\n",
      "Epoch 90, loss: 2.094649\n",
      "Epoch 91, loss: 2.094632\n",
      "Epoch 92, loss: 2.094602\n",
      "Epoch 93, loss: 2.094575\n",
      "Epoch 94, loss: 2.094547\n",
      "Epoch 95, loss: 2.094502\n",
      "Epoch 96, loss: 2.094482\n",
      "Epoch 97, loss: 2.094445\n",
      "Epoch 98, loss: 2.094421\n",
      "Epoch 99, loss: 2.094394\n",
      "Epoch 100, loss: 2.094376\n",
      "Epoch 101, loss: 2.094325\n",
      "Epoch 102, loss: 2.094300\n",
      "Epoch 103, loss: 2.094284\n",
      "Epoch 104, loss: 2.094247\n",
      "Epoch 105, loss: 2.094218\n",
      "Epoch 106, loss: 2.094201\n",
      "Epoch 107, loss: 2.094159\n",
      "Epoch 108, loss: 2.094145\n",
      "Epoch 109, loss: 2.094087\n",
      "Epoch 110, loss: 2.094087\n",
      "Epoch 111, loss: 2.094048\n",
      "Epoch 112, loss: 2.094011\n",
      "Epoch 113, loss: 2.093982\n",
      "Epoch 114, loss: 2.093950\n",
      "Epoch 115, loss: 2.093936\n",
      "Epoch 116, loss: 2.093884\n",
      "Epoch 117, loss: 2.093863\n",
      "Epoch 118, loss: 2.093847\n",
      "Epoch 119, loss: 2.093832\n",
      "Epoch 120, loss: 2.093784\n",
      "Epoch 121, loss: 2.093760\n",
      "Epoch 122, loss: 2.093735\n",
      "Epoch 123, loss: 2.093694\n",
      "Epoch 124, loss: 2.093674\n",
      "Epoch 125, loss: 2.093641\n",
      "Epoch 126, loss: 2.093611\n",
      "Epoch 127, loss: 2.093585\n",
      "Epoch 128, loss: 2.093574\n",
      "Epoch 129, loss: 2.093561\n",
      "Epoch 130, loss: 2.093518\n",
      "Epoch 131, loss: 2.093479\n",
      "Epoch 132, loss: 2.093447\n",
      "Epoch 133, loss: 2.093423\n",
      "Epoch 134, loss: 2.093408\n",
      "Epoch 135, loss: 2.093368\n",
      "Epoch 136, loss: 2.093340\n",
      "Epoch 137, loss: 2.093309\n",
      "Epoch 138, loss: 2.093275\n",
      "Epoch 139, loss: 2.093265\n",
      "Epoch 140, loss: 2.093244\n",
      "Epoch 141, loss: 2.093204\n",
      "Epoch 142, loss: 2.093200\n",
      "Epoch 143, loss: 2.093183\n",
      "Epoch 144, loss: 2.093135\n",
      "Epoch 145, loss: 2.093109\n",
      "Epoch 146, loss: 2.093070\n",
      "Epoch 147, loss: 2.093043\n",
      "Epoch 148, loss: 2.093017\n",
      "Epoch 149, loss: 2.092978\n",
      "Epoch 150, loss: 2.092971\n",
      "Epoch 151, loss: 2.092952\n",
      "Epoch 152, loss: 2.092921\n",
      "Epoch 153, loss: 2.092890\n",
      "Epoch 154, loss: 2.092876\n",
      "Epoch 155, loss: 2.092846\n",
      "Epoch 156, loss: 2.092804\n",
      "Epoch 157, loss: 2.092778\n",
      "Epoch 158, loss: 2.092760\n",
      "Epoch 159, loss: 2.092733\n",
      "Epoch 160, loss: 2.092728\n",
      "Epoch 161, loss: 2.092689\n",
      "Epoch 162, loss: 2.092665\n",
      "Epoch 163, loss: 2.092649\n",
      "Epoch 164, loss: 2.092620\n",
      "Epoch 165, loss: 2.092594\n",
      "Epoch 166, loss: 2.092564\n",
      "Epoch 167, loss: 2.092528\n",
      "Epoch 168, loss: 2.092513\n",
      "Epoch 169, loss: 2.092487\n",
      "Epoch 170, loss: 2.092460\n",
      "Epoch 171, loss: 2.092423\n",
      "Epoch 172, loss: 2.092414\n",
      "Epoch 173, loss: 2.092384\n",
      "Epoch 174, loss: 2.092349\n",
      "Epoch 175, loss: 2.092350\n",
      "Epoch 176, loss: 2.092313\n",
      "Epoch 177, loss: 2.092278\n",
      "Epoch 178, loss: 2.092288\n",
      "Epoch 179, loss: 2.092231\n",
      "Epoch 180, loss: 2.092192\n",
      "Epoch 181, loss: 2.092188\n",
      "Epoch 182, loss: 2.092144\n",
      "Epoch 183, loss: 2.092116\n",
      "Epoch 184, loss: 2.092116\n",
      "Epoch 185, loss: 2.092071\n",
      "Epoch 186, loss: 2.092050\n",
      "Epoch 187, loss: 2.092033\n",
      "Epoch 188, loss: 2.092009\n",
      "Epoch 189, loss: 2.091993\n",
      "Epoch 190, loss: 2.091943\n",
      "Epoch 191, loss: 2.091922\n",
      "Epoch 192, loss: 2.091902\n",
      "Epoch 193, loss: 2.091890\n",
      "Epoch 194, loss: 2.091862\n",
      "Epoch 195, loss: 2.091817\n",
      "Epoch 196, loss: 2.091796\n",
      "Epoch 197, loss: 2.091775\n",
      "Epoch 198, loss: 2.091746\n",
      "Epoch 199, loss: 2.091724\n",
      "Epoch 0, loss: 2.090304\n",
      "Epoch 1, loss: 2.090162\n",
      "Epoch 2, loss: 2.090054\n",
      "Epoch 3, loss: 2.089948\n",
      "Epoch 4, loss: 2.089869\n",
      "Epoch 5, loss: 2.089789\n",
      "Epoch 6, loss: 2.089717\n",
      "Epoch 7, loss: 2.089648\n",
      "Epoch 8, loss: 2.089581\n",
      "Epoch 9, loss: 2.089503\n",
      "Epoch 10, loss: 2.089448\n",
      "Epoch 11, loss: 2.089391\n",
      "Epoch 12, loss: 2.089327\n",
      "Epoch 13, loss: 2.089275\n",
      "Epoch 14, loss: 2.089240\n",
      "Epoch 15, loss: 2.089172\n",
      "Epoch 16, loss: 2.089126\n",
      "Epoch 17, loss: 2.089083\n",
      "Epoch 18, loss: 2.089041\n",
      "Epoch 19, loss: 2.089001\n",
      "Epoch 20, loss: 2.088940\n",
      "Epoch 21, loss: 2.088928\n",
      "Epoch 22, loss: 2.088857\n",
      "Epoch 23, loss: 2.088804\n",
      "Epoch 24, loss: 2.088756\n",
      "Epoch 25, loss: 2.088718\n",
      "Epoch 26, loss: 2.088652\n",
      "Epoch 27, loss: 2.088626\n",
      "Epoch 28, loss: 2.088584\n",
      "Epoch 29, loss: 2.088533\n",
      "Epoch 30, loss: 2.088523\n",
      "Epoch 31, loss: 2.088469\n",
      "Epoch 32, loss: 2.088421\n",
      "Epoch 33, loss: 2.088371\n",
      "Epoch 34, loss: 2.088340\n",
      "Epoch 35, loss: 2.088297\n",
      "Epoch 36, loss: 2.088264\n",
      "Epoch 37, loss: 2.088211\n",
      "Epoch 38, loss: 2.088159\n",
      "Epoch 39, loss: 2.088143\n",
      "Epoch 40, loss: 2.088093\n",
      "Epoch 41, loss: 2.088045\n",
      "Epoch 42, loss: 2.088010\n",
      "Epoch 43, loss: 2.087981\n",
      "Epoch 44, loss: 2.087930\n",
      "Epoch 45, loss: 2.087888\n",
      "Epoch 46, loss: 2.087854\n",
      "Epoch 47, loss: 2.087825\n",
      "Epoch 48, loss: 2.087772\n",
      "Epoch 49, loss: 2.087751\n",
      "Epoch 50, loss: 2.087714\n",
      "Epoch 51, loss: 2.087682\n",
      "Epoch 52, loss: 2.087630\n",
      "Epoch 53, loss: 2.087596\n",
      "Epoch 54, loss: 2.087560\n",
      "Epoch 55, loss: 2.087530\n",
      "Epoch 56, loss: 2.087492\n",
      "Epoch 57, loss: 2.087459\n",
      "Epoch 58, loss: 2.087409\n",
      "Epoch 59, loss: 2.087382\n",
      "Epoch 60, loss: 2.087353\n",
      "Epoch 61, loss: 2.087309\n",
      "Epoch 62, loss: 2.087292\n",
      "Epoch 63, loss: 2.087240\n",
      "Epoch 64, loss: 2.087206\n",
      "Epoch 65, loss: 2.087179\n",
      "Epoch 66, loss: 2.087144\n",
      "Epoch 67, loss: 2.087114\n",
      "Epoch 68, loss: 2.087066\n",
      "Epoch 69, loss: 2.087039\n",
      "Epoch 70, loss: 2.086996\n",
      "Epoch 71, loss: 2.086956\n",
      "Epoch 72, loss: 2.086932\n",
      "Epoch 73, loss: 2.086887\n",
      "Epoch 74, loss: 2.086860\n",
      "Epoch 75, loss: 2.086834\n",
      "Epoch 76, loss: 2.086802\n",
      "Epoch 77, loss: 2.086773\n",
      "Epoch 78, loss: 2.086730\n",
      "Epoch 79, loss: 2.086704\n",
      "Epoch 80, loss: 2.086670\n",
      "Epoch 81, loss: 2.086636\n",
      "Epoch 82, loss: 2.086615\n",
      "Epoch 83, loss: 2.086584\n",
      "Epoch 84, loss: 2.086555\n",
      "Epoch 85, loss: 2.086527\n",
      "Epoch 86, loss: 2.086472\n",
      "Epoch 87, loss: 2.086438\n",
      "Epoch 88, loss: 2.086405\n",
      "Epoch 89, loss: 2.086401\n",
      "Epoch 90, loss: 2.086354\n",
      "Epoch 91, loss: 2.086309\n",
      "Epoch 92, loss: 2.086293\n",
      "Epoch 93, loss: 2.086265\n",
      "Epoch 94, loss: 2.086225\n",
      "Epoch 95, loss: 2.086211\n",
      "Epoch 96, loss: 2.086151\n",
      "Epoch 97, loss: 2.086136\n",
      "Epoch 98, loss: 2.086114\n",
      "Epoch 99, loss: 2.086075\n",
      "Epoch 100, loss: 2.086044\n",
      "Epoch 101, loss: 2.086009\n",
      "Epoch 102, loss: 2.085994\n",
      "Epoch 103, loss: 2.085963\n",
      "Epoch 104, loss: 2.085909\n",
      "Epoch 105, loss: 2.085908\n",
      "Epoch 106, loss: 2.085866\n",
      "Epoch 107, loss: 2.085844\n",
      "Epoch 108, loss: 2.085814\n",
      "Epoch 109, loss: 2.085789\n",
      "Epoch 110, loss: 2.085746\n",
      "Epoch 111, loss: 2.085722\n",
      "Epoch 112, loss: 2.085680\n",
      "Epoch 113, loss: 2.085653\n",
      "Epoch 114, loss: 2.085634\n",
      "Epoch 115, loss: 2.085607\n",
      "Epoch 116, loss: 2.085602\n",
      "Epoch 117, loss: 2.085554\n",
      "Epoch 118, loss: 2.085513\n",
      "Epoch 119, loss: 2.085501\n",
      "Epoch 120, loss: 2.085471\n",
      "Epoch 121, loss: 2.085447\n",
      "Epoch 122, loss: 2.085427\n",
      "Epoch 123, loss: 2.085391\n",
      "Epoch 124, loss: 2.085373\n",
      "Epoch 125, loss: 2.085312\n",
      "Epoch 126, loss: 2.085306\n",
      "Epoch 127, loss: 2.085264\n",
      "Epoch 128, loss: 2.085225\n",
      "Epoch 129, loss: 2.085203\n",
      "Epoch 130, loss: 2.085173\n",
      "Epoch 131, loss: 2.085144\n",
      "Epoch 132, loss: 2.085134\n",
      "Epoch 133, loss: 2.085092\n",
      "Epoch 134, loss: 2.085076\n",
      "Epoch 135, loss: 2.085043\n",
      "Epoch 136, loss: 2.085018\n",
      "Epoch 137, loss: 2.084991\n",
      "Epoch 138, loss: 2.084980\n",
      "Epoch 139, loss: 2.084934\n",
      "Epoch 140, loss: 2.084917\n",
      "Epoch 141, loss: 2.084879\n",
      "Epoch 142, loss: 2.084857\n",
      "Epoch 143, loss: 2.084830\n",
      "Epoch 144, loss: 2.084806\n",
      "Epoch 145, loss: 2.084782\n",
      "Epoch 146, loss: 2.084749\n",
      "Epoch 147, loss: 2.084705\n",
      "Epoch 148, loss: 2.084695\n",
      "Epoch 149, loss: 2.084666\n",
      "Epoch 150, loss: 2.084649\n",
      "Epoch 151, loss: 2.084606\n",
      "Epoch 152, loss: 2.084610\n",
      "Epoch 153, loss: 2.084574\n",
      "Epoch 154, loss: 2.084541\n",
      "Epoch 155, loss: 2.084518\n",
      "Epoch 156, loss: 2.084479\n",
      "Epoch 157, loss: 2.084475\n",
      "Epoch 158, loss: 2.084426\n",
      "Epoch 159, loss: 2.084402\n",
      "Epoch 160, loss: 2.084377\n",
      "Epoch 161, loss: 2.084388\n",
      "Epoch 162, loss: 2.084328\n",
      "Epoch 163, loss: 2.084292\n",
      "Epoch 164, loss: 2.084274\n",
      "Epoch 165, loss: 2.084255\n",
      "Epoch 166, loss: 2.084218\n",
      "Epoch 167, loss: 2.084214\n",
      "Epoch 168, loss: 2.084180\n",
      "Epoch 169, loss: 2.084150\n",
      "Epoch 170, loss: 2.084119\n",
      "Epoch 171, loss: 2.084090\n",
      "Epoch 172, loss: 2.084078\n",
      "Epoch 173, loss: 2.084049\n",
      "Epoch 174, loss: 2.084032\n",
      "Epoch 175, loss: 2.083996\n",
      "Epoch 176, loss: 2.083980\n",
      "Epoch 177, loss: 2.083940\n",
      "Epoch 178, loss: 2.083944\n",
      "Epoch 179, loss: 2.083906\n",
      "Epoch 180, loss: 2.083886\n",
      "Epoch 181, loss: 2.083838\n",
      "Epoch 182, loss: 2.083822\n",
      "Epoch 183, loss: 2.083809\n",
      "Epoch 184, loss: 2.083775\n",
      "Epoch 185, loss: 2.083755\n",
      "Epoch 186, loss: 2.083720\n",
      "Epoch 187, loss: 2.083701\n",
      "Epoch 188, loss: 2.083678\n",
      "Epoch 189, loss: 2.083652\n",
      "Epoch 190, loss: 2.083633\n",
      "Epoch 191, loss: 2.083604\n",
      "Epoch 192, loss: 2.083573\n",
      "Epoch 193, loss: 2.083569\n",
      "Epoch 194, loss: 2.083529\n",
      "Epoch 195, loss: 2.083515\n",
      "Epoch 196, loss: 2.083485\n",
      "Epoch 197, loss: 2.083448\n",
      "Epoch 198, loss: 2.083428\n",
      "Epoch 199, loss: 2.083410\n",
      "Epoch 0, loss: 2.089686\n",
      "Epoch 1, loss: 2.089553\n",
      "Epoch 2, loss: 2.089494\n",
      "Epoch 3, loss: 2.089390\n",
      "Epoch 4, loss: 2.089301\n",
      "Epoch 5, loss: 2.089246\n",
      "Epoch 6, loss: 2.089176\n",
      "Epoch 7, loss: 2.089137\n",
      "Epoch 8, loss: 2.089080\n",
      "Epoch 9, loss: 2.089053\n",
      "Epoch 10, loss: 2.088955\n",
      "Epoch 11, loss: 2.088928\n",
      "Epoch 12, loss: 2.088879\n",
      "Epoch 13, loss: 2.088822\n",
      "Epoch 14, loss: 2.088775\n",
      "Epoch 15, loss: 2.088730\n",
      "Epoch 16, loss: 2.088676\n",
      "Epoch 17, loss: 2.088635\n",
      "Epoch 18, loss: 2.088617\n",
      "Epoch 19, loss: 2.088538\n",
      "Epoch 20, loss: 2.088502\n",
      "Epoch 21, loss: 2.088458\n",
      "Epoch 22, loss: 2.088415\n",
      "Epoch 23, loss: 2.088355\n",
      "Epoch 24, loss: 2.088336\n",
      "Epoch 25, loss: 2.088290\n",
      "Epoch 26, loss: 2.088225\n",
      "Epoch 27, loss: 2.088200\n",
      "Epoch 28, loss: 2.088162\n",
      "Epoch 29, loss: 2.088115\n",
      "Epoch 30, loss: 2.088072\n",
      "Epoch 31, loss: 2.088039\n",
      "Epoch 32, loss: 2.088002\n",
      "Epoch 33, loss: 2.087959\n",
      "Epoch 34, loss: 2.087921\n",
      "Epoch 35, loss: 2.087879\n",
      "Epoch 36, loss: 2.087843\n",
      "Epoch 37, loss: 2.087820\n",
      "Epoch 38, loss: 2.087757\n",
      "Epoch 39, loss: 2.087706\n",
      "Epoch 40, loss: 2.087709\n",
      "Epoch 41, loss: 2.087639\n",
      "Epoch 42, loss: 2.087596\n",
      "Epoch 43, loss: 2.087567\n",
      "Epoch 44, loss: 2.087526\n",
      "Epoch 45, loss: 2.087504\n",
      "Epoch 46, loss: 2.087464\n",
      "Epoch 47, loss: 2.087428\n",
      "Epoch 48, loss: 2.087379\n",
      "Epoch 49, loss: 2.087356\n",
      "Epoch 50, loss: 2.087317\n",
      "Epoch 51, loss: 2.087266\n",
      "Epoch 52, loss: 2.087250\n",
      "Epoch 53, loss: 2.087196\n",
      "Epoch 54, loss: 2.087153\n",
      "Epoch 55, loss: 2.087125\n",
      "Epoch 56, loss: 2.087110\n",
      "Epoch 57, loss: 2.087052\n",
      "Epoch 58, loss: 2.087035\n",
      "Epoch 59, loss: 2.087005\n",
      "Epoch 60, loss: 2.086958\n",
      "Epoch 61, loss: 2.086910\n",
      "Epoch 62, loss: 2.086877\n",
      "Epoch 63, loss: 2.086875\n",
      "Epoch 64, loss: 2.086830\n",
      "Epoch 65, loss: 2.086813\n",
      "Epoch 66, loss: 2.086766\n",
      "Epoch 67, loss: 2.086728\n",
      "Epoch 68, loss: 2.086695\n",
      "Epoch 69, loss: 2.086655\n",
      "Epoch 70, loss: 2.086645\n",
      "Epoch 71, loss: 2.086589\n",
      "Epoch 72, loss: 2.086570\n",
      "Epoch 73, loss: 2.086551\n",
      "Epoch 74, loss: 2.086506\n",
      "Epoch 75, loss: 2.086473\n",
      "Epoch 76, loss: 2.086441\n",
      "Epoch 77, loss: 2.086400\n",
      "Epoch 78, loss: 2.086391\n",
      "Epoch 79, loss: 2.086344\n",
      "Epoch 80, loss: 2.086310\n",
      "Epoch 81, loss: 2.086268\n",
      "Epoch 82, loss: 2.086260\n",
      "Epoch 83, loss: 2.086224\n",
      "Epoch 84, loss: 2.086182\n",
      "Epoch 85, loss: 2.086158\n",
      "Epoch 86, loss: 2.086137\n",
      "Epoch 87, loss: 2.086101\n",
      "Epoch 88, loss: 2.086055\n",
      "Epoch 89, loss: 2.086043\n",
      "Epoch 90, loss: 2.086010\n",
      "Epoch 91, loss: 2.085975\n",
      "Epoch 92, loss: 2.085951\n",
      "Epoch 93, loss: 2.085938\n",
      "Epoch 94, loss: 2.085886\n",
      "Epoch 95, loss: 2.085875\n",
      "Epoch 96, loss: 2.085848\n",
      "Epoch 97, loss: 2.085803\n",
      "Epoch 98, loss: 2.085769\n",
      "Epoch 99, loss: 2.085769\n",
      "Epoch 100, loss: 2.085718\n",
      "Epoch 101, loss: 2.085699\n",
      "Epoch 102, loss: 2.085652\n",
      "Epoch 103, loss: 2.085637\n",
      "Epoch 104, loss: 2.085616\n",
      "Epoch 105, loss: 2.085571\n",
      "Epoch 106, loss: 2.085550\n",
      "Epoch 107, loss: 2.085520\n",
      "Epoch 108, loss: 2.085502\n",
      "Epoch 109, loss: 2.085455\n",
      "Epoch 110, loss: 2.085444\n",
      "Epoch 111, loss: 2.085421\n",
      "Epoch 112, loss: 2.085391\n",
      "Epoch 113, loss: 2.085345\n",
      "Epoch 114, loss: 2.085336\n",
      "Epoch 115, loss: 2.085292\n",
      "Epoch 116, loss: 2.085271\n",
      "Epoch 117, loss: 2.085248\n",
      "Epoch 118, loss: 2.085224\n",
      "Epoch 119, loss: 2.085192\n",
      "Epoch 120, loss: 2.085170\n",
      "Epoch 121, loss: 2.085121\n",
      "Epoch 122, loss: 2.085112\n",
      "Epoch 123, loss: 2.085074\n",
      "Epoch 124, loss: 2.085078\n",
      "Epoch 125, loss: 2.085024\n",
      "Epoch 126, loss: 2.085012\n",
      "Epoch 127, loss: 2.084976\n",
      "Epoch 128, loss: 2.084935\n",
      "Epoch 129, loss: 2.084931\n",
      "Epoch 130, loss: 2.084896\n",
      "Epoch 131, loss: 2.084871\n",
      "Epoch 132, loss: 2.084850\n",
      "Epoch 133, loss: 2.084825\n",
      "Epoch 134, loss: 2.084788\n",
      "Epoch 135, loss: 2.084749\n",
      "Epoch 136, loss: 2.084740\n",
      "Epoch 137, loss: 2.084738\n",
      "Epoch 138, loss: 2.084685\n",
      "Epoch 139, loss: 2.084658\n",
      "Epoch 140, loss: 2.084641\n",
      "Epoch 141, loss: 2.084623\n",
      "Epoch 142, loss: 2.084583\n",
      "Epoch 143, loss: 2.084564\n",
      "Epoch 144, loss: 2.084533\n",
      "Epoch 145, loss: 2.084493\n",
      "Epoch 146, loss: 2.084471\n",
      "Epoch 147, loss: 2.084460\n",
      "Epoch 148, loss: 2.084412\n",
      "Epoch 149, loss: 2.084391\n",
      "Epoch 150, loss: 2.084372\n",
      "Epoch 151, loss: 2.084366\n",
      "Epoch 152, loss: 2.084347\n",
      "Epoch 153, loss: 2.084294\n",
      "Epoch 154, loss: 2.084270\n",
      "Epoch 155, loss: 2.084263\n",
      "Epoch 156, loss: 2.084217\n",
      "Epoch 157, loss: 2.084207\n",
      "Epoch 158, loss: 2.084191\n",
      "Epoch 159, loss: 2.084149\n",
      "Epoch 160, loss: 2.084146\n",
      "Epoch 161, loss: 2.084093\n",
      "Epoch 162, loss: 2.084069\n",
      "Epoch 163, loss: 2.084073\n",
      "Epoch 164, loss: 2.084023\n",
      "Epoch 165, loss: 2.084021\n",
      "Epoch 166, loss: 2.083978\n",
      "Epoch 167, loss: 2.083976\n",
      "Epoch 168, loss: 2.083920\n",
      "Epoch 169, loss: 2.083895\n",
      "Epoch 170, loss: 2.083873\n",
      "Epoch 171, loss: 2.083864\n",
      "Epoch 172, loss: 2.083831\n",
      "Epoch 173, loss: 2.083793\n",
      "Epoch 174, loss: 2.083779\n",
      "Epoch 175, loss: 2.083763\n",
      "Epoch 176, loss: 2.083740\n",
      "Epoch 177, loss: 2.083717\n",
      "Epoch 178, loss: 2.083674\n",
      "Epoch 179, loss: 2.083661\n",
      "Epoch 180, loss: 2.083643\n",
      "Epoch 181, loss: 2.083621\n",
      "Epoch 182, loss: 2.083592\n",
      "Epoch 183, loss: 2.083570\n",
      "Epoch 184, loss: 2.083543\n",
      "Epoch 185, loss: 2.083524\n",
      "Epoch 186, loss: 2.083521\n",
      "Epoch 187, loss: 2.083479\n",
      "Epoch 188, loss: 2.083442\n",
      "Epoch 189, loss: 2.083420\n",
      "Epoch 190, loss: 2.083407\n",
      "Epoch 191, loss: 2.083386\n",
      "Epoch 192, loss: 2.083360\n",
      "Epoch 193, loss: 2.083350\n",
      "Epoch 194, loss: 2.083337\n",
      "Epoch 195, loss: 2.083301\n",
      "Epoch 196, loss: 2.083276\n",
      "Epoch 197, loss: 2.083236\n",
      "Epoch 198, loss: 2.083211\n",
      "Epoch 199, loss: 2.083203\n",
      "Epoch 0, loss: 2.085976\n",
      "Epoch 1, loss: 2.085946\n",
      "Epoch 2, loss: 2.085896\n",
      "Epoch 3, loss: 2.085844\n",
      "Epoch 4, loss: 2.085800\n",
      "Epoch 5, loss: 2.085793\n",
      "Epoch 6, loss: 2.085764\n",
      "Epoch 7, loss: 2.085721\n",
      "Epoch 8, loss: 2.085714\n",
      "Epoch 9, loss: 2.085688\n",
      "Epoch 10, loss: 2.085662\n",
      "Epoch 11, loss: 2.085672\n",
      "Epoch 12, loss: 2.085625\n",
      "Epoch 13, loss: 2.085599\n",
      "Epoch 14, loss: 2.085585\n",
      "Epoch 15, loss: 2.085575\n",
      "Epoch 16, loss: 2.085531\n",
      "Epoch 17, loss: 2.085528\n",
      "Epoch 18, loss: 2.085515\n",
      "Epoch 19, loss: 2.085476\n",
      "Epoch 20, loss: 2.085459\n",
      "Epoch 21, loss: 2.085448\n",
      "Epoch 22, loss: 2.085405\n",
      "Epoch 23, loss: 2.085421\n",
      "Epoch 24, loss: 2.085389\n",
      "Epoch 25, loss: 2.085361\n",
      "Epoch 26, loss: 2.085356\n",
      "Epoch 27, loss: 2.085329\n",
      "Epoch 28, loss: 2.085322\n",
      "Epoch 29, loss: 2.085293\n",
      "Epoch 30, loss: 2.085294\n",
      "Epoch 31, loss: 2.085271\n",
      "Epoch 32, loss: 2.085227\n",
      "Epoch 33, loss: 2.085219\n",
      "Epoch 34, loss: 2.085195\n",
      "Epoch 35, loss: 2.085182\n",
      "Epoch 36, loss: 2.085169\n",
      "Epoch 37, loss: 2.085158\n",
      "Epoch 38, loss: 2.085111\n",
      "Epoch 39, loss: 2.085120\n",
      "Epoch 40, loss: 2.085111\n",
      "Epoch 41, loss: 2.085074\n",
      "Epoch 42, loss: 2.085072\n",
      "Epoch 43, loss: 2.085056\n",
      "Epoch 44, loss: 2.085035\n",
      "Epoch 45, loss: 2.085016\n",
      "Epoch 46, loss: 2.085007\n",
      "Epoch 47, loss: 2.084983\n",
      "Epoch 48, loss: 2.084973\n",
      "Epoch 49, loss: 2.084941\n",
      "Epoch 50, loss: 2.084924\n",
      "Epoch 51, loss: 2.084909\n",
      "Epoch 52, loss: 2.084902\n",
      "Epoch 53, loss: 2.084869\n",
      "Epoch 54, loss: 2.084870\n",
      "Epoch 55, loss: 2.084858\n",
      "Epoch 56, loss: 2.084814\n",
      "Epoch 57, loss: 2.084824\n",
      "Epoch 58, loss: 2.084793\n",
      "Epoch 59, loss: 2.084783\n",
      "Epoch 60, loss: 2.084778\n",
      "Epoch 61, loss: 2.084760\n",
      "Epoch 62, loss: 2.084748\n",
      "Epoch 63, loss: 2.084712\n",
      "Epoch 64, loss: 2.084713\n",
      "Epoch 65, loss: 2.084696\n",
      "Epoch 66, loss: 2.084668\n",
      "Epoch 67, loss: 2.084647\n",
      "Epoch 68, loss: 2.084630\n",
      "Epoch 69, loss: 2.084620\n",
      "Epoch 70, loss: 2.084596\n",
      "Epoch 71, loss: 2.084583\n",
      "Epoch 72, loss: 2.084579\n",
      "Epoch 73, loss: 2.084562\n",
      "Epoch 74, loss: 2.084550\n",
      "Epoch 75, loss: 2.084531\n",
      "Epoch 76, loss: 2.084521\n",
      "Epoch 77, loss: 2.084484\n",
      "Epoch 78, loss: 2.084473\n",
      "Epoch 79, loss: 2.084458\n",
      "Epoch 80, loss: 2.084448\n",
      "Epoch 81, loss: 2.084440\n",
      "Epoch 82, loss: 2.084430\n",
      "Epoch 83, loss: 2.084406\n",
      "Epoch 84, loss: 2.084397\n",
      "Epoch 85, loss: 2.084363\n",
      "Epoch 86, loss: 2.084351\n",
      "Epoch 87, loss: 2.084345\n",
      "Epoch 88, loss: 2.084359\n",
      "Epoch 89, loss: 2.084316\n",
      "Epoch 90, loss: 2.084300\n",
      "Epoch 91, loss: 2.084293\n",
      "Epoch 92, loss: 2.084280\n",
      "Epoch 93, loss: 2.084264\n",
      "Epoch 94, loss: 2.084250\n",
      "Epoch 95, loss: 2.084221\n",
      "Epoch 96, loss: 2.084213\n",
      "Epoch 97, loss: 2.084197\n",
      "Epoch 98, loss: 2.084186\n",
      "Epoch 99, loss: 2.084176\n",
      "Epoch 100, loss: 2.084164\n",
      "Epoch 101, loss: 2.084132\n",
      "Epoch 102, loss: 2.084132\n",
      "Epoch 103, loss: 2.084100\n",
      "Epoch 104, loss: 2.084080\n",
      "Epoch 105, loss: 2.084082\n",
      "Epoch 106, loss: 2.084055\n",
      "Epoch 107, loss: 2.084055\n",
      "Epoch 108, loss: 2.084042\n",
      "Epoch 109, loss: 2.084025\n",
      "Epoch 110, loss: 2.084004\n",
      "Epoch 111, loss: 2.083995\n",
      "Epoch 112, loss: 2.083976\n",
      "Epoch 113, loss: 2.083960\n",
      "Epoch 114, loss: 2.083943\n",
      "Epoch 115, loss: 2.083935\n",
      "Epoch 116, loss: 2.083920\n",
      "Epoch 117, loss: 2.083915\n",
      "Epoch 118, loss: 2.083898\n",
      "Epoch 119, loss: 2.083869\n",
      "Epoch 120, loss: 2.083866\n",
      "Epoch 121, loss: 2.083841\n",
      "Epoch 122, loss: 2.083843\n",
      "Epoch 123, loss: 2.083808\n",
      "Epoch 124, loss: 2.083823\n",
      "Epoch 125, loss: 2.083801\n",
      "Epoch 126, loss: 2.083776\n",
      "Epoch 127, loss: 2.083761\n",
      "Epoch 128, loss: 2.083752\n",
      "Epoch 129, loss: 2.083731\n",
      "Epoch 130, loss: 2.083710\n",
      "Epoch 131, loss: 2.083711\n",
      "Epoch 132, loss: 2.083685\n",
      "Epoch 133, loss: 2.083673\n",
      "Epoch 134, loss: 2.083657\n",
      "Epoch 135, loss: 2.083658\n",
      "Epoch 136, loss: 2.083631\n",
      "Epoch 137, loss: 2.083641\n",
      "Epoch 138, loss: 2.083612\n",
      "Epoch 139, loss: 2.083583\n",
      "Epoch 140, loss: 2.083594\n",
      "Epoch 141, loss: 2.083572\n",
      "Epoch 142, loss: 2.083549\n",
      "Epoch 143, loss: 2.083522\n",
      "Epoch 144, loss: 2.083537\n",
      "Epoch 145, loss: 2.083505\n",
      "Epoch 146, loss: 2.083485\n",
      "Epoch 147, loss: 2.083488\n",
      "Epoch 148, loss: 2.083460\n",
      "Epoch 149, loss: 2.083437\n",
      "Epoch 150, loss: 2.083434\n",
      "Epoch 151, loss: 2.083419\n",
      "Epoch 152, loss: 2.083403\n",
      "Epoch 153, loss: 2.083380\n",
      "Epoch 154, loss: 2.083364\n",
      "Epoch 155, loss: 2.083367\n",
      "Epoch 156, loss: 2.083343\n",
      "Epoch 157, loss: 2.083349\n",
      "Epoch 158, loss: 2.083330\n",
      "Epoch 159, loss: 2.083303\n",
      "Epoch 160, loss: 2.083281\n",
      "Epoch 161, loss: 2.083267\n",
      "Epoch 162, loss: 2.083265\n",
      "Epoch 163, loss: 2.083254\n",
      "Epoch 164, loss: 2.083222\n",
      "Epoch 165, loss: 2.083213\n",
      "Epoch 166, loss: 2.083200\n",
      "Epoch 167, loss: 2.083193\n",
      "Epoch 168, loss: 2.083173\n",
      "Epoch 169, loss: 2.083179\n",
      "Epoch 170, loss: 2.083161\n",
      "Epoch 171, loss: 2.083146\n",
      "Epoch 172, loss: 2.083120\n",
      "Epoch 173, loss: 2.083119\n",
      "Epoch 174, loss: 2.083132\n",
      "Epoch 175, loss: 2.083103\n",
      "Epoch 176, loss: 2.083071\n",
      "Epoch 177, loss: 2.083052\n",
      "Epoch 178, loss: 2.083038\n",
      "Epoch 179, loss: 2.083035\n",
      "Epoch 180, loss: 2.083026\n",
      "Epoch 181, loss: 2.083003\n",
      "Epoch 182, loss: 2.082993\n",
      "Epoch 183, loss: 2.082978\n",
      "Epoch 184, loss: 2.082965\n",
      "Epoch 185, loss: 2.082952\n",
      "Epoch 186, loss: 2.082939\n",
      "Epoch 187, loss: 2.082908\n",
      "Epoch 188, loss: 2.082914\n",
      "Epoch 189, loss: 2.082872\n",
      "Epoch 190, loss: 2.082879\n",
      "Epoch 191, loss: 2.082854\n",
      "Epoch 192, loss: 2.082863\n",
      "Epoch 193, loss: 2.082834\n",
      "Epoch 194, loss: 2.082825\n",
      "Epoch 195, loss: 2.082803\n",
      "Epoch 196, loss: 2.082788\n",
      "Epoch 197, loss: 2.082780\n",
      "Epoch 198, loss: 2.082775\n",
      "Epoch 199, loss: 2.082735\n",
      "Epoch 0, loss: 2.085704\n",
      "Epoch 1, loss: 2.085697\n",
      "Epoch 2, loss: 2.085692\n",
      "Epoch 3, loss: 2.085683\n",
      "Epoch 4, loss: 2.085677\n",
      "Epoch 5, loss: 2.085670\n",
      "Epoch 6, loss: 2.085663\n",
      "Epoch 7, loss: 2.085655\n",
      "Epoch 8, loss: 2.085649\n",
      "Epoch 9, loss: 2.085645\n",
      "Epoch 10, loss: 2.085640\n",
      "Epoch 11, loss: 2.085630\n",
      "Epoch 12, loss: 2.085624\n",
      "Epoch 13, loss: 2.085620\n",
      "Epoch 14, loss: 2.085613\n",
      "Epoch 15, loss: 2.085609\n",
      "Epoch 16, loss: 2.085602\n",
      "Epoch 17, loss: 2.085598\n",
      "Epoch 18, loss: 2.085590\n",
      "Epoch 19, loss: 2.085585\n",
      "Epoch 20, loss: 2.085581\n",
      "Epoch 21, loss: 2.085574\n",
      "Epoch 22, loss: 2.085570\n",
      "Epoch 23, loss: 2.085564\n",
      "Epoch 24, loss: 2.085559\n",
      "Epoch 25, loss: 2.085554\n",
      "Epoch 26, loss: 2.085550\n",
      "Epoch 27, loss: 2.085544\n",
      "Epoch 28, loss: 2.085538\n",
      "Epoch 29, loss: 2.085535\n",
      "Epoch 30, loss: 2.085529\n",
      "Epoch 31, loss: 2.085524\n",
      "Epoch 32, loss: 2.085521\n",
      "Epoch 33, loss: 2.085515\n",
      "Epoch 34, loss: 2.085510\n",
      "Epoch 35, loss: 2.085507\n",
      "Epoch 36, loss: 2.085501\n",
      "Epoch 37, loss: 2.085497\n",
      "Epoch 38, loss: 2.085492\n",
      "Epoch 39, loss: 2.085487\n",
      "Epoch 40, loss: 2.085483\n",
      "Epoch 41, loss: 2.085478\n",
      "Epoch 42, loss: 2.085476\n",
      "Epoch 43, loss: 2.085470\n",
      "Epoch 44, loss: 2.085464\n",
      "Epoch 45, loss: 2.085460\n",
      "Epoch 46, loss: 2.085457\n",
      "Epoch 47, loss: 2.085451\n",
      "Epoch 48, loss: 2.085448\n",
      "Epoch 49, loss: 2.085444\n",
      "Epoch 50, loss: 2.085440\n",
      "Epoch 51, loss: 2.085434\n",
      "Epoch 52, loss: 2.085431\n",
      "Epoch 53, loss: 2.085427\n",
      "Epoch 54, loss: 2.085422\n",
      "Epoch 55, loss: 2.085421\n",
      "Epoch 56, loss: 2.085416\n",
      "Epoch 57, loss: 2.085410\n",
      "Epoch 58, loss: 2.085405\n",
      "Epoch 59, loss: 2.085403\n",
      "Epoch 60, loss: 2.085400\n",
      "Epoch 61, loss: 2.085394\n",
      "Epoch 62, loss: 2.085389\n",
      "Epoch 63, loss: 2.085386\n",
      "Epoch 64, loss: 2.085380\n",
      "Epoch 65, loss: 2.085380\n",
      "Epoch 66, loss: 2.085373\n",
      "Epoch 67, loss: 2.085370\n",
      "Epoch 68, loss: 2.085365\n",
      "Epoch 69, loss: 2.085362\n",
      "Epoch 70, loss: 2.085359\n",
      "Epoch 71, loss: 2.085355\n",
      "Epoch 72, loss: 2.085350\n",
      "Epoch 73, loss: 2.085346\n",
      "Epoch 74, loss: 2.085340\n",
      "Epoch 75, loss: 2.085338\n",
      "Epoch 76, loss: 2.085334\n",
      "Epoch 77, loss: 2.085331\n",
      "Epoch 78, loss: 2.085328\n",
      "Epoch 79, loss: 2.085324\n",
      "Epoch 80, loss: 2.085319\n",
      "Epoch 81, loss: 2.085316\n",
      "Epoch 82, loss: 2.085312\n",
      "Epoch 83, loss: 2.085306\n",
      "Epoch 84, loss: 2.085304\n",
      "Epoch 85, loss: 2.085299\n",
      "Epoch 86, loss: 2.085295\n",
      "Epoch 87, loss: 2.085292\n",
      "Epoch 88, loss: 2.085289\n",
      "Epoch 89, loss: 2.085284\n",
      "Epoch 90, loss: 2.085281\n",
      "Epoch 91, loss: 2.085277\n",
      "Epoch 92, loss: 2.085271\n",
      "Epoch 93, loss: 2.085271\n",
      "Epoch 94, loss: 2.085266\n",
      "Epoch 95, loss: 2.085263\n",
      "Epoch 96, loss: 2.085259\n",
      "Epoch 97, loss: 2.085256\n",
      "Epoch 98, loss: 2.085250\n",
      "Epoch 99, loss: 2.085247\n",
      "Epoch 100, loss: 2.085245\n",
      "Epoch 101, loss: 2.085240\n",
      "Epoch 102, loss: 2.085237\n",
      "Epoch 103, loss: 2.085233\n",
      "Epoch 104, loss: 2.085229\n",
      "Epoch 105, loss: 2.085225\n",
      "Epoch 106, loss: 2.085222\n",
      "Epoch 107, loss: 2.085217\n",
      "Epoch 108, loss: 2.085216\n",
      "Epoch 109, loss: 2.085211\n",
      "Epoch 110, loss: 2.085206\n",
      "Epoch 111, loss: 2.085201\n",
      "Epoch 112, loss: 2.085200\n",
      "Epoch 113, loss: 2.085195\n",
      "Epoch 114, loss: 2.085194\n",
      "Epoch 115, loss: 2.085189\n",
      "Epoch 116, loss: 2.085184\n",
      "Epoch 117, loss: 2.085179\n",
      "Epoch 118, loss: 2.085178\n",
      "Epoch 119, loss: 2.085173\n",
      "Epoch 120, loss: 2.085171\n",
      "Epoch 121, loss: 2.085167\n",
      "Epoch 122, loss: 2.085162\n",
      "Epoch 123, loss: 2.085159\n",
      "Epoch 124, loss: 2.085155\n",
      "Epoch 125, loss: 2.085152\n",
      "Epoch 126, loss: 2.085148\n",
      "Epoch 127, loss: 2.085145\n",
      "Epoch 128, loss: 2.085142\n",
      "Epoch 129, loss: 2.085139\n",
      "Epoch 130, loss: 2.085135\n",
      "Epoch 131, loss: 2.085131\n",
      "Epoch 132, loss: 2.085127\n",
      "Epoch 133, loss: 2.085126\n",
      "Epoch 134, loss: 2.085121\n",
      "Epoch 135, loss: 2.085116\n",
      "Epoch 136, loss: 2.085113\n",
      "Epoch 137, loss: 2.085109\n",
      "Epoch 138, loss: 2.085107\n",
      "Epoch 139, loss: 2.085102\n",
      "Epoch 140, loss: 2.085098\n",
      "Epoch 141, loss: 2.085095\n",
      "Epoch 142, loss: 2.085092\n",
      "Epoch 143, loss: 2.085088\n",
      "Epoch 144, loss: 2.085085\n",
      "Epoch 145, loss: 2.085082\n",
      "Epoch 146, loss: 2.085078\n",
      "Epoch 147, loss: 2.085073\n",
      "Epoch 148, loss: 2.085070\n",
      "Epoch 149, loss: 2.085065\n",
      "Epoch 150, loss: 2.085063\n",
      "Epoch 151, loss: 2.085060\n",
      "Epoch 152, loss: 2.085055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cf0265019a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0my_train_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreg_strength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Рабочий стол/python/dl_on_fingers/dl_on_fingers/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, batch_size, learning_rate, reg, epochs)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mtrain_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0my_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mloss_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mloss_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Рабочий стол/python/dl_on_fingers/dl_on_fingers/assignments/assignment1/linear_classifer.py\u001b[0m in \u001b[0;36mlinear_softmax\u001b[0;34m(X, W, target_index)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     '''\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "num_folds = 5\n",
    "train_folds_X = np.array_split(train_X, num_folds)\n",
    "train_folds_y = np.array_split(train_y, num_folds)\n",
    "\n",
    "result = {}\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for reg_strength in reg_strengths:    \n",
    "        scores = list()\n",
    "        for n_fold in range(num_folds):\n",
    "            X_train_tmp = list(train_folds_X)\n",
    "            X_test_tmp = X_train_tmp.pop(n_fold)\n",
    "            X_train_tmp = np.concatenate(X_train_tmp)\n",
    "        \n",
    "            y_train_tmp = list(train_folds_y)\n",
    "            y_test_tmp = y_train_tmp.pop(n_fold)\n",
    "            y_train_tmp = np.concatenate(y_train_tmp)\n",
    "        \n",
    "            classifier.fit(X_train_tmp, y_train_tmp, epochs=num_epochs, learning_rate=learning_rate, batch_size=batch_size, reg=reg_strength)\n",
    "            \n",
    "            pred = classifier.predict(X_test_tmp)\n",
    "            accuracy = multiclass_accuracy(pred, y_test_tmp)\n",
    "            scores.append(accuracy)\n",
    "        \n",
    "        accuracy_tmp = np.mean(scores)\n",
    "        if accuracy_tmp > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy_tmp\n",
    "            classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=learning_rate, batch_size=batch_size, reg=reg_strength)\n",
    "            best_classifier = classifier\n",
    "            print(\"Best accuracy: %4.2f\" % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
