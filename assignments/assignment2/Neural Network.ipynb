{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-6cc8c6fecdee>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "<ipython-input-3-6cc8c6fecdee>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.216398, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.153856, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.099106, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285874, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.168890, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.063678, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.365672, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.103934, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.222549, Train accuracy: 0.210333, val accuracy: 0.218000\n",
      "Loss: 2.059842, Train accuracy: 0.233111, val accuracy: 0.237000\n",
      "Loss: 2.115231, Train accuracy: 0.260667, val accuracy: 0.257000\n",
      "Loss: 2.090787, Train accuracy: 0.272111, val accuracy: 0.275000\n",
      "Loss: 2.098957, Train accuracy: 0.288222, val accuracy: 0.292000\n",
      "Loss: 1.974340, Train accuracy: 0.306889, val accuracy: 0.310000\n",
      "Loss: 2.011588, Train accuracy: 0.333000, val accuracy: 0.336000\n",
      "Loss: 1.819071, Train accuracy: 0.369444, val accuracy: 0.366000\n",
      "Loss: 1.805035, Train accuracy: 0.398222, val accuracy: 0.391000\n",
      "Loss: 2.001364, Train accuracy: 0.415444, val accuracy: 0.405000\n",
      "Loss: 1.930639, Train accuracy: 0.435778, val accuracy: 0.432000\n",
      "Loss: 1.738233, Train accuracy: 0.467111, val accuracy: 0.463000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD())\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9f4a78ceb0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.134298, Train accuracy: 0.218889, val accuracy: 0.228000\n",
      "Loss: 1.760945, Train accuracy: 0.405778, val accuracy: 0.380000\n",
      "Loss: 1.238614, Train accuracy: 0.556000, val accuracy: 0.533000\n",
      "Loss: 1.761091, Train accuracy: 0.581333, val accuracy: 0.580000\n",
      "Loss: 0.849264, Train accuracy: 0.669556, val accuracy: 0.664000\n",
      "Loss: 0.926747, Train accuracy: 0.678333, val accuracy: 0.646000\n",
      "Loss: 1.337701, Train accuracy: 0.711111, val accuracy: 0.685000\n",
      "Loss: 0.877371, Train accuracy: 0.746111, val accuracy: 0.710000\n",
      "Loss: 0.627769, Train accuracy: 0.753000, val accuracy: 0.694000\n",
      "Loss: 0.766936, Train accuracy: 0.773889, val accuracy: 0.699000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), num_epochs=10, learning_rate=1e-1, learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.154439, Train accuracy: 0.196889, val accuracy: 0.207000\n",
      "Loss: 2.018256, Train accuracy: 0.350444, val accuracy: 0.345000\n",
      "Loss: 1.777366, Train accuracy: 0.515889, val accuracy: 0.509000\n",
      "Loss: 1.429423, Train accuracy: 0.604333, val accuracy: 0.614000\n",
      "Loss: 1.382985, Train accuracy: 0.666222, val accuracy: 0.657000\n",
      "Loss: 1.848315, Train accuracy: 0.695000, val accuracy: 0.668000\n",
      "Loss: 0.988692, Train accuracy: 0.670444, val accuracy: 0.644000\n",
      "Loss: 1.892633, Train accuracy: 0.679667, val accuracy: 0.645000\n",
      "Loss: 1.502153, Train accuracy: 0.731000, val accuracy: 0.691000\n",
      "Loss: 1.146144, Train accuracy: 0.729556, val accuracy: 0.671000\n",
      "Loss: 1.027118, Train accuracy: 0.749444, val accuracy: 0.697000\n",
      "Loss: 1.253604, Train accuracy: 0.758556, val accuracy: 0.683000\n",
      "Loss: 0.683497, Train accuracy: 0.770111, val accuracy: 0.729000\n",
      "Loss: 0.950653, Train accuracy: 0.783667, val accuracy: 0.706000\n",
      "Loss: 1.289287, Train accuracy: 0.792667, val accuracy: 0.720000\n",
      "Loss: 0.818379, Train accuracy: 0.786000, val accuracy: 0.726000\n",
      "Loss: 0.887855, Train accuracy: 0.812556, val accuracy: 0.741000\n",
      "Loss: 0.950455, Train accuracy: 0.821778, val accuracy: 0.728000\n",
      "Loss: 0.884506, Train accuracy: 0.798111, val accuracy: 0.726000\n",
      "Loss: 1.040186, Train accuracy: 0.814556, val accuracy: 0.749000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-3)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331566, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.327830, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.298602, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.260111, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.294415, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.251854, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.298362, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.202128, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.310806, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.116070, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.250636, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.312734, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.130894, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.183713, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.043726, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.717408, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.004275, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.643377, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.880290, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.137874, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.846464, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.307836, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.973467, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.789630, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.247063, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.973122, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.068409, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.812731, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.997977, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 2.013583, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.626466, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.524134, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.515214, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.448521, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 1.938838, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.540637, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.628341, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.908089, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.918830, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.157492, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.591847, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.722412, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.417973, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.951997, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.632413, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.328661, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.350033, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.658952, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.138258, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.960261, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.830577, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.493052, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 1.615255, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.248842, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.485511, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.308680, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.545167, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.623569, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.081988, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.094690, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.281432, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.499026, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 2.004929, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.857869, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.971147, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.777913, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.175896, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.864064, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.034564, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.206900, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.691938, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.348793, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.435030, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.619697, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.313562, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.600011, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.534786, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.279276, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.038921, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.307814, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.279392, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.012608, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.407412, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.894351, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.355228, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 2.020480, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.319574, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.404098, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.359016, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.275392, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.103872, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.670950, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.015849, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.907790, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.084163, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.135526, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.188529, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.422835, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.566177, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.993749, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.001250, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.225372, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.487581, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.358065, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.094462, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.317933, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.394488, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.947160, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.495729, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.931568, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.435120, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.000825, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.191991, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.684819, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.371131, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.460345, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.503883, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.336961, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.239998, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.269916, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.478522, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.118792, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.186345, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.285760, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.506989, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.383719, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.672111, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.345955, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.373666, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.012802, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.249122, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.293137, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.286737, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.418880, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.397799, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.285408, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.402033, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.422368, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.339966, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.233328, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.230194, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.260232, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.456348, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.157295, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.261034, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.463535, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.305819, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.343644, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.496993, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.444643, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
